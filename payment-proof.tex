\section{Proof of Theorem~\ref{rst:budget}}
\label{sec:payment-proof}

We restate the theorem here for convenience:

\begin{rtheorem}{Theorem}{\ref{rst:budget}}
\dkcomment{Copy here the final form of the theorem once it is finalized.}
\end{rtheorem}

The high-level idea of the proof is motivated by Lemma~\ref{lem:numP},
which shows that the expected \emph{number} of payments is constant.
Unfortunately, in contrast to the regret, there is no hard upper bound
on the payment in any one round.
If a draw of a particular arm comes out wildly inaccurate --- which is
an event of low but positive probability ---
then agents may require very large incentives to pull this arm again
in the future (and correct the inaccurate estimate).
The high payments are offset by the exceedingly low probability of
having to incur them, but a rigorous analysis requires some care:
if a high payment is required in one phase, this indicates a very
inaccurate estimate, which may require multiple phases to correct.
Hence, we need to handle dependency of payments across time steps and
phases.

To reason about such estimation errors formally, we define
\emph{envelopes} of sample paths.
A sample path \SP captures all the random events affecting the
algorithm, i.e., the random draws \AgentV{t} of agents and the
noise \NoiseV[t] in the draws the pulled arms,
with an infite \dkcomment{Is it infinite?} time horizon.
\dkcomment{We should be consistent about using filtration vs.~sample
  path.}
\pfcomment{I removed discussion of filtrations and replaced them by discussion of history.}

With foresight, we define $g(s, \ell) := \frac{12 \sigma \ell}{s^{2/5}}$.
Let $\ErrV{t}{i} = \ArmEV{t}{i} - \ArmV{i}$ be the estimation
error for the attribute vector \ArmV{i} at time $t$,
with components \Err{t}{i}{j}.
For any sample path \SP, let $s (t,\SP)$ be the phase number that the
algorithm is in at time $t$ with the sample path \SP.
Define the sets

\begin{align*}
\hat{\mathcal{L}}_\ell
  & = \Set{\SP}{|\Err{t}{i}{j}(\SP)| \leq g(s(t,\SP),\ell)
    \mbox{ for all } i,j,t},\\
\Env{1} & = \hat{\mathcal{L}}_1,\\ 
\Env{\ell} & = \hat{\mathcal{L}}_{\ell} \setminus \hat{\mathcal{L}}_{\ell-1}
  \quad \mbox{ for } \ell \geq 2.
\end{align*}

% \begin{align*}
% L'[\ell](t)
%   & = \Set{\SP}{|\Err{t}{i}{j}(\SP)|\leq g(s(t,\SP),\ell), \forall i,j\}
% \end{align*}
We call \Env{\ell} the \Kth{\ell} envelope.
In words, \Env{\ell} consists of all sample paths such that at all times
$t$, all coordinates of all arm estimation errors are bounded by
$g(s, \ell)$,
but for at least one time $t$, at least one coordinate of one arm
estimation error is \emph{not} bounded by $g(s, \ell-1)$.
When \SP is clear, we omit it in the notation for
\Err{t}{i}{j}, payments, etc.
The importance of envelopes is that for small $\ell$, the payments are
tightly bounded, while for large $\ell$, the cumulative probability of
the sample paths in \Env{\ell} is small.
This is captured by the following two lemmas.

\begin{lemma} \label{lem:sample-path-payment}
If $\SP \in \Env{\ell}$ and $s(t,\SP) = s$, then
the payment in step $t$ is upper-bounded by
$\bar{c} (s,\ell) = \MAXR + 2 \Diam d \cdot g(s,\ell)$.
\dkcomment{Verify that the way I talk about $t$ vs.~$s$ is right.}
\end{lemma}

\begin{proof}
The maximum payment is upper-bounded by the maximum perceived
difference in value for any agent type and any two arms:
\begin{align*}
\bar{c} (s, \ell) & \leq 
\max_{\AgV} \left(  \max_{i} \AgV \cdot \ArmEV{t}{i}
                 - \min_{i'} \AgV \cdot \ArmEV{t}{i'} \right) \\
& = \max_{\AgV} \left( \max_{i}\AgV \cdot (\ErrV{t}{i}+\ArmV{i})
                    - \min_{i'}\AgV \cdot (\ErrV{t}{i'}+\ArmV{i'}) \right) \\
& \leq \max_{\AgV} \left(  \max_{i} \AgV \cdot \ArmV{i}
                        - \min_{i'} \AgV \cdot \ArmV{i'}
                        + \max_{i} \AgV \cdot \ErrV{t}{i}
                        - \min_{i'} \AgV \cdot \ErrV{t}{i'} \right) \\
& \leq \MAXR + 2 \Diam d \cdot g(s,\ell). 
\end{align*}
The final inequality used the definition of the envelope.
\end{proof}

\begin{lemma} \label{lem:envelope-probability}
For every $\ell \geq 2$, we have that
$\Prob{\SP \in \Env{\ell}} \leq 24 \ARMNUM d\exp(-1.8(\ell-1)^2)$. 
\end{lemma}

\begin{proof}
The proof idea is quite similar to the proof of
Lemma~\ref{lem:round-prob};
however, the specific form of the envelopes necessitates some subtle
changes in the specific forms of the bounds used.
For \SP to be in \Env{\ell}, by definition,
for at least one time $t$,
at least one coordinate of at least one arm's estimation error must
exceed $g(s(t,\SP),\ell-1)$.
For now, fix an arm $i$ and coordinate $j$.

Recall that $\NumPull{t}{i}(\SP) \geq s(t,\SP)$ is the number of times
that arm $i$ has been pulled by time $t$ under \SP.
We define the scaled estimation error
$S_{i,j}(t) := \frac{\NumPull{t}{i} \cdot \Err{t}{i}{j}}{2\sigma}$.
The advantage of $S_{i,j}(t)$ is that it is 
a summation of $1/2$-Gaussian random variables, 
so we will be able to apply Lemma~\ref{lem:ACI-inequality}.
First notice that we can express the desired probability as follows:

\begin{align*}
\Prob{|\Err{t}{i}{j}| \geq g(s(t,\SP), \ell-1)}
& = \Prob{S_{i,j}(t) \geq \frac{6\NumPull{t}{i}(\ell-1)}{s(t,\SP)^{2/5}}}\\
& \leq \; \Prob{S_{i,j}(t) \geq 6\NumPull{t}{i}^{3/5}(\ell-1)}. 
\end{align*}

We will show below that
$5 n^{3/5} \geq \sqrt{0.6n \log(\log_{1.1}(n)+1)}$
for all $n \geq 1$.
Applying this inequality and sublinearity of $\sqrt{\cdot}$,
we obtain that

\begin{align*}
6 \NumPull{t}{i}^{3/5}(\ell-1)
& \geq 5 \NumPull{t}{i}^{3/5} + \NumPull{t}{i}^{3/5}(\ell-1)  \\
& \geq \sqrt{0.6 \NumPull{t}{i} \log(\log_{1.1}(\NumPull{t}{i})+1)}
     + \sqrt{(\ell-1)^2 \NumPull{t}{i}}  \\
& \geq \sqrt{0.6\NumPull{t}{i}\log(\log_{1.1}(\NumPull{t}{i})+1)+(\ell-1)^2 \NumPull{t}{i}}
\; =: \; h(\NumPull{t}{i}).
\end{align*}

Let the random stopping time $\tau_{i,j}$ be the first value of
\NumPull{t}{i} (i.e., the first pull of arm $i$) such that 
$S_{i,j}(\tau_{i,j}) > h(\NumPull{t}{i})$.
$\tau_{i,j} = \infty$ means that the estimation error never exceeds 
$h(\NumPull{t}{i})$.
We apply Lemma~\ref{lem:ACI-inequality}
to $S_{i,j}(t)$, with a stopping time of $\tau_{i,j}$ and
with $b(n) = (\ell-1)^2$ \dkcomment{omit dependence on $n$ here?}.
We obtain that the probability that $\tau_{i,j} < \infty$ and 
$S_{i,j}(\tau_{i,j}) > h(\tau_{i,j})$ is at most
$12 \e^{-1.8 b(n)}$.
By definition of $\tau_{i,j}$, the event is the same as simply
$\tau_{i,j} < \infty$.
Applying the lemma again to $-S_{i,j}(\tau_{i,j})$,
and taking a union bound over both cases,
gives us that the probability that there is \emph{any} time $t$ with
$|S_{i,j}(\NumPull{t}{i})| > h (\NumPull{t}{i})$
is at most $24 \e^{-1.8 b(n)}$.
Hence, the probability that the error at \emph{any} time $t$ exceeds
$g(s(t,\SP), \ell-1)$ is bounded by 
\begin{align*}
\Prob{|\Err{t}{i}{j}| \geq g(s(t,\SP), \ell-1)}
& \leq 24 \e^{-1.8(\ell-1)^2}.
\end{align*}
Now, taking a union bound over all arms $i$ and
coordinates $j$ completes the proof.

It remains to show that
$5 n^{3/5} \geq \sqrt{0.6n \log(\log_{1.1}(n)+1)}$
for all $n \geq 1$.
By squaring the inequality and canceling out a factor $n$,
the statement is equivalent to showing that
$25 n^{1/5} \geq \log(\log_{1.1}(n)+1)$.
We will show the stronger statement that
$25 n^{1/5} \geq \log_{1.1}(n)+1$.  
To see this, notice that the derivative of the left-hand side is
always strictly larger than the derivative of the right-hand side,
so the difference between the sides is minimized at $n=1$,
where it is positive. 
\end{proof}

Next, we show that for any sample path \SP in the envelope \Env{\ell},
we can bound the total number of payments made in terms of $\ell$.
Define $h(\ell) := \max \left( \exp(\frac{2}{\MinProb}),
\left( \frac{48 \sigma \ell \TieDensity \Diam d}{\MinProb} \right)^{5/2}
\right)$.

\begin{lemma} \label{lem:envelope-payments}
Let \SP be a sample path in \Env{\ell}.
Then, under \SP, the algorithm makes payments at most for the first 
$h(\ell)$ phases. 
\end{lemma}

\begin{proof}
The proof is similar to that of Lemma~\ref{lem:numP}.
Fix a sample path $\SP \in \Env{\ell}$.
Consider a phase $s > h(\ell)$.

By definition of \Env{\ell}, the coordinate-wise estimation errors at
times $t$ in phase $s$ are
$|\Arm{i}{j} - \ArmE{t}{i}{j}| \leq g(s,\ell)$.
In particular, all agent typs \AgV whose preference for their best arm
is sufficiently clear
($\AgV \cdot \ArmV{\Best{\AgV}} - \AgV \cdot \ArmV{\Second{\AgV}}
> 2\Diam d g(s,\ell)$) will pull their best arm.

By Lemma~\ref{lem:sdelta}, the \emph{total} measure of agents (across
all arms) whose best and second-best arm differ in utility by at most
$2\Diam d \cdot g(s,\ell)$ is at most $2L \Diam d \cdot g(s,\ell)$.
Substituting the definition of $g$, because 
$s > h(\ell) \geq \left( \frac{48 \sigma \ell \TieDensity \Diam d}{\MinProb} \right)^{5/2}$,
the total measure of these agents is bounded by $\MinProb/2$.

By Assumption~\ref{A3}, at least a \MinProb fraction of agents have
$i$ as their best arm, so even with estimation errors, at least
$\MinProb - \MinProb/2 = \MinProb/2$ fraction will still prefer to
pull arm $i$.

Because $s > h(\ell) \geq \exp(\frac{2}{\MinProb})$, 
we get that $1/\log(s) \leq \MinProb/2$, so in phase $s$,
the algorithm will not incentivize any arms whose pulling probability
is more than $\MinProb/2$, which is all arms.
So no payments will be made in phase $s$.
\end{proof}


\begin{extraproof}{Theorem~\ref{rst:budget}}
We can write the total expected payment as
\begin{align*}
  \Expect{\sum_{t=1}^{\infty} \PayA{t}}
& = \sum_{\ell = 1}^{\infty} \sum_{\SP \in \Env{\ell}}
  \Prob{\SP} \cdot \sum_{t=1}^{\infty} \PayA{t}(\SP)
\; = \; \sum_{\ell = 1}^{\infty} \sum_{\SP \in \Env{\ell}}
  \Prob{\SP} \cdot \sum_{s=1}^{\infty} \sum_{t: s(t,\SP) = s} \PayA{t}(\SP).
\end{align*}

By Lemma~\ref{lem:envelope-payments},
the payments are 0 for $s > h(\ell)$,
and by Lemma~\ref{lem:sample-path-payment},
when $\SP \in \Env{\ell}$ and $s(t,\SP) = s$, we can bound
$\PayA{t}(\SP) \leq \MAXR + 2 \Diam d \cdot g(s,\ell)$.
Furthermore, in any one phase, because each arm is incentivized at
most once, there are at most \ARMNUM payments total.
Substituting these bounds, we obtain that

\begin{align*}
  \Expect{\sum_{t=1}^{\infty} \PayA{t}}
& \leq \sum_{\ell = 1}^{\infty} \sum_{\SP \in \Env{\ell}} \Prob{\SP} \cdot
  \sum_{s=1}^{h(\ell)} \ARMNUM \cdot (\MAXR + 2 \Diam d \cdot g(s,\ell))
\\ & = \ARMNUM \cdot
  \sum_{\ell = 1}^{\infty} \Prob{\SP \in \Env{\ell}} \cdot
  \sum_{s=1}^{h(\ell)} \left( \MAXR + \frac{24 \Diam d \sigma \ell}{s^{2/5}} \right).
\end{align*}

We now lower-bound $s^{2/5} \geq 1$, split off the term for $\ell=1$
and bound $\Prob{\SP \in \Env{1}} \leq 1$, and apply
Lemma~\ref{lem:envelope-probability} to the remaining
$\Prob{\SP \in \Env{\ell}}$ terms, to bound

\begin{align*}
\Expect{\sum_{t=1}^{\infty} \PayA{t}}
& \leq
\ARMNUM \cdot \sum_{s=1}^{h(1)} (\MAXR + 24 \Diam d \sigma)
+ \ARMNUM \cdot
  \sum_{\ell = 2}^{\infty} 24 \ARMNUM d\exp(-1.8(\ell-1)^2) \cdot
  \sum_{s=1}^{h(\ell)} (\MAXR + 24 \Diam d \sigma \ell)
\\ & \leq
\ARMNUM \cdot h(1) \cdot (\MAXR + 24 \Diam d \sigma)
+ 24 \ARMNUM^2 d \cdot \sum_{\ell = 2}^{\infty}
     \exp(-1.8(\ell-1)^2) \cdot h(\ell) \cdot (\MAXR + 24 \Diam d \sigma \ell).
\end{align*}
Because $h(\ell)$ and $(\MAXR + 24 \Diam d \sigma \ell)$ grow
polynomially in $\ell$, whereas $\exp(-1.8(\ell-1)^2)$ decreases
exponentially in $\ell$, the sum is dominated by its first term, and
the overall expected payment is bounded by

\begin{align*}
O \left(\ARMNUM^2 d \cdot (\MAXR + \Diam d \sigma) \cdot h(1) \right)
& = 
O \left(\ARMNUM^2 \MAXR \Diam^2 \TieDensity d^3 \sigma
\cdot \exp(\frac{2}{\MinProb}) \right).
\end{align*}
\dkcomment{$p^{-5/2}$ is dominated by $\exp(2/p)$.}
\pfcomment{the bound got tighter.  I didn't carry this forward anywhere where we report on / use the bound.}
\end{extraproof}
