\documentclass{article}
\usepackage[utf8]{inputenc}

\usepackage[english]{babel}

\usepackage{blindtext}
\usepackage{amssymb}

\usepackage[]{algorithm}
\usepackage[]{algorithmic}
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{xr} 
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsthm}
\usepackage{comment}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{dblfloatfix}
\usepackage{bbm}

\newcommand{\IE}{IE} 
\newcommand{\argmax}{\mathop{\mathrm{argmax}}}



\newcommand\floor[1]{\lfloor#1\rfloor}
\newcommand\ceil[1]{\lceil#1\rceil}

\newcommand{\bccomment}[1]{{\color{blue}BC: #1}}
\newcommand{\pfcomment}[1]{{\color{blue}PF: #1}}


\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{assumption}{Assumption}



\title{Incentivizing Exploration with Heterogeneous Utilities}

\begin{document}

\maketitle


\section{Introduction}
In this paper, we study incentivizing exploration with heterogeneous agent preferences.
In this problem,   
arms have unknown multivariate attributes, and agents have heterogeneous linear utility functions that map these attribute vectors onto utilities. Agents see noisy observations of attributes of arms pulled by all previous agents, and estimate an arms' attribute vector by the simple average of these observations.  Agents are selfish, and pull the arm with the largest estimated utility summed with an optional arm-specific incentivizing payment chosen by the principle.
We study strategies for choosing such incentive payments that seek to maximize the total utility derived by agents, subject to a limitation on the total incentive payment.  To accomplish this goal, a strategy must induce sufficient exploration to reveal arms' attributes, while still letting agents select myopically and according to their preferences sufficiently often that high-utility arms are chosen and incentive payments are kept small.

Our problem setting models online review aggregators like Amazon, Yelp, and Tripadvisor that host crowdsourced reviews that users use when selecting products, restaurants, and vacation destinations.  Users of these websites wish to choose the product / restaurant / vacation (item) that is best according to their preference over attributes, and rely on previous users' reviews to estimate these attributes.  These reviews provide not just cardinal feedback, but also a description of attributes that a user with different preferences may use to select their preferred item.  An item with only a small number of reviews might have inaccurate attribute estimates, leading users to choose not to try it even though it may actually be the best choice for some subset of those users.  Without incentives, this situation may persist and decrease welfare for the platform's user base.  By offering incentives, either through price reductions by Amazon or coupons from Yelp or Tripadvisor, a platform may induce more reviews of unexplored items and provide more value over the long term.

% \item \textit{Citizen Science}:  \cite{frazier2014incentivizing} mentioned for scientific organizations like Galaxy Zoo, they will guide the enthusiasts to explore the less-explored parts of domain. For each enthusiast, he/she may have different preference towards the location's sight-seeing, weather, distance and safety etc. Thus, as the organization, it needs to incentivize each person differently. 

\pfcomment{Banguri, I'm a bit confused about the citizen science application --- let's leave it out of the dissertation, but leave it in the paper.  Can you add some more detail?  For example, with Galaxy Zoo, my understanding is that you are looking at galaxies on a computer screen and so there is no weather.  Maybe you are talking about e-bird?  Can you write something that clarifies this and then we can talk more?
Also, if you can come up with an example of an application with implicit attribute feedback, include it here.}


% Something interesting to study would be to alter the search rankings to influence people
% Also which items to spam people about reviewing

In this problem context, we study a simple policy that usually exploits, incentivizing agents to pull an arm only when the set of agent utility functions that would pull this arm without incentives has probability below a time-varying threshold.  \pfcomment{Say something about what we show for this policy.}

\pfcomment{Talk about how our regret bounds compare to what our regret bounds would be in a homogenous preference setting, to clarify the following sentences.}
These results suggest that heterogeneous agent preferences reduces but does not eliminate the need to incentive exploration, in relation to single-preference settings.  Essentially, heterogeneity provides a certain amount of exploration for free.


We broadly categorize the literature into two categories based on whether there is money transfer.
With money transfer, \cite{frazier2014incentivizing} considers a problem setting where the principal pays the agents extra money in order to incentivize them to explore. In their paper, they assume all agents have equal value of money and provide a complete characterization of achievable reward with a fixed budget. \cite{han2015incentivizing} generalizes this framework to allow agents have heterogeneous value of money. Further, it assumes there is a external signal scheme which provides partial information about each agent. Under this setting, they proved a similar bound as in \cite{frazier2014incentivizing} which depends on the signal scheme.

Without money transfer but using information asymmetry, \cite{kremer2014implementing} considers a simple model where agents arrive to the principal one by one and there are only two actions at each time. \cite{mansour2015bayesian} generalizes \cite{kremer2014implementing} by allowing finite number of actions at each time. \cite{mansour2016bayesian} considers a problem setting where there are multiple agents at each time and allow agents to interact with each other. In these papers, the principal provides each agent an recommendation at each time, which is Bayesian incentive-compatible. They prove principal can achieve constant regret when utilities are deterministic and logarithmic regret when utilities are stochastic.



We structure our paper as follows: Section~\ref{sec:prob} formulates our problem; Section~\ref{sec:ub} states our algorithm and proves that we can achieve $O(N^2+(\log(T))^2)$ regret with $O(N^3)$ incentive budget; Section~\ref{sec:lb} constructs an example showing regret is at least $\Omega(\log(T))$ in the worst case, regardless of incentive budget.




\section{Problem Setting}
\label{sec:prob}

We have $N$ arms. Arm $i$ has a fixed but unknown attribute vector $u_i\in \mathbb{R}^{m}$. 

A stream of myopic selfish agents come to our system.  Agent $t$ has linear preferences over attributes described by a vector $\theta_t \in \mathbb{R}^m$ that is unknown to the principal and drawn i.i.d. from a known distribution $F(\cdot)$. We refer synonomously to an agent and that agent's preference vector: when we say ``an agent $\theta$'', we mean ``an agent with preference vector $\theta$.''

Each agent $t$ chooses an arm to pull $A_t$, according to a process described below, and obtains utility $\theta_t \cdot u_{A_{t}}$.  The principal and all agents then see a noisy observation of the attribute vector of the pulled arm of the form $O_t=u_{A_{t}}+\epsilon_{t}$, where $\epsilon_t\sim N(0, \sigma^2 I_{m})$ is independent normally distributed noise, and $I_m$ denotes an $m$-dimensional identity matrix.  Although we assume a common variance across attributes for simplicity of presentation, our theoretical results hold if the variance differs.

At each time $t$, for each arm $i$, we (the principal) offer a non-negative payment $c_{i,t}\geq 0$ based on previous observations.
We assume that agent $t$ chooses to pull the arm that myopically maximizes the sum of this payment and an estimate of the utility obtained $\theta_t \cdot u_{i,t}$ where $u_{i,t}$ denotes the simple average of $O_s$ over all previous pulls of arm $i$ if it has been pulled at least once, or a default value of $0$ if not.  That is, $u_{i,t} = \sum_{s<t} O_s 1\{A_s = i\} / \sum_{s<t} 1\{A_s = i\}$ if the denominator is strictly positive and $0$ otherwise; and $A_t=\argmax_{i}\{\theta_t\cdot u_{i,t}+c_{i,t}\}$, breaking ties in favor of the arm with the highest incentive.  We use $c_t=c_{A_{t},t}$ to denote the actual incentive payment at time $t$. For simplicity, we assume all arms have been pulled once at time $t=1$ in this paper (we can offer a large payment at the beginning to make this happen). \pfcomment{Bangrui, I assumed that $u(i,t)$ starts from $0$.  Do you want to do this, or do you want to assume that every arm is pulled at least once?  Or do you want to assume that they start from some general starting value?  We could perhaps do the analysis assuming they start from $0$, but then comment that the proofs all carry through if they start from some other value.}

This behavior may be recovered if agents are Bayesian and share a common non-informative prior distribution that is constant over $\mathbb{R}^m$ and know $\sigma^2$.  In this case, the posterior distribution on $u_{i}$ at time $t$ is multivariate normal with mean $u_{i,t}$, and the expected value of $\theta_t \cdot u_i$ under this posterior conditioned on $\theta_t$ is $\theta_t \cdot u_{i,t}$ (see equation $2.13$ in section $2.5$, \cite{Ge04}).  Alternatively, one may simply take our assumption that agents use the average as their estimate of an attribute value directly without such a Bayesian justification.

We define the regret at time $t$ as $r(t)=\max_{i}\{\theta_{t}\cdot u_{i}\}-\theta_t\cdot u_{A_t}$, and the cumulative regret up to time T as $R(T)=\sum_{t=1}^{T}r(t)$. Define the cumulative payment up to time T similarly as $C(t)=\sum_{t=1}^{T}c(t)$. 
As the principal, we want to find a strategy $\mathcal{A}$ under which both the cumulative expected regret $\mathbb{E}_{\mathcal{A}}[R(T)]$ and the cumulative expected payment $\mathbb{E}_{\mathcal{A}}[C(T)]$ are small.

To support later development, we define three additional pieces of notation.
We let $B(\theta)$ and $\hat{B}(\theta)$ refer to the index of the arm that is best and second best for an agent with preference vector $\theta$, $B(\theta) \in \argmax_i \theta \cdot u_i$ and $\hat{B}(\theta)=\argmax_{i\neq B(\theta)}\theta\cdot u_{i}$, breaking ties uniformly at random. We let $N(i,t)$ denote the cardinality of $A_t$, i.e., the number of pulls of arm $i$ at times up to and including $t$.

\section{Upper Bound}
\label{sec:ub}

In this section, we propose a simple policy that mostly exploits, and occasionally incentivizes exploration when an arm would be unpulled by all agent types given the current posterior. We prove that with the help of heterogeneous preferences, we can get a certain amount of exploration for free via heterogeneity. 

\subsection{Our Algorithm}
Before we introduce our algorithm, we need to use the following definition:

\begin{definition}
We call time $t_{n}$ as the starting point of the $n^{th}$ round if $t_{n}=\min_{t}\{\forall i, N(i,t)\geq n\}$. We call the time period from $t_{n}$ to $t_{n+1}$ as the $n^{th}$ round.
\end{definition}

We consider an algorithm that offers incentive to an arm $i$ if and only if both of the following criterias are met:
\begin{itemize}
\item the myopic probability of pulling arm $i$ is low; 
\item arm $i$ has not been played or incentivized before in this round.
\end{itemize}
We break ties randomly. If we decides to incentivize an arm, we use ``pay whatever it takes'' strategy (We use ``pay whatever it takes'' strategy mainly because of its simplicity. In Section~\ref{sec:pi}, we provide a more realistic algorithm which has the same payment budget bound and regret bound). This algorithm does not need to know $T$ in advance. The detailed algorithm is as follows:

\begin{algorithm}
\caption{Algorithm: Incentivizing Exploration}
\label{Alg1}
\begin{algorithmic}
\STATE Set n = 1 to denote the round number;
\FOR{ $t = 1, 2, 3, \cdots$}{
	\IF{the myopic probability of pulling an arm is less than $n^{-1}$ and no agents have pulled that arm in this round (break ties randomly)}
	\STATE{ we pay whatever it takes to incentivize that arm}\ELSE 
		\STATE {let the agents play myopically}\ENDIF
		\STATE $n = min_{i} N(i,t)$
}\ENDFOR

\end{algorithmic}
\end{algorithm}


\subsection{Assumptions}
In this secion, we discuss several assumptions that we need to acheive our results. Throughout this paper, we use $F(\cdot)$ to denote the agent's preference distribution, which could either be a continuous distribution or a discrete distribution. Denote

\begin{align}
\Omega_{i,j}=\{\theta:B(\theta)=i, \hat{B}(\theta)=j\}, \nonumber 
\end{align}
which is the users whose best arm is arm $i$ and the second best arm is arm $j$. We need the following assumptions in our analysis:

\begin{assumption} Let $f_{i,j}(y)$ be the marginal probability density function (or marginal probability mass function if $F(\cdot)$ is a discrete distribution) of $(u_i-u_j)\cdot\theta$ conditioned on $\theta \in \Omega_{i,j}$. We assume $M^{'}=\max_{i,j}\limsup_{y\rightarrow 0^{+}}f_{i,j}(y) <\infty$.
\label{A1}
\end{assumption}

Intuitively, assumption~\ref{A1} states that there are not many agents who are indifferent between their best arm and the second best arm. For simplicity in our analysis, we use a slightly stronger condition below: $M=\max_{i,j}sup_{y\in R^{+}}f_{i,j}(y) <\infty$.


\begin{assumption} We assume $F$ has a compact support set. Without loss of generality, we assume $\theta\in [0,W]^m$.
\label{A2}
\end{assumption}

Throughout this write up, we use $R$ to denote the maximum regret it can occur at each time. We know $R<\infty$ due to Assumption~\ref{A2}. 

\begin{assumption}
Denote $p=\min_{i}P(\{\theta: B(\theta)=i\})$. Throughout this paper, we assume $p>0$.
\label{A3}
\end{assumption}

Assumption~\ref{A3} means for each arm $i$, there exist a positive proportion of users whose best arm is $i$. 



\subsection{General Results}

In this section, we prove using Algorithm~\ref{Alg1}, we can achieve $O(N^2+(\log(T))^2)$ regret bound with $O(N^3)$ payment budget.

Before we start our analysis, we need the following notations. Denote $S(\delta)$ as the proportion of user whose utility difference between their best arm and the second best arm is less than $\delta$. Formally, $S(\delta)=P(\theta: \theta \cdot u_{B(\theta)}-\theta\cdot u_{\hat{B}(\theta)}\leq \delta)$. Denote $p(\delta)=\min_{i}P(\{\theta:\theta\cdot u_{B(\theta)}-\theta\cdot u_{\hat{B}(\theta)}>\delta\})$. Based on $p(\delta)$'s definition, we know $p(0)=p$. 

With the help of our assumptions and the above notations, we now state our main results:

\begin{theorem}
The payment budget for Algorithm~\ref{Alg1} is bounded above by $O(N^3)$. \bccomment{Our current analysis of the cumulative expected payment is wrong. See the comments of Lemma 6 for details}
\label{rst:budget}
\end{theorem}


\begin{theorem}
The cumulative regret for Algorithm~\ref{Alg1} is bounded above by $O(N^2 m + M m^2(\log(T))^2)$.
\label{rst:regret}
\end{theorem}

Before we prove our main results, we need to prove several lemmas. First, based on Assumption~\ref{A1}, we can get the following bound for $S(\delta)$.

\begin{lemma}
$S(\delta)\leq M\delta$.
\label{lemma:sdelta}
\end{lemma}

\begin{proof}
\begin{align}
S(\delta)&=\sum_{B(\theta)=i}\sum_{\hat{B}(\theta)=j}P(\{\theta(u_{i}-u_{j})<\delta\}) \nonumber \\
&=\sum_{B(\theta)=i}\sum_{\hat{B}(\theta)=j}P(\{\theta(u_{i}-u_{j})<\delta|\theta\in \Omega_{i,j}\})P(\theta\in \Omega_{i,j}) \nonumber \\
&\leq \sum_{B(\theta)=i}\sum_{\hat{B}(\theta)=j}M\delta \times P(\theta\in \Omega_{i,j}) \nonumber \\
&=M\delta \nonumber 
\end{align}
\end{proof}

The following lemma bounds the probability of making a mistake if we let the agents play myopically in the $n^{th}$ round, given that the utility difference between his/her best and second best arm is bounded below by a constant.

\begin{lemma}
For those user whose utility difference between their best arm and the second best arm is at least $2m\lambda$, the probability that myopic action can incur regret at any time $t\in [t_{n-1},t_{n}-1]$ in the $n^{th}$ round is bounded above by $24Nm\exp\left(-\frac{1.8n\lambda^2}{8\sigma^2}\right)$, for $n\geq n_{0}=\max\{50, \frac{23.04\sigma^4}{\lambda^4}\}$.
\label{round:prob}
\end{lemma}


We need the following lemma in order to prove Lemma~\ref{round:prob}.

\begin{lemma}
For $n\geq n_{0}=\max\{50, \frac{23.04\sigma^4}{\lambda^4}\}$, we have
\begin{align}
\frac{n\lambda}{2\sqrt{2}\sigma}\geq \sqrt{0.6n\log(\log_{1.1}(n)+1)}. \nonumber
\end{align}
\label{n0-inequality}
\end{lemma}

\begin{proof}
First, we observe that
\begin{align}
&\frac{n\lambda}{2\sqrt{2}\sigma}\geq \sqrt{0.6n\log(\log_{1.1}(n)+1)} \nonumber \\
\iff &\frac{n}{\log(\log_{1.1}(n)+1)}\geq \frac{4.8\sigma^2}{\lambda^2}. \nonumber 
\end{align}
Since $\log(x)\leq x-1$ for $x>0$, we know 
\begin{align}
\log(\log_{1.1}(n)+1)=\log\left(\frac{\log(n)}{\log(1.1)}+1\right)\leq \log(11\log(n)+1)\leq \log(11n)\leq 3+\log(n). \nonumber
\end{align}

Thus, we know
\begin{align}
\frac{n}{\log(\log_{1.1}(n)+1)}\geq \frac{n}{3+\log(n)}. \nonumber 
\end{align}

In order to prove the original lemma, we just need to show for $n\geq n_{0}$, we have
\begin{align}
\frac{n}{3+\log(n)}\geq \frac{4.8\sigma^2}{\lambda^2}. \label{n0-equ}
\end{align}
Inequality~(\ref{n0-equ}) is true because the following two observations:
\begin{itemize}
\item for $n\geq 50$, we have $\frac{n}{3+\log(n)}\geq n^{0.5}$;
\item $n\geq \frac{23.04\sigma^4}{\lambda^4}$ we have $n^{0.5}\geq {4.8\sigma^{2}}{\lambda^{2}}$.
\end{itemize}

Thus, we know our lemma is true.

\end{proof}

We also need to use an adaptive concentration inequality, which is introduce in \cite{zhao2016adaptive}. For reference, we state it as a Lemma.

\begin{lemma}[Corollary 1 in \cite{zhao2016adaptive}]
Let $X_{i}$ be zero mean $1/2$-subgaussian random variables. $\{S_{n}=\sum_{i=1}^{n}X_{i},n\geq 1\}$ be a random walk. Let $J$ be any random variable taking value in $\mathbb{N}$. If
\begin{align}
f(n)=\sqrt{0.6n\log(\log_{1.1}(n)+1)+bn}, \nonumber
\end{align}
then
\begin{align}
Pr[S_{J}\geq f(J)]\leq 12e^{-1.8b}. \nonumber
\end{align}
\label{ACI-inequality}
\end{lemma}


Now we can get back to prove Lemma~\ref{round:prob}.

\begin{proof}
In the $n^{th}$ round, we know all arms have been pulled at least $n$ times. Thus, the probability that myopic action can occur regret is bounded by
\begin{align}
&P(\text{myopic action incurs regret})\nonumber \\
\leq &P(\exists i, \exists j, |u_{i,t}^{j}-u_{i}^{j}|>\lambda) \nonumber \\
\leq & \sum_{i}\sum_{j} P(|u_{i,t}^{j}-u_{i}^{j}|>\lambda) \nonumber \\
= &  \sum_{i}\sum_{j} P\left(\bigg|\sum_{k\in \tau_{i,t}}\epsilon_{k}\bigg|>N(i,t)\lambda\right). \label{ACI}
\end{align}

To bound equation~(\ref{ACI}), we need to use Lemma~\ref{ACI-inequality}. Similar to Lemma~\ref{ACI-inequality}, we define
\begin{align}
S_{N(i,t)}=\sum_{k\in\tau_{i,t}}\frac{\epsilon_{k}}{\sqrt{2}\sigma}=\frac{1}{\sqrt{2}\sigma}\sum_{k\in\tau_{i,t}}\epsilon_{k}. \nonumber
\end{align}

Based on Lemma~\ref{n0-inequality}, we know there exists a $n_{0}=\max\{50, \frac{23.04\sigma^2}{\lambda^2}\}$ such that for $n\geq n_{0}$, we have
\begin{align}
\frac{n\lambda}{2\sqrt{2}\sigma}\geq \sqrt{0.6n\log(\log_{1.1}(n)+1)}. \nonumber
\end{align}

Thus, if we set $b=\frac{n\lambda^2}{8\sigma^2}$ in Lemma~\ref{ACI-inequality}, for any $N(i,t)\geq n\geq n_{0}$, we have
\begin{align}
\frac{N(i,t)\lambda}{\sqrt{2}\sigma}\geq & \sqrt{0.6N(i,t)\log(\log_{1.1}(N(i,t))+1)}+\frac{\lambda}{2\sqrt{2}\sigma}\sqrt{n N(i,t)} \nonumber \\
\geq & \sqrt{0.6N(i,t)\log(\log_{1.1}(N(i,t))+1)+bN(i,t)}, \nonumber 
\end{align}
where the last inequality is because $\sqrt{x}+\sqrt{y}\geq \sqrt{x+y}$. Thus, we have
\begin{align}
&P\left(\bigg|\sum_{k\in \tau_{i,t}}\epsilon_{k}\bigg|>N(i,t)\lambda\right) \nonumber \\
=&2P\left(S_{N(i,t)}>\frac{N(i,t)\lambda}{\sqrt{2}\sigma}\right) \nonumber \\
\leq & 2P\left(S_{N(i,t)}> \sqrt{0.6 N_{i,t}\log(\log_{1.1}(N(i,t))+1)+b N(i,t)}\right) \nonumber \\
\leq & 24\exp( -1.8b) \nonumber \\
= & 24\exp\left(\frac{-1.8 n\lambda^2}{8\sigma^2}\right). \nonumber
\end{align}


Thus, we know
\begin{align}
&\sum_{i}\sum_{j} P\left(\bigg|\sum_{k\in \tau_{i,t}}\epsilon_{k}\bigg|>N(i,t)\lambda\right)  \nonumber \\
=& 24Nm \exp\left(\frac{-1.8 n\lambda^2}{8\sigma^2}\right). \nonumber
\end{align}

\end{proof}

Before we start analyzing the cumulative regret, we first prove the following lemma which bounds the expected length of each round.

\begin{lemma}
Using our algorithm, we have $\mathbb{E}[t_{n}-t_{n-1}]\leq Nn$, $\forall n\geq 1$.
\label{round:length}
\end{lemma}


\begin{proof}
In each round, each arm needs to be pulled at least once. Denote $X_{i}$ to be the number of agents come to the system between $i-1$ different arms were pulled and the $i^{th}$ different arm was pulled. Then we know 
\begin{align}
\mathbb{E}[t_{n}-t_{n-1}]=\sum_{i=1}^{N}E[X_{i}]. \nonumber
\end{align}


Denote $t^{0}=t_{n-1}+\sum_{j=1}^{i-1}X_{j}$, then $X_{i}$ follows a ``geometric distribution'', which counts the total number of trials before the $i^{th}$ different arm were pulled at time $t=t^{0},t^{0}+1,\cdots$. At each time $t=t^{0}, t^{0}+1,\cdots$, there are two possibilities for a success trial:
\begin{itemize}
\item If there exists an arm has not been pulled and the proportion of agents who would pull this arm is less than $n^{-1}$, then based on our algorithm, the principal will offer an payment for that arm and it will be pulled. In this case, the probability that the $i^{th}$ different arm will be pulled is 1.  
\item There exists at least $n^{-1}$ proportion of agents who will choose an un-pulled arm as his/her myopic action. In this case, the probability that the $i^{th}$ different arm will be pulled is at least $n^{-1}$.
\end{itemize}

Since $X_{i}$ follows a ``geometric distribution'' with success probability at least $n^{-1}$ at each time, we know 
\begin{align}
E[t_{n}-t_{n-1}]\leq Nn. \nonumber
\end{align}
\end{proof}

\begin{lemma}
For $t\in[t_n,t_{n+1}]$, the expected incentive payment $E[c(t)]$ is bounded above by $\Delta + 2WNm \sqrt{\frac{2}{n\pi}}$. 
\label{lemma:payment}
\end{lemma}

\begin{proof}
Denote $\Delta=\sup_{\theta}\max_{i}\{\theta\cdot u_{B(\theta)}-\theta\cdot u_{i}\}$ to be the theoretical maximum payment with no noisy. We consider the expected payment in the $n^{th}$ round. Denote $\epsilon_{i,t}=u_{i,t}-u_{i}$.

\begin{align}
&E[c(t)] \nonumber \\
\leq & \Delta \times P(c(t)>0) + E[\max_{i}\theta_t\cdot \epsilon_{i,t}-\min_{j}\theta_t\cdot \epsilon_{j,t}1\{c(t)>0\}] \nonumber \\
\leq & \Delta + E[\max_{i}\theta_t\cdot \epsilon_{i,t}-\min_{j}\theta_t\cdot \epsilon_{j,t}] \nonumber \\
\leq & \Delta + \sum_{j=1}^{m}2WE[\max_{i}|\epsilon_{i,t}^{j}|] \nonumber \\
\leq & \Delta + 2W\sum_{j=1}^{m}E\left[\sum_{i=1}^{N}|\epsilon_{i,t}^{j}|\right] \nonumber \\
\leq & \Delta + 2WNm E[|\epsilon_{1,t}^{1}|] \nonumber \\
\leq & \Delta + 2WNm \sqrt{\frac{2}{n\pi}} \nonumber
\end{align}

\end{proof}



Now we are ready to prove our first main result, Theorem~\ref{rst:budget}. \bccomment{The current proof about the cumulative expected payment is wrong. We need to bound the $E[c(t)|\textit{our algorithm decides to incentivize}]$ instead of $E[c(t)]$. However, the analysis for the cumulative regret is correct}

\begin{proof}
If $|u_{i}^{j}-u_{i,t}^{j}|\leq \lambda$ is true $\forall i$, $\forall j$, then we know for those $\theta\in \{\theta:\theta\cdot u_{B(\theta)}-\max_{j\neq B(\theta)}\{\theta \cdot u_{j}\}\geq 2m\lambda\}$, they will correctly identify their best arm. Thus we know, in the $n^{th}$ round, if $|u_{i}^{j}-u_{i,t}^{j}|\leq \frac{p^{-1}(\frac{p}{2})}{2m}$ $\forall i$ and $\forall j$, and $n^{-1}\leq p/2$, we do not need to incentivize any arms. In order to have $n^{-1}\leq \frac{p}{2}$, we need $n\geq \frac{2}{p}$. Denote $n_1=\max\{n_{0}, \frac{2}{p}\}$. Denote $\delta_{0}=p^{-1}(\frac{p}{2})>0$ (because of Assumption~\ref{A1}).The cumulative payment is bounded above by:
\begin{align}
&E[C(T)] \nonumber \\
= & \sum_{t=1}^{\infty}E[c(t)\mathbbm{1}\{\text{need to offer payment to an arm}\}] \nonumber \\
\leq &\sum_{n=1}^{\infty}\sum_{t=1}^{\infty}E[c(t)\mathbbm{1}\{t_{n-1}\leq t\leq t_{n}: \text{myopic action occurs regret for agents with } \nonumber \\
&\theta\in \{\theta:\theta\cdot u_{B(\theta)}-\max_{j\neq B(\theta)}\{\theta \cdot u_{j}\}\geq p^{-1}(n^{-1})\}\}] \nonumber \\
\leq &\sum_{n=n_{1}}^{\infty}\sum_{t=1}^{\infty}\left(\Delta+Nm\sqrt{\frac{2}{n\pi}}\right)P(\text{myopic action occurs regret for agents with }\nonumber \\
&\theta\in \{\theta:\theta\cdot u_{B(\theta)}-\max_{j\neq B(\theta)}\{\theta \cdot u_{j}\}\geq \delta_{0}\}|t_{n-1}\leq t\leq t_{n})P(t_{n-1}\leq t\leq t_{n})+\sum_{n=1}^{n_1}N\left(\Delta+\sqrt{\frac{2}{n\pi}}\right) \nonumber \\
\leq & \sum_{n=n_{1}}^{\infty}24Nm \exp\left(\frac{-1.8 n\delta_{0}^2}{32m^2\sigma^2}\right)\times Nn\left(\Delta+Nm\sqrt{\frac{2}{n_1\pi}}\right)+\sum_{n=1}^{n_1}N\left(\Delta+\sqrt{\frac{2}{n\pi}}\right) \nonumber 
\end{align}

We know the expected payment is bounded above by:
\begin{align}
E[C(T)] \leq 24N^2 m \frac{e^{\frac{1.8\delta_{0}}{32m^2\sigma^2}}}{(e^{\frac{1.8\delta_{0}}{32m^2\sigma^2}}-1)^2}\left(\Delta+Nm\sqrt{\frac{2}{n_1\pi}}\right)+\sum_{n=1}^{n_1}N\left(\Delta+\sqrt{\frac{2}{n\pi}}\right). \nonumber
\end{align}

Since we have $N$ arms in total, we know the $\mathbb{E}[C(T)]=O(N^3)$.
\end{proof}


Based on the proof of Theorem~\ref{rst:budget}, we know the expected number of payments we need to make is less than
\begin{align}
&\sum_{n=n_{1}}^{\infty}24Nm \exp\left(\frac{-1.8 n\delta_{0}^2}{32m^2\sigma^2}\right)\times Nn+Nn_{1} \nonumber \\
=&24N^2 m \frac{e^{\frac{1.8\delta_{0}}{32m^2\sigma^2}}}{(e^{\frac{1.8\delta_{0}}{32m^2\sigma^2}}-1)^2} + Nn_1, \nonumber
\end{align}
which we will use in the proof of Theorem~\ref{rst:regret}. Now we are ready to prove our second main result, Theorem~\ref{rst:regret}.

\begin{proof}
For regret occurred in the first $n_0$ round, it is bounded above by $\sum_{n=1}^{n_{0}}NRn$.

For regret occurred after the first $n_0$ round, it has two different components: the regret occurred when we let the agents play myopically and the regret occured when we incentivize the agents. For the regret occurred when we let the agents play myopically at time t, it consists of the following two components:
\begin{itemize}
\item For those user whose utility difference between their best and the second best arm is greater than $f(t)$: suppose it is at the $n(t)^{th}$ round, the probability of these user making a mistake is bounded above by $24Nm\exp\left(-\frac{1.8n(t) f(t)^2}{32 m^2\sigma^2}\right)$ at time $t$ and the expected regret is bounded above by $24Nm\exp\left(-\frac{1.8n(t) f(t)^2}{32 m^2\sigma^2}\right)\times R$. We denote the regret occured by these agents as $r_1(t)$.
\item For those user whose utility difference between their best and the second best arm is smaller than $f(t)$: this happens with probability $S(f(t))$ at each time and regret is bounded above by $S(f(t)) \times f(t)=Mf(t)^2$. We denote the regret occured by these agents as $r_2(t)$.
\end{itemize}

Thus, the cumulative expected regret occured up to time $T$ when we let the agent play myopically is bounded above by:
\begin{align}
&E\left[\sum_{t=1}^{T}r(t)\right] \nonumber \\
=&E\left[\sum_{t=1}^{t_{n_{0}}} r(t) + \sum_{t=t_{n_{0}}}^{T}(r_1(t)+r_2(t))\right]  \nonumber \\
\leq & \sum_{n=1}^{n_{0}}NRn + E\left[\sum_{t=t_{n_{0}}}^{T}r_1(t)\right]+ E\left[\sum_{t=t_{n_{0}}}^{T}r_2(t)\right] \nonumber \\
\leq & \sum_{n=1}^{n_{0}}NRn + E\left[\sum_{n=n_{0}}^{T}\sum_{t=t_{n}}^{t_{n+1}}r_1(t)\right]+ E\left[\sum_{t=1}^{T}r_2(t)\right] \nonumber \\
\leq & \sum_{n=1}^{n_{0}}NRn + \sum_{n=n_{0}}^{T}E\left[\sum_{t=t_{n}}^{t_{n+1}}24Nm\exp\left(-\frac{1.8n f(t)^2}{32 m^2\sigma^2}\right) R\right]+ \sum_{t=1}^{T}Mf(t)^2 \nonumber \\
\leq & \sum_{n=1}^{n_{0}}NRn + \sum_{n=n_{0}}^{T}E\left[24Nm\exp\left(-\frac{1.8n f(t_n)^2}{32 m^2\sigma^2}\right)R \times (t_{n+1}-t_{n})\right]+ \sum_{t=1}^{T}Mf(t)^2 \nonumber \\
\leq & \sum_{n=1}^{n_{0}}NRn + \sum_{n=1}^{T}E\left[24Nm\exp\left(-\frac{1.8n f(n)^2}{32 m^2\sigma^2}\right)R\times (t_{n+1}-t_{n})\right]+ \sum_{t=1}^{T}Mf(t)^2 \nonumber \\
\leq & \sum_{n=1}^{n_{0}}NRn + \sum_{n=1}^{T} 24Nm\exp\left(-\frac{1.8n f(n)^2}{32 m^2\sigma^2}\right)\times R \times Nn+ \sum_{t=1}^{T}Mf(t)^2 \nonumber
\end{align}

The expected regret occurred when we incentivize the agents is calculated in Theorem~\ref{rst:budget}. Thus the cumulative regret at time $T$ is bounded above by
\begin{align}
&\sum_{n=1}^{n_{0}}NRn + \sum_{n=1}^{T} 24Nm\exp\left(-\frac{1.8n f(n)^2}{32 m^2\sigma^2}\right)\times R \times Nn+ \sum_{t=1}^{T}Mf(t)^2 \nonumber \\
+ & 24N^2 m \frac{e^{\frac{1.8\delta_{0}}{32m^2\sigma^2}}}{(e^{\frac{1.8\delta_{0}}{32m^2\sigma^2}}-1)^2}+N\left(\max\left\{n_{0},\frac{2}{p}\right\}\right). \nonumber
\end{align}

For a fixed $T$, we only need to minimizing the following two terms since all others are constant:

\begin{align}
\sum_{n=1}^{T} 24Nm\exp\left(-\frac{1.8n f(n)^2}{32 m^2\sigma^2}\right)\times R \times Nn+ \sum_{t=1}^{T}Mf(t)^2.
\end{align}


If we set $f^2(t)=\frac{2\log(T)\times 32m^2\sigma^2}{1.8n}$, then
\begin{align}
&\sum_{n=1}^{T} 24Nm\exp\left(-\frac{1.8n f(n)^2}{32 m^2\sigma^2}\right)\times R \times Nn+ \sum_{t=1}^{T}Mf(t)^2 \nonumber \\ 
\leq & \sum_{n=1}^{T} 24N^2 mnR \exp\left(-2\log(T)\right)  + \frac{64m^2\sigma^2 M\log(T)}{1.8}\sum_{t=1}^{T}\frac{1}{n} \nonumber \\
\leq &  24N^2 m R\frac{T(T-1)}{2T^2}  + 35.56 m^2\sigma^2 M\log(T)(\log(T)+1) \nonumber \\
\leq &  12 N^2 m R  + 35.56 m^2\sigma^2 M\log(T)(\log(T)+1). \nonumber
\end{align}

Thus, the cumulative expected regret is bounded by $O(N^2 m + M m^2\log(T))$.
\end{proof}


\subsection{Special Case}

In this section, we consider a special case of our problem, in which the agent's preference distribution $F(\cdot)$ satisfies $S(\delta)=0$ for $\delta<\delta^{'}$. The proof is similar to the proof of Theorem~\ref{rst:regret}, but for $t$ large enough, $S(f(t))=0$. In this case, with $O(N^3)$ payment budget, we can achieve $O(N^2)$ regret.

\subsection{Practical Issues}
\label{sec:pi}

In our algorithm, we take ``pay whatever it takes'' strategy when we decided to incentivize the agent. In our proof, the only time we used ''pay whatever it takes'' is in Lemma~\ref{round:length}. Without loss of generality, suppose we want to incentivize arm $i$ at time $t$ at the $n^{th}$ round. Based on the proof of Lemma~\ref{round:length}, as long as we offer a payment $c_{i,t}$ such that arm $i$ will have at least $n^{-1}$ probability being pulled at time $t$, our results still hold true. We could compute this $c_{i,t}$ dynamically based on $F(\cdot)$ as well as our current estimate $u_{i,t}$. Here is the revised algorithm which would work well in practice:

\begin{algorithm}
\caption{Algorithm: Incentivizing Exploration Revised}
\label{Alg2}
\begin{algorithmic}
\STATE Set n = 1 to denote the round number;
\FOR{ $t = 1, 2, 3, \cdots$}{
	\IF{the probability of pulling an arm $i$ is less than $n^{-1}$ and no agents have pulled that arm in this round (break ties randomly)}
	\STATE{Offer payment $c_{i,t}=\inf\{c: P(\theta\sim F: \theta\cdot u_{i,t}+c>max_{j}\theta\cdot u_{j,t})>n^{-1}\}$}\ELSE 
		\STATE {let the agents play myopically}\ENDIF
		\STATE $n = min_{i} N(i,t)$
}\ENDFOR

\end{algorithmic}
\end{algorithm}


The same proof would work and we can get the exact same results as ``pay whatever it takes'' strategy. 

\section{Lower Bound $O(\log(T))$}
\label{sec:lb}

In this section, we assume $\theta$ follows a continuous distribution $F(\cdot)$. We provide a example to show the best possible lower bound is $O(N\log(T))$.

Suppose you have two arms. Arm $1$ has utility vector $(0,0)$ and arm $2$ has utility vector $(0,1)$. We assume the users' preference are uniformly distributed on the unit circle. If the user knows the true reward for both arms, then the users with preference on the bottom half circle will choose arm $1$ and the users with preference on the top half circle will choose arm $2$.

Consider the following algorithm: at each step, let the agents play myopically; however, they are going to see the noisy rewards for both arms.

For simplicity, we assume that the agents already know the true utility vector for arm $1$. Without loss of generality, denote $u_{t} = (0,1)+(z_{t1},z_{t,2}) = (0,1)+ (N(0,1/t),N(0,1/t))$ to be the estimate reward for arm $2$ (Without loss of generality, we assume the variance for the noise is $1$). 

At time $t$, suppose we know arm $1$ has utility vector $(0,0)$ and arm $2$ has utility vector $u_t\sim (N(0,\frac{1}{t}), N(1,\frac{1}{t}))$. Since we know $(z_{t,1}, z_{t,2})$ is symmetric around $(0,0)$, we know 

\begin{align}
&E[r_t] \nonumber \\
= &E[r_t | z_{t,1}>0,z_{t,2}>0] \times P(z_{t,1}>0,z_{t,2}>0) + E[r_t |z_{t,1}>0,z_{t,2}<0] \times P(z_{t,1}>0,z_{t,2}<0) \nonumber \\
= &E[r_t | z_{t,1}<0,z_{t,2}>0] \times P(z_{t,1}<0,z_{n,2}>0) + E[r_t |z_{t,1}<0,z_{t,2}<0] \times P(z_{t,1}<0,z_{t,2}<0) \nonumber \\
\geq & 0.25 \times E[r_t | z_{t,1}>0, z_{t,2}>0]. \nonumber
\end{align}

Given $z_{t,1}>0$ and $z_{t,2}>0$, we know the user between $(-1,0)$ and $\left(\frac{-1-z_{t,2}}{\sqrt{z_{t,1}^2+(1+z_{t,2})^2}}, \frac{z_{t,1}}{\sqrt{z_{t,1}^2+(1+z_{t,2})^2}}\right)$ and the user between $(1,0)$ to $\left(\frac{1+z_{t,2}}{\sqrt{z_{t,1}^2+(1+z_{t,2})^2}}, \frac{-z_{t,1}}{\sqrt{z_{t,1}^2+(1+z_{t,2})^2}}\right)$ will make a mistake. And the regret is the absolute value of the second coordinate of the user's preference vector. Thus, we know

\begin{align}
&E[r_t| z_{t,1}>0, z_{t,2}>0] \nonumber \\
=& 4\times 2 \int_{0}^{\infty} \int_{0}^{\infty} \int_{0}^{\arctan\left(\frac{z_{t,1}}{1+z_{t,2}}\right)}\frac{\sin(\theta)}{2\pi}d(\theta)\frac{e^{-\frac{t \times z_{t,1}^2}{2}}\sqrt{t}}{\sqrt{2\pi}}d(z_{t,1})\frac{e^{-\frac{t \times z_{t,2}^2}{2}}\sqrt{t}}{\sqrt{2\pi}}d(z_{t,2}) \nonumber \\
=& \frac{2}{\pi^2}\int_{0}^{\infty} \int_{0}^{\infty}t\times \left[1-\frac{1+z_{t,2}}{\sqrt{z_{t,1}^2+(1+z_{t,2})^2}}\right]e^{-\frac{t \times z_{t,1}^2}{2}}e^{-\frac{t \times z_{t,2}^2}{2}}d(z_{t,1})d(z_{t,2}) \nonumber \\
=& \frac{2}{\pi^2}\int_{0}^{\infty} \int_{0}^{\infty} \left[1-\frac{\sqrt{t}+z_{t,2}}{\sqrt{z_{t,1}^2+(\sqrt{t}+z_{t,2})^2}}\right]e^{-\frac{z_{t,1}^2}{2}}e^{-\frac{z_{t,2}^2}{2}}d(z_{t,1})d(z_{t,2}) \nonumber 
\end{align}

Below, we want to show 
\begin{align}
\lim_{t\rightarrow\infty}\frac{E[r_t| z_{t,1}>0, z_{t,2}>0]}{t} = O(1). \nonumber
\end{align}

Denote $d(t)=t\left[1-\frac{\sqrt{t}+z_{t,2}}{\sqrt{z_{t,1}^2+(\sqrt{t}+z_{t,2})^2}}\right]$.
Since
\begin{align}
d^{'}(t)=\frac{-z_{t,1}^2(2z_{t,2}+3\sqrt{t})-2(z_{t,2}+\sqrt{t})^3}{2(z_{t,1}^2+(z_{t,2}+\sqrt{t})^2)^{3/2}}+1. \nonumber
\end{align}

and $\lim_{t\rightarrow \infty}d^{'}(t)=2$, we know for $n$ large enough, $d(t)$ is a increasing function in terms of t. Thus, based on the Monotone Convergence Theorem, we have

\begin{align}
&\lim_{t\rightarrow \infty}\frac{E[r_t| z_{t,1}>0, z_{t,2}>0]}{t} \nonumber \\
=& \frac{2}{\pi^2}\int_{0}^{\infty} \int_{0}^{\infty}\lim_{t\rightarrow \infty}\left[ \left[1-\frac{\sqrt{t}+z_{t,2}}{\sqrt{z_{t,1}^2+(\sqrt{t}+z_{t,2})^2}}\right]e^{-\frac{z_{t,1}^2}{2}}e^{-\frac{z_{t,2}^2}{2}}\right]d(z_{t,1})d(z_{t,2}) \nonumber  
\end{align}


Based on Wolframalpha \bccomment{still not sure how to compute this by hand}, we know

\begin{align}
\lim_{t\rightarrow \infty} d(t)=\frac{z_{t,1}^2}{2}. \nonumber
\end{align}
Thus,
\begin{align}
&\lim_{t\rightarrow \infty}\frac{E[r_t| z_{t,1}>0, z_{t,2}>0]}{t} \nonumber \\
=&\frac{2}{\pi^2}\int_{0}^{\infty} \int_{0}^{\infty}\lim_{t\rightarrow \infty}\left[ \frac{z_{t,1}^2}{2}e^{-\frac{z_{t,1}^2}{2}}e^{-\frac{z_{t,2}^2}{2}}\right]d(z_{t,1})d(z_{t,2}) = \frac{1}{2\pi}. \nonumber
\end{align}


Thus, the cumulative expected regret is at least $O(\log(T))$.


\section{Conclusion}


\bibliography{reference}{}
\bibliographystyle{plain}


\end{document}

