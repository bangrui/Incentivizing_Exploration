\section{Overview of Results and Discussion}

Our main algorithm is presented as
Algorithm~\ref{alg:basic-incentivizing}
in Section~\ref{sec:ub}.
Our main result is the following pair of theorems%
\footnote{Recall that we omit the dependence on parameters other than
  \ARMNUM, \MinProb, $T$ unless making a particular point.},
analyzing the payments and regret of the algorithm.

\begin{theorem} \label{rst:budget}
The expected total payment of
Algorithm~\ref{alg:basic-incentivizing} is at most
$O \left(\ARMNUM^2 \cdot \e^{2/\MinProb} \right)$.
\end{theorem}

\begin{theorem} \label{rst:regret}
For any time horizon $T$, the expected cumulative regret for
Algorithm~\ref{alg:basic-incentivizing} up to time $T$ is bounded
above by 
$O \left(\ARMNUM \cdot \e^{2/\MinProb} + \TieDensity \ARMNUM \log^3(T) \right)$.
\end{theorem}

When $\TieDensity = 0$, the bound of Theorem~\ref{rst:regret} is
constant in $T$;
thus, the algorithm achieves constant regret for constant payment. 
The case $\TieDensity = 0$ arises, for instance, in the case of a
discrete distribution for agents.
In fact, in the case of a discrete distribution of agents,
it is possible to modify the algorithm to reduce the dependence on
\MinProb from exponential to polynomial.
Theorem~\ref{rst:discrete} (given later) states that the modified
algorithm achieves expected regret
$O \left(\frac{\ARMNUM}{\MinProb^4} \right)$
with expected payments of
$O \left(\frac{\ARMNUM^2}{\MinProb^{5/2}} \right)$.

The fact that constant regret can be achieved with constant payment
(independent of $T$) when $\TieDensity = 0$ suggests aiming for a
constant bound more generally, i.e., for $\TieDensity > 0$.
That such a bound is unachievable is shown in Section~\ref{sec:lb},
where we show a lower bound of $\Omega(\log(T))$ on
the expected regret of any algorithm.
The instance is very simple: it has two arms, one of which has known
attributes; in addition, one draw from the other arm is observed in
each step $t$ even when it is not pulled.
While the probability of pulling the wrong arm decreases over time, it
does not do so fast enough, causing the stated regret.

The exponential dependence on $1/\MinProb$ implies an exponential
dependence on \ARMNUM (because $\MinProb \leq 1/\ARMNUM$).
This exponential dependence arises from a need to continue to
incentivize arm pulls to ensure that nearly tied agents learn their
best arms quickly.
Aside from the assumption that $\TieDensity = 0$, another assumption
allows us to eliminate this exponential dependency.
Namely, when \MinProb (or a lower bound on it) is known ahead of time,
the algorithm can be modified to incentivize arms less aggressively.
As shown in Theorem~\ref{rst:known-p},
the modified algorithm will have expected regret at most
$O \left(\frac{\ARMNUM}{\MinProb^3}
+\frac{\ARMNUM \TieDensity \log^3(T)}{\MinProb} \right)$,
with expected payments of at most
$O \left(\frac{\ARMNUM^2}{\MinProb^{5/2}} \right)$.

We compare these bounds to those for standard bandits,
focusing on the dependence on $T$.  
The standard bandit setting is the case when the agent types \AgV
are concentrated on a single point, and agents pull arms at the
principal's direction without requiring payment.
(Notice that this setting violates our Assumption~\ref{A3},
so out bounds do not apply to it.)
Then, the payment is $0$ and the expected regret scales as
$\Theta(\log(T))$ \cite[Theorem 2.1]{bubeck2012regret}.

Our algorithm's payment is constant in $T$,
while its regret is $O(\log^3(T))$ in general with a lower bound of
$\Omega(\log(T))$;
when preferences are discrete, our algorithm's regret is
constant in $T$.
Thus, viewed solely in terms of the dependence on $T$,
the best performance achievable seems comparable to that in a standard
multi-armed bandit problem;
but when preferences our discrete, the constant regret (rather than
$\Theta(\log T)$ surpasses a standard multi-armed bandit.
This may seem surprising, because our setting is one in which the
principal has both less control and less information than in the
standard bandit setting.
The result arises because heterogeneity in preferences indeed provides
free exploration, in which all of the arms can be pulled infinitely
often without incurring regret.

While heterogeneity in preferences enables this free exploration,
heterogeneity alone is not always sufficient for enabling performance
improvements compare to the standard bandit setting.
Indeed, consider a version in which agents are still heterogeneous,
but the principal has the power to pull arms directly.
Unless the heterogeneity can be observed (i.e., the principal knows
the agents' types), the principal will be unable to correctly choose
each agent's preferred arm,
even with infinite exploration of arm attributes.
Hence, in this setting, the regret will grow as $\Omega(T)$. 

Thus, reaping the benefits of (unobserved) heterogeneous preferences
requires the principal to give up direct control of the arms,
providing agents the autonomy they need to express their private
information about their own preferences.
Our results show that simple arm-based incentives are sufficient
to overcome the apparent challenges created by this abdication of
control.
