\section{Overview of Results and Discussion}

\dkcomment{In the results discussion clarify which results need to
  know $T$ and which don't.}

%
% State the main theorem(s) formally.
%

We will show below that our algorithm (described formally below as Algorithm~\ref{alg:basic-incentivizing}) achieves $O \left(\MAXR\cdot  N\exp(\frac{2}{p})\right) + O(\ARMNUM\Diam^2 d^2\sigma^2 \TieDensity(\log(T))^3)$ cumulative regret with $O \left(\ARMNUM^2 \MAXR \Diam^2 \TieDensity d^3 \sigma\cdot \exp(\frac{2}{\MinProb}) \right)$ payment budget.  We state these results formally, which together constitute our main results.

%\begin{theorem}
%The payment budget for Algorithm~\ref{alg:basic-incentivizing} is bounded above by 
%\begin{align}
%O \left(\ARMNUM^2 \MAXR \Diam^2 \TieDensity d^3 \sigma\cdot \exp(\frac{2}{\MinProb}) \right). \nonumber 
%\end{align} 
%\label{rst:budget}
%\end{theorem}

%\begin{theorem}
%For any given $T$, the cumulative regret for Algorithm~\ref{alg:basic-incentivizing}
%is bounded above by 
%\begin{align}
%O \left(\MAXR\cdot  N\exp(\frac{2}{p})\right) +
%O(\ARMNUM\Diam^2 d^2\sigma^2 \TieDensity(\log(T))^3).
%\end{align}
%\label{rst:regret}
%\end{theorem}

% Discuss lower bounds or impossibility results.
We also show a lower bound of $\Omega(\log(T))$ (section~\ref{sec:lb}) on the regret of any algorithm when agents have a density. 

In the specific case of discrete distributions over agent preferences, in which $L=0$ \pfcomment{check whether this is the right description}, we obtain tighter bounds:

%Discuss extensions.

% Discuss the justification or comparison to standard bandits.
We compare these results to those possible for standard bandits. In standard bandits, the payment is obviously $0$, and the regret scales as $N \sqrt{T}$ \pfcomment{add cite}.

% Discuss the comparison in the 2x2 square.

\pfcomment{Below this is old text}

As we discuss in more detail in Section~\ref{sec:main-discussion},
the reason we obtain stronger regret bounds than in the standard bandit
setting is that the agents' heterogeneity in preferences will
naturally lead to exploration.

Here we pursue an alternate direction to our analysis of incentivizing exploration with heterogeneous preferences: we consider approximation-style results comparing achievable algorithm performance across four different problem settings, described in the table below.

\begin{center}
\begin{tabular}{ c|c|c| } 
 \hline
     & Pulls arms directly & Needs to incentivize \\ 
\hline
 Perfect Info on Preferences & $C11$: The ``God'' Policy & $C12$ \\ 
 Partial Info on Preferences & $C21$ & $C22$: actual algorithm \\ 
 \hline
\end{tabular}
\end{center}

Columns indicate whether the principal pulls arms directly (left column) or incentivizes agents who in turn actually pull the arms. Rows indicate whether the principal knows the preferences of the agents (top row) or not (bottom row).   Problem settings correspond to cells and are described by the names C11, C12, and so on. An optimal algorithm for problem setting C11 has the best performance possible.

We wish to make two high-level points through this analysis:

\begin{itemize}
\item First, better performance is possible in setting C22 than in setting C21, in the sense we can find tight approximation guarantees relative to C11 that are better for C22 than for C21.  This is achieved by setting up incentives in C22 that still allow agents to select arms aligned with their preferences, providing selections of arms that contain more information than in C21.
\item Second, heterogeneity in preferences improves performance over what can be achieved in a setting with homogeneous preferences.  This would be in the spirit of David K.'s results that heterogeneity in preference for money can be exploited to reduce the budget required.
\end{itemize}



Section~\ref{sec:lb} constructs an example showing regret is $\Omega(\log(T))$ in the worst case, regardless of incentive budget.

