\subsection{Bounding the Total Regret}
In bounding the total regret, because agents' types are from a compact
set by Assumption~\ref{A2}, the maximum regret in any one round is
bounded by a constant.
We use $\MAXR = \max_{\AgV, i,i'} \AgV \cdot (\ArmV{i} - \ArmV{i'})$
to denote this constant upper bound on the maximum regret that can be
incurred in any one time step. 


\begin{emptyextraproof}{Theorem~\ref{rst:regret}}
Regret can arise in two ways:
(1) an agent was incentivized to pull a suboptimal arm, or
(2) an agent myopically pulled a suboptimal arm.
By Lemma~\ref{lem:numP}, Algorithm~\ref{alg:basic-incentivizing}
incentivizes agents at most 
$O\left( N\exp\left(\frac{2}{p}\right) \right)$
times in expectation, each time causing regret at most \MAXR,
for a total expected regret of at most
$O\left(\MAXR\cdot N\exp\left(\frac{2}{p}\right) \right)$.
For the rest of the proof, we focus on the regret incurred when agents
pull arms myopically and make mistakes.

We distinguish between agents incurring large regret
(which requires severe misestimates of arm locations;
such misestimates are exponentially unlikely to occur), 
and agents incurring small positive regret,
which requires these agents to be almost tied in their preference for
the best arm.
To be more precise, we define (with foresight) a phase-dependent
cutoff
$g(s) = \sqrt{\frac{128\log(T) \cdot \Diam^2 d^2 \sigma^2}{1.8 s}}$,
and consider a regret exceeding $g(s)$ large.
\pfcomment{We use $g$ in 3 places: here, when talking about adding new linear
features in section 2, and in Lemma 11.  Maybe it's worth using different notation.}

For most of the proof, we will focus on the case $g(s) \leq \TieCutoff$.
The remaining phases are the ones with
$s \leq \frac{128}{1.8} \cdot \log(T) \cdot \Diam^2 d^2 \sigma^2/\TieCutoff^2$.
Each such phase lasts for at most $\ARMNUM \cdot \log(s)$ steps
in expectation by Lemma~\ref{lem:phase-length},
and each step incurs regret at most \MAXR.
The total expected regret incurred in these phases is at most
$100 \ARMNUM \MAXR \log^2(T) \cdot \Diam^3 d^3 \sigma^3/\TieCutoff^3$.
\pfcomment{I get the $NR$, but where does $100 \log^2(T)  D^3 d^3 \sigma^3/\hat{z}^3$ come from?  I assume this an identity for $\sum_{s=1}^m \log(s)$ I don't know --- could we add the identity we are using?}

For the remainder of the proof, we consider phases $s$
with $g(s) \leq \TieCutoff$.
We first consider agents \AgV incurring positive regret less than $g(s)$.
The second-best arm \Second{\AgV} of such an agent \AgV must satisfy
$\AgV \cdot (\ArmV{\Best{\AgV}} - \ArmV{\Second{\AgV}}) \leq g(s)$.
By Assumption~\ref{A1}, and because $g(s) \leq \TieCutoff$,
%Lemma~\ref{lem:sdelta},
the total measure of such agents
is $\AlmostTied{g(s)} \leq \TieDensity \cdot g(s)$,
and each such agent incurs regret at most $g(s)$.
Summing over all time steps,
and using Lemma~\ref{lem:phase-length} to bound the number of time
steps in phase $s$, 
\pfcomment{We'll have to fix a probability bug here.}
the expected total regret from such agents is at most
\begin{align}
\Expect{\sum_{s=1}^{T} \sum_{t=t_{s-1}+1}^{t_{s}} \TieDensity \cdot g^2(s)}
& \leq \sum_{s=1}^{T} \TieDensity \cdot g^2(s) \ARMNUM \log(s).
\label{equ:small_regret_bound}
\end{align}

We next bound the regret incurred in time steps with large regret.
Let the random variable $t_s$ be the final time step in phase $s$,
for each $s$.
\dkcomment{We have already used $t_s$ before, e.g., in the preceding
  formula. Do we need to redefine it here?}
\pfcomment{Above we define $t_s$ to be the start time of phase $s$.}
\dkcomment{So the new $t_s$ is the old $t_{s+1}-1$? That's a really
  confusing subtle notation change. Is there a way we can avoid this
  and just decide on one or the other, and use it consistently?}
Define the stopping times $\tau_{s}^{k}$ to be the \Kth{k}
time step in the \Kth{s} phase,
with $\tau_{s}^{k} = \infty$ when phase $s$ has fewer than $k$ steps,
i.e., $k > t_{s}-t_{s-1}$.
By Lemma~\ref{lem:right-choice},
under \AccEU{\tau_s^k}{\frac{g(s)}{2\Diam d}}, no agent at time $t$ will incur
regret more than $g(s)$.
\dkcomment{Instead of the extra case distinction, I had thought of
  defining $g(s)$ to be the minimum of the current definition and
  \TieCutoff. Unfortunately, I think that we cannot necessarily apply
  Lemma~\ref{lem:right-choice} with that definition. Just putting that
  here in case you are thinking of simplifying the proof this way.}
  
To bound the probability of \AccEU{\tau_s^k}{\frac{g(s)}{2\Diam d}},
we use Lemma~\ref{lem:round-prob}
(with $x_s = x'_s = \frac{g(s)}{2 \Diam d}$
and a stopping time of $\tau_s^k$).
We first verify that this choice of $x_s, x'_s$
satisfies the assumption of Lemma~\ref{lem:round-prob} for all
$s \geq 2$,
i.e., that
$\sqrt{0.6 n \cdot \log (\log_{1.1}(n) + 1) + \frac{n x_n^2}{16 \sigma^2}}
\leq \frac{n x_n}{2 \sigma}$
for all $n \geq 2$.
Substituting that
$x_n = \frac{g(n)}{2 \Diam d} = \sqrt{\frac{32\log(T) \cdot \sigma^2}{1.8 n}}$,
the left-hand side is

\[
  \sqrt{0.6 n \cdot \log (\log_{1.1}(n) + 1) + \frac{2 \log T}{1.8}}
  \; \leq \;
  \sqrt{0.6 n \cdot \log (\log_{1.1}(n) + 1) + \frac{2 n \log T}{1.8}},
\]
while the right-hand side is $\sqrt{\frac{8 n \log(T)}{1.8}}$.
Squaring both sides and canceling out common terms,
the desired inequality is equivalent to
$\frac{50}{9} \log T \geq \log(\log_{1.1}(n) + 1)$.
Because $n \leq T$, it is sufficient to show that
$\frac{50}{9} \log T \geq \log(\log_{1.1}(T) + 1)$,
which can be verified by numerical calculation for $T=2$ and a
derivative test.

Now, by Lemma~\ref{lem:round-prob} and a union bound over all arms $i$
and attributes $j$, we get that 
\begin{align*}
\Prob{\AccEU{\tau_s^k}{x_s}}
& \leq 24 \ARMNUM d \cdot \exp \left( \frac{-1.8 s x^2}{16 \sigma^2} \right)
\; = \; \frac{24 \ARMNUM d}{T^2}.
\end{align*}

In the low-probability event, we simply 
bound the maximum regret by \MAXR.
Now, applying Lemma~\ref{lem:phase-length},
the total expected regret from large-regret steps is at most

\begin{align*}
\sum_{s = 1}^T \sum_{t=t_s}^{t_{s+1}-1}
  \Prob{\mbox{incur regret exceeding } g(s) \mbox{ at step } t} \cdot \MAXR
& \leq
 \sum_{s = 1}^T \frac{24 \ARMNUM d}{T^2} \cdot \MAXR \cdot \ARMNUM \log(s)
\\ & \leq 12 \ARMNUM^2 d \MAXR.
\end{align*}

Adding all \dkedit{four} types of regret terms,
and substituting that 
$g^2(s) = \frac{128\log(T) \cdot \Diam^2 d^2 \sigma^2}{1.8 s}$,
the cumulative regret up to time $T$ is at most

\begin{align*}
\lefteqn{O \left(\ARMNUM \MAXR \cdot \exp \left( \frac{2}{p} \right)
\dkedit{+ \ARMNUM \MAXR \log^2(T)}
+ \ARMNUM^2 \MAXR d
+ \ARMNUM \TieDensity \cdot \sum_{s=1}^{T} g^2(s) \log(s) \right)}
\\ & = 
O \left(\ARMNUM  \cdot \exp \left( \frac{2}{p} \right)
+ \ARMNUM \TieDensity \cdot \log^3(T) \right).\QED
\end{align*}
\end{emptyextraproof}
