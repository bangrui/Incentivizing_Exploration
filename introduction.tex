\section{Introduction}

A large number of sites are designed to facilitate joint discovery,
sharing, and recommendations of content by large numbers of users.
Such sites include news, photo, or video sharing sites,
sites to review restaurants, hotels, or travel experiences,
online stores at which users write reviews (such as Amazon),
and citizen science projects
(such as eBird \citep{sullivan2009ebird} or Galaxy Zoo).
\dkcomment{Insert citations.}
By learning from the experiences of other users, individuals can make
better choices to improve their own experience.

Viewed more abstractly, the users are jointly exploring a space of
many options (products, news stories, photos, birdwatching sites,
patches of sky to train their telescopes on, $\ldots$),
with the implicit goal of identifying the ``best'' ones.
Therefore, such scenarios can be fruitfully modeled in a bandit
learning framework.
However, contrary to standard bandit settings, the utilities of the
decision makers (the users) are not aligned with the overall utility.
Societally (i.e., in a suitable aggregate over all users),
it would be desirable to engage in considerable exploration of
different options, so as to provide higher rewards for a large number
of future users.
However, individual users only interact with the site a limited number
of times, and therefore have little incentive for exploration.
A particular clean model is obtained when each user interacts with the
site only once, and hence has no intrinsic utility for exploration. 
This model has been the subject of prior work, and forms the basis of
the present submission.

In order to effect an outcome which is close to societally optimal,
it is necessary to provide exploration incentives to the individual users.
This was noted in two recent lines of work:
\citet{kremer2014implementing}
and \citet{mansour2015bayesian,mansour2016bayesian}
assume that the site (also called the \emph{principal}) has an
informational advantage in being the only one to observe the results
of past arm pulls.
(Such an assumption applies, for instance, to route recommendations in
driving.)
The principal can exploit this advantage and make recommendations to
the individual agents which it is in their best interest to follow.
\citet{frazier2014incentivizing} and 
\citet{han2015incentivizing} instead assume that the results of all
past arm pulls are publicly observable
(such as reviews on an online retail site).
They instead allow payments which the principal can offer to users as
a reward for pulling particular arms.
We follow the model of \citet{frazier2014incentivizing,han2015incentivizing}
and consider a multi-armed bandit model in which the principal can
offer payments to the users for pulling particular arms.

Past work has assumed that users are homogeneous, i.e., the expected
reward a user derives from an arm is the same for all users.%
\footnote{\citet{han2015incentivizing} assume that users are
heterogeneous in their tradeoff between utility derived from arm pulls
and utility derived from the principal's payment.}
In reality, users will have different preferences, e.g., gastronomic,
political, aesthetic, practical, etc.,
and the preferences can typically not be observed by the principal.
This heterogeneity presents both a challenge and an opportunity.
On the one hand, it necessitates the exploration of more arms,
so as to let all users discover their best option.
On the other hand, the heterogeneity also provides exploration
``for free,'' as users will not simply herd on one ``best'' arm.
\emph{Our goal is to explore the impact of user heterogeneity
on the tradeoff between societal utility and the principal's payments.}

At a high level, we model the setting as follows.
(Formal definitions are given in Section~\ref{sec:prob}.)
Arms and users (or \emph{agents}) are characterized by payoff-relevant
\emph{attribute} (or \emph{feature}) vectors,
both drawn from known distributions.
An agent's reward from pulling an arm is the inner product of their
vectors (plus noise).
When an arm is pulled, a noisy version of its attribute vector is
observed by everyone.
Agents are myopic and will pull the arm whose expected attribute
vector (based on past noisy observations) maximizes their reward.
The principal can incentivize agents to pull particular arms by
offering rewards for the specific arm.
The principal's goal is to keep the cumulative regret across all
agents small, while incurring only small total payments.
Our main theorm can be stated informally as follows:

\begin{theorem} \label{thm:main-intro}
Assume that for each arm, at least a constant fraction of the
population likes this arm best.
If furthermore, the density of ties in agent preferences between arms
vanishes (in a sense made precise in Section~\ref{sec:ub}),
there is a policy that achieves constant
expected regret $C$ and constant%
\footnote{The constants $C,C'$ depend on the number of arms and the
  smallest fraction preferring any one arm.} expected payment $C'$.
If near-ties have non-zero density,
then the regret is bounded by $O(C + C'' \log^2(T))$,
while the expected payment is still bounded by $C'$;
here, $C''$ depends on the ``tie density.''
\end{theorem}

The policy achieving the result of Theorem~\ref{thm:main-intro} is
quite simple: it mostly lets agents exploit arms, but incentivizes
them to explore when arms that appear close enough to being tied are
underexplored. It is presented in detail in Section~\ref{sec:ub}.
The result can be smoothly extended when there some arms that are not
preferred by any agents.

\dkcomment{I strongly dislike the ``Table of contents'' paragraph, and
  much prefer weaving pointers into the main text.}