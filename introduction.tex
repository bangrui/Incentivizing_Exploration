\section{Introduction}

Here we pursue an alternate direction to our analysis of incentivizing exploration with heterogeneous preferences: we consider approximation-style results comparing achievable algorithm performance across four different problem settings, described in the table below.

\begin{center}
\begin{tabular}{ c|c|c| } 
 \hline
     & Pulls arms directly & Needs to incentivize \\ 
\hline
 Perfect Info on Preferences & $C11$: The ``God'' Policy & $C12$ \\ 
 Partial Info on Preferences & $C21$ & $C22$: actual algorithm \\ 
 \hline
\end{tabular}
\end{center}

Columns indicate whether the principal pulls arms directly (left column) or incentivizes agents who in turn actually pull the arms. Rows indicate whether the principal knows the preferences of the agents (top row) or not (bottom row).   Problem settings correspond to cells and are described by the names C11, C12, and so on. An optimal algorithm for problem setting C11 has the best performance possible.

We wish to make two high-level points through this analysis:

\begin{itemize}
\item First, better performance is possible in setting C22 than in setting C21, in the sense we can find tight approximation guarantees relative to C11 that are better for C22 than for C21.  This is achieved by setting up incentives in C22 that still allow agents to select arms aligned with their preferences, providing selections of arms that contain more information than in C21.
\item Second, heterogeneity in preferences improves performance over what can be achieved in a setting with homogeneous preferences.  This would be in the spirit of David K.'s results that heterogeneity in preference for money can be exploited to reduce the budget required.
\end{itemize}





In this paper, we study incentivizing exploration with heterogeneous agent preferences.
In this problem,   
arms have unknown multivariate attributes, and agents have heterogeneous linear utility functions that map these attribute vectors onto utilities. Agents see noisy observations of attributes of arms pulled by all previous agents, and estimate an arms' attribute vector by the simple average of these observations.  Agents are selfish, and pull the arm with the largest estimated utility summed with an optional arm-specific incentivizing payment chosen by the principle.
We study strategies for choosing such incentive payments that seek to maximize the total utility derived by agents, subject to a limitation on the total incentive payment.  To accomplish this goal, a strategy must induce sufficient exploration to reveal arms' attributes, while still letting agents select myopically and according to their preferences sufficiently often that high-utility arms are chosen and incentive payments are kept small.

Our problem setting models online review aggregators like Amazon, Yelp, and Tripadvisor that host crowdsourced reviews.  Users of these websites wish to use the reviews hosted there to choose the product / restaurant / vacation (generically referred to as an ``item'') that is best according to their preferences.  These reviews provide not just cardinal feedback, but also a description of items' attributes that a user may consider together with their personal preferences to select their preferred item.  An item with few reviews might have inaccurate attribute estimates, leading users to avoid it even though it may actually be their best choice.  Without incentives, this situation may persist and decrease welfare for the platform's user base.  By offering incentives, either through price reductions by Amazon or coupons from Yelp or Tripadvisor, a platform may induce more reviews of unexplored items and provide more value over the long term.

Our problem setting also applies to crowd science platforms like eBird \cite{frazier2014incentivizing, sullivan2009ebird}. EBird guides birding enthusiasts through a website to explore and report their findings to the birding community. Each user report contains information about when, where and how they go birding and what birds they see and hear. EBird may wish to incentivize enthusiasts to explore less-explored birding locations and provide more accurate reports on these locations. Each enthusiast may have different preference over a location's attributes such as the diversity of bird species, weather, distance and safety. By offering enthusiasts incentivies to explore, eBird can create a more accurate body of reports and provide better value to the birding community.

% Something interesting to study would be to alter the search rankings to influence people
% Also which items to spam people about reviewing

In this problem context, we study a simple policy that usually exploits, incentivizing agents to pull an arm only when the set of agent utility functions that would pull this arm without incentives has probability below a time-varying threshold. In our paper, we assume all arms are some agents' best arm. Under this assumption, we prove that with $O(N^2)$ payment budget, this policy has $O(N^2+M(\log(T))^2)$ cumulative expected regret where $M$ is an upper bound on the limiting marginal probability density of agent utilities that are nearly indifferent between their best and the second best arm. If all agents' utility difference between their best and second best arm is bounded below by a positive number, which typically happens when the agent utility distribution is discrete, this policy achieve constant cumulative expected regret $O(N^2)$. The key difference between our problem setting and both the homogenous preference setting and the traditional multi-armed bandits setting is that we must incentivize agents to try suboptimal arms much less often, since all arms are some agents' best arm.  Essentially, heterogeneity provides free exploration.  These results suggest that heterogeneous agent preferences reduce but do not eliminate the need to incentive exploration, in relation to single-preference settings. 

We broadly categorize the relevant previous literature into two categories based on whether there is money transfer.
With money transfer, \cite{frazier2014incentivizing} considers a problem setting where the principal pays agents money to explore. This work assumes all agents have equal value for money and provide a complete characterization of achievable reward with a fixed budget. \cite{han2015incentivizing} generalizes this framework to include agents with heterogeneous value for money, and to allow an external signal to provide partial information about this valuation. Under this setting, this work proves a bound on achievable reward as a function of the budget and the signal scheme.

Without money transfer but using information asymmetry, \cite{kremer2014implementing} considers a simple model where agents arrive to the principal one by one and there are only two actions at each time. \cite{mansour2015bayesian} generalizes \cite{kremer2014implementing} by allowing a finite number of actions at each time. \cite{mansour2016bayesian} considers a problem setting where there are multiple agents at each time and agents may interact with each other. In these papers, the principal provides each agent a recommendation at each time that is Bayesian incentive-compatible. They prove the principal can achieve constant regret when utilities are deterministic and logarithmic regret when utilities are stochastic.



We structure our paper as follows: Section~\ref{sec:prob} formulates our problem; Section~\ref{sec:ub} states our algorithm and proves that we can achieve $O(N^2+M(\log(T))^2)$ regret with $O(N^2)$ incentive budget; Section~\ref{sec:lb} constructs an example showing regret is $\Omega(\log(T))$ in the worst case, regardless of incentive budget.

