\section{Introduction}

Many websites and apps are designed to facilitate joint discovery,
sharing, and recommendations of content.
Such sites include news, photo, and video sharing sites,
sites to review restaurants, hotels, or travel experiences,
online stores at which users write reviews (such as Amazon),
and citizen science projects
(such as eBird \citep{sullivan2009ebird,xue-ebird} or Galaxy Zoo \citep{lintott-galaxy-zoo}).
By learning from the experiences of other users, individuals can
improve their own experience \citep{schmit2017human}.

Viewed more abstractly, users jointly explore a space of
many options (products, news stories, photos, birdwatching sites,
patches of sky to train their telescopes on, $\ldots$),
with the implicit goal of identifying the ``best'' ones.
Therefore, such scenarios can be fruitfully modeled in a bandit
learning framework.
However, contrary to standard bandit settings, the utilities of the
decision makers (the users) are not aligned with the overall utility.
Societally (i.e., in a suitable aggregate over all users),
it would be desirable to engage in considerable exploration of
different options, so as to provide higher rewards for a large number
of future users.
However, individual users only interact with the site a limited number
of times, and therefore have little incentive for exploration.
A particularly clean model is obtained when each user interacts with the
site only once, and hence has no intrinsic utility for exploration. 
This model has been the subject of prior work, and forms the basis of
the present submission.

To effect an outcome close to societally optimal,
it is necessary to provide exploration incentives to the individual users.
This was noted in two recent lines of work:
\citet{kremer2014implementing}
and \citet{mansour2015bayesian,mansour2016bayesian}
assume that the site (also called the \emph{principal}) has an
informational advantage in being the only one to observe the results
of past arm pulls.
(Such an assumption applies, for instance, to route recommendations in
driving.)
The principal can exploit her%
\footnote{We use male pronouns to refer to users and female pronouns
  to refer to the principal.}
advantage and make recommendations to the individual agents that 
are in their best interest to follow.
\citet{frazier2014incentivizing} and 
\citet{han2015incentivizing} instead assume that the results of all
past arm pulls are publicly observable
(such as reviews on an online retail site).
They instead suppose the principal can offer payments to users as
a reward for pulling particular arms.
We follow the model of \citet{frazier2014incentivizing} and
\citet{han2015incentivizing} 
and consider a multi-armed bandit model in which the principal can
offer payments to the users for pulling particular arms.

Past work has assumed that users are homogeneous, i.e., the expected
reward a user derives from an arm is the same for all users.%
\footnote{\citet{han2015incentivizing} assume that users are
heterogeneous in their tradeoff between utility derived from arm pulls
and utility derived from the principal's payment.}
In reality, users have different preferences, e.g., gastronomic,
political, aesthetic, practical, etc.
Indeed, the websites and mobile apps most widely used for joint discovery,
sharing, and recommendation of content tend to concern products and items 
with large amounts of heterogeneity in preferences (movies, restaurants,
videos, travel experiences), and not items which have a universally
agreed-on best order.
This is perhaps because regimes with heterogeneous preferences are the
ones where discovery of the best items is the most difficult for people, 
and thus where online
platforms tend to provide the greatest value.  Thus, we see an appropriate
accounting for heterogeneity as critical to an understanding of incentivizing
exploration in online communities.

Heterogeneity presents both a challenge and an opportunity.
On the one hand, unobserved heterogeneity hides critical
information about an agent's preferences from the principal.
On the other hand, heterogeneity also presents her an opportunity,
through the possibility of ``free exploration.''
Even when left unincentivized, agents will
play a variety of arms, revealing information about their attributes.
This stands in sharp contrast to the case of homogeneous preferences,
where unincentivized agents will herd onto a single apparently best arm;
thus, effecting essentially any exploration at all requires incentives.

In order to take advantage of unobserved heterogeneity,
the principal has to give up some control, allowing the agent to
reveal his preferences through action, rather than obscuring them with
incentives.
However, she cannot give up control completely,
and must use incentives to force agents to explore against their
preferences, for the greater good. 

With these challenges and opportunities in mind, our goal is to understand the
impact of user heterogeneity on the principal's ability to achieve
high social utility with low incentive payments,
and on the best approaches for doing so.
We wish to understand whether incentivizing exploration with
heterogeneous preferences  is ``harder'' or ``easier'' than with
homogeneous ones,
and to understand how exploration strategies that do well in the
heterogeneous preference setting differ from those in the homogeneous one.

Toward that end, we model our setting as follows.
(We describe our model at a high level here,
with formal definitions given in Section~\ref{sec:prob}.)
The \ARMNUM arms and users (or \emph{agents})
are characterized by payoff-relevant
\emph{attribute} (or \emph{feature}) vectors.
Arms' attributes are a priori unknown,
and agents' attributes are drawn from a known distribution.
An agent's reward from pulling an arm is the inner product of his
vector with the arm's vector (plus noise).
When an arm is pulled, a noisy version of its attribute vector is
observed by everyone.
\pfedit{This models full-text product reviews on websites like Amazon, or ratings of ``service'', ``value'', and other restaurant attributes on websites like Tripadvisor.  }
Agents are myopic and will pull the arm whose expected attribute
vector (based on past noisy observations) maximizes their reward.
\pfedit{Agents' ability to effectively base decisions on all past observations models platforms' abilities to aggregate and effectively display past feedback, through simple averages of ratings and automatic summarization of full-text reviews \citep{wang2010product,liu2012movie,abulaish2009feature}.  While platforms can manipulate the display of user data, most have an incentive to be seen as truthful.}
The principal can incentivize agents to pull particular arms by
offering rewards for the specific arm.
The principal's goal is to keep the cumulative regret across all
agents small, while incurring only small total payments.
Our main theorem can be stated informally as follows:

%One example where agents give noisy observations of items’ feature vectors is restaurant reviews on TripAdvisor.  Users provide not just an overall rating, but also separate ratings for Service, Value, Food and Atmosphere.  We may treat these as our feature vector.  Platforms collect similar explicit feature vector observations whenever their users rate not just satisfaction with an item but the more granular components that contribute to it.

%As another example, consider websites like Amazon and Yelp whose users provide full-text reviews of items.  Reviews on Yelp describe a restaurants’ attributes (service, value, etc.), and reviews on Amazon similarly describe products’ attributes.  Thus, we can think of a user reading past reviews as aggregating past users’ observations of items’ feature vectors.  In this way, platforms with full-text reviews implicitly provide feature vector observations.

%As a third example, a platform with full-text reviews could run an NLP algorithm to extract features and sentiments from reviews, and then surface them to users as rated attributes.


\begin{theorem} \label{thm:main-intro}
Assume that for each arm, at least a constant fraction \MinProb
of the population likes this arm best,
and let \TieDensity be a measure of the ``density of near-ties''
between agents' arm preferences
(in a sense made precise in Section~\ref{subsec:discrete}).
There is a policy that achieves expected 
cumulative regret $O (\ARMNUM \e^{2/\MinProb} + \ARMNUM \log^3(T))$,
using expected cumulative payments of $O(\ARMNUM^2 \e^{2/\MinProb})$.
In particular, when agents who are close to tied between two arms have measure $0$,
both the expected regret and expected payment are bounded by constants
(with respect to $T$). 
\end{theorem}

The policy achieving the result of Theorem~\ref{thm:main-intro} is
quite simple: it mostly lets agents exploit arms, but incentivizes
them to explore when arms appear unlikely to be pulled without incentives.
It is presented in detail in Section~\ref{sec:ub}.
