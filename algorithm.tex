\section{Main Algorithm and Analysis}
\label{sec:ub}

The algorithm achieving the claimed bounds of
Theorem~\ref{thm:main-overview} is fairly simple.
It mostly allows agents to exploit, but when an arm is \pfdelete{estimated to
be extremely} \pfedit{sufficiently} unlikely to be pulled\pfdelete{by any agents},
the algorithm incentivizes this arm with a payment high enough
to guarantee the next agent pulls it.
This way the algorithm ensures each arm is pulled often enough.

More precisely, the algorithm divides time into \emph{phases}
$s = 1, 2, 3, \ldots$.
Phase $s$ \pfcomment{starts?} ends when each arm has been pulled at least $s$ times.
We indicate the start time of phase $s$ by $t_s$.
\pfcomment{According to the notation we had before, in the .tex file below here,
phase $s$ starts when I have $s$ pulls of each arm.  Also, we should make sure things are clear regarding the free pull at time $0$.}
An arm $i$ is \emph{payment-eligible} at time $t$ (in phase $s$)
if both of the following hold:
\pfdelete{
We call time $t_{s}=\min_{t}\{\forall i, \NumPull{t}{i} \geq s\}$ the
\emph{starting point of the $s^{th}$ phase}.
We call the set of times $[t_{s}, t_{s+1})$ the
\emph{$n^{th}$ phase}.
}

\begin{itemize}
\item $i$ has been pulled at most%
\footnote{in fact: exactly, since the algorithm entered phase $s$}
$s-1$ times up to time $t$, i.e., $\NumPull{t}{s} \leq s-1$.
\item Based on the current estimates \ArmEV{t}{i'} of all arms'
attribute vectors, the probability of pulling arm $i$ is less than $1/\log(s)$. \bccomment{Do we need to mention what if $\frac{1}{\log(s)}>1$? }
Let $\PullProb{t}{i} = \Prob[\AgV \sim \AgentDist]{\AgV \cdot \ArmEV{t}{i} > \AgV
  \cdot \ArmEV{t}{i'} \mbox{ for all } i' \neq i}$
be the probability that arm $i$ will be pulled
by the next (random) agent based on the current estimates. 
Then, the second criterion for payment-eligibility is that
$\PullProb{t}{i} < 1/\log(s)$.
\pfedit{$1/\log(1)$ is treated as $\infty$, and if $1/\log(s) \ge 1$ then this condition is automatically met.}
\end{itemize}

When multiple arms are payment-eligible, the algorithm picks one
uniformly at random to incentivize.
When the algorithm decides to incentivize an arm $i$,
it offers ``whatever it takes,'' i.e., offers a payment of
$\Pay{t}{i} = \max_{\AgV,i'} \AgV \cdot (\ArmEV{t}{i'} - \ArmEV{t}{i})$.
The maximum for \AgV is taken over the support of \AgentDist;
recall that we assumed this support to be compact.
The payment \Pay{t}{i} may appear unnecessarily high.
Indeed, we show in Section~\ref{sec:pi} that it suffices to
incentivize only a $1/\log(s)$ fraction of the agents.
\dkcomment{Given that we don't prove anything there, is ``show'' the
  right word?}
Here, we focus on the higher payments for simplicity of analysis.

Notice that \PullProb{t}{i} depends on the estimates for \emph{all}
arms; thus, by pulling some other arm $i'$, an arm $i$ may become
payment-eligible, or cease to be so.
Algorithm~\ref{alg:basic-incentivizing} gives the full details.


\begin{algorithm}
\caption{Algorithm: Incentivizing Exploration \label{alg:basic-incentivizing}}
\begin{algorithmic}
\STATE Set the current phase number $s = 1$.
\FOR{time steps $t = 1, 2, 3, \ldots$}
\IF{$\NumPull{t}{i} \geq s$ for all arms $i$}
\STATE Increment the phase $s = s + 1$.
\ENDIF
\IF{there is a payment-eligible arm $i$}
\STATE Let $i$ be a uniformly random payment-eligible arm.
\STATE Offer payment
$\Pay{t}{i} = \max_{\AgV,i'} \AgV \cdot (\ArmEV{t}{i'} - \ArmEV{t}{i})$
for pulling arm $i$
(and payment 0 for all other arms).
\ELSE
\STATE Let agent $t$ play myopically, i.e., offer payments 0 for all arms.
\ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}

The high-level idea of the proof of Theorem~\ref{thm:main-overview}
is the following.
Because the algorithm ensures that each arm is pulled
``frequently enough,''
the estimates \ArmEV{t}{i} become exponentially more accurate in the
phase number $s$.
Thus, the fraction of agents who misidentify their best arm decreases
exponentially as well.
Because for each arm, enough agents have a true preference for this
arm, once the arms' attribute vectors are learned well enough,
the algorithm will not need to incentivize any more,
resulting in a payment bound independent of $T$.
Similarly, the regret will decrease exponentially, and mostly accrue
due to agents close to the border \pfcomment{we could clarify what border} making mistakes in their arm choices.
The detailed analysis following this outline is somewhat subtle, though,
due to the dependencies between the agents' arm pulls and the
estimates which in turn are based on past arm pulls.
We begin with several technical lemmas that are used for both the
payment and regret bounds.

To capture the probability masses of problematic and non-problematic
agents \pfcomment{in another pass we could seek to clarify what we mean by being problematic in the context of the border statement above}, we define the following:
$\AlmostTied{\delta} = \Prob[\AgV \sim \AgentDist]{%
\AgV \cdot (\ArmV{\Best{\AgV}} - \ArmV{\Second{\AgV}}) \leq \delta}$
is the total probability mass (across all arms) of agents
whose (actual) preference for their best arm over their second-best
arm is at most $\delta$.
\pfcomment{We can check through the whole paper on whether we have $\AgV \sim \AgentDist$ in the subscript or just $\AgV \sim \AgentDist$.  I think we have only $\AgentDist$ in the problem description.}
The minimum probability mass of users who prefer their best arm
sufficiently strongly over their second-best arm is denoted by
$\ClearPref{\delta} = \min_{i}
\Prob[\AgV \sim \AgentDist]{\Best{\AgV}=i,%
\AgV \cdot (\ArmV{\Best{\AgV}} - \ArmV{\Second{\AgV}}) > \delta}$.
Notice that \AlmostTied{\delta} (being a measure of problematic
agents) is defined as a sum over all arms,
while  \ClearPref{\delta} (being a measure of non-problematic agents)
is defined as a minimum over arms.
Also notice that $\ClearPref{0} = \MinProb$.
\dkcomment{We should stress somewhere that we assume that actual ties
  have measure 0.}


In this section, we prove Algorithm~\ref{alg:basic-incentivizing} achieves $O(\ARMNUM^2+\TieDensity(\log(T))^2)$ cumulative regret with $O(\ARMNUM^2)$ payment budget.  This is stated in the following pair of theorems, which together constitute our main results.

\begin{theorem}
The payment budget for Algorithm~\ref{alg:basic-incentivizing} is bounded above by $O(\ARMNUM^2)$. 
\label{rst:budget}
\end{theorem}


\begin{theorem}
The cumulative regret for Algorithm~\ref{alg:basic-incentivizing}
is bounded above by $O(\ARMNUM^2 d + \TieDensity d^2(\log(T))^2)$.
\label{rst:regret}
\end{theorem}

\subsection{General Lemmas}

We begin by showing that under our assumptions, the measure of
problematic arms cannot be too large.

\begin{lemma} \label{lem:sdelta}
$\AlmostTied{\delta} \leq \TieDensity \cdot \delta$ for all $\delta$.
\end{lemma}

\begin{emptyproof}
Using the upper bound from Assumption~\ref{A1}, we can bound

\begin{align*}
\AlmostTied{\delta}
& = \sum_{i,i'} \ProbC{\AgV \cdot (\ArmV{i} - \ArmV{i'}) \leq \delta}%
    {\AgV \in \FirstTwo{i}{i'}}
  \cdot \Prob{\AgV \in \FirstTwo{i}{i'}}\\
& \leq \sum_{i,i'} \TieDensity \cdot \delta
    \cdot \Prob{\AgV \in \FirstTwo{i}{i'}}
\; = \; \TieDensity \cdot \delta. \QED
\end{align*}
\end{emptyproof}

We next bound the length of any phase, which will be useful to bound
the cumulative regret of the early rounds
(before tail bounds have kicked in).

\begin{lemma} \label{lem:phase-length}
For any $s\geq 3$, the expected length of phase $s$ is at most
$\ARMNUM \cdot \log(s)$ time steps.
\end{lemma}

\begin{proof}
Let $S$ be the set of arms that have been pulled at most
$s-1$ times at the start of phase $s$.
Thus, phase $s$ \pfedit{has ended} when each $i \in S$ has been pulled at least once.
Consider any time step $t$.
If $S$ contains at least one payment-eligible arm $i$,
then some payment-eligible arm will be pulled with probability 1,
and every payment-eligible arm is in $S$.
In particular, the size of $S$ decreases with probability 1.
If $S$ contains no payment-eligible arm,
then by definition of payment-eligibility,
each arm $i \in S$ has probability at least $1/\log(s)$ of being pulled.
In particular, the size of $S$ decreases with probability at least $1/\log(s)$.

In either case, the expected number of time steps until
$|S|$ decreases is at most $s$,
so the expected number of steps until $S$ is empty is at most
$\ARMNUM \cdot \log(s)$.
\dkcomment{If we wanted, this bound could probably be improved using a
  coupon collector argument. But it would look uglier.}
\end{proof}
                  
The key technical lemma captures the intuition that
the estimates of the arms' attribute vectors become
exponentially more accurate with increasing phases $s$.
That intuition is captured by the following lemma.

\begin{lemma} \label{lem:round-prob}
Let $x > 0$ be arbitrary.
Recall that $\sigma$ is the standard deviation of the Gaussian noise.
Let $\LatePhase \geq \max \SET{50, \frac{92.16 \sigma^4}{x^4}}$.

\dkcomment{Is it problematic that we use $\sigma$ for two things here?}
Let $\tau$ be any stopping time that is almost surely \dkcomment{What
  exactly do we mean by this here?} in phase $s$,
with respect to the filtration
$\mathcal{F}_{t} = \sigma (\Pull{1}, \ldots, \Pull{t},
                          \PayA{1}, \ldots, \PayA{t},
                          \ObsV{1}, \ldots, \ObsV{t})$.
Then, for all phases $s \geq \LatePhase$,
with probability at least 
$1- 24 \ARMNUM d \exp\left(-\frac{1.8 s x^2}{16 \sigma^2} \right)$,
the following holds for all arms $i$ and attributes $j$
simultaneously:
$|\ArmE{\tau}{i}{j} - \Arm{i}{j}| \leq x$.
\dkcomment{We should clarify what the probability is over. It seems to
  be over the entire past history, but not the current type.}
\end{lemma}

The proof of Lemma~\ref{lem:round-prob} is based on an adaptive
concentration inequality due to \cite{zhao2016adaptive},
given as Lemma~\ref{lem:ACI-inequality}.

\begin{lemma}[Corollary 1 of \cite{zhao2016adaptive}]
\label{lem:ACI-inequality}
Let $X_i$ be zero-mean $1/2$-subgaussian random variables,
and $\SET{S_n = \sum_{i=1}^n X_i, n \geq 1}$ the corresponding random walk.
Let $J$ be any stopping time with respect to $\SET{X_1, X_2, \ldots}$.
(We allow $J$ to take the value $\infty$,
defining $\Prob{J = \infty} = 1 - \lim_{n \to \infty} \Prob{J \leq n}$.)
Define \dkcomment{I'm making the dependence of $b$ on $n$ explicit,
  since that's how we're using it.}
\begin{align*}
g(n) & = \sqrt{0.6 n \cdot \log (\log_{1.1}(n) + 1) + n \cdot b(n)}.
\end{align*}
Then, 
\begin{align*}
\Prob{J < \infty \mbox{ and } S_J \geq g(J)} \leq 12 \e^{-1.8 b(n)}.
\end{align*}
\end{lemma}

\begin{extraproof}{Lemma~\ref{lem:round-prob}}
Fix an arm $i$ and attribute $j$.
For any $n \geq 1$, let $k_n$ be the time step right after arm $i$ has
been pulled for the \Kth{n} time.
\dkcomment{I'm writing $n$ instead of $s$ here because $S_s$ would
  look ugly, and $n$ is consistent with the lemma.
  Or we could rename $S$ to something else.}
Define
$S_n := \frac{n \cdot (\ArmE{k_n}{i}{j} - \Arm{i}{j})}{2 \sigma}$
to be the sum of all attribute-$j$ noise components up to and
including the \Kth{n} pull of arm $i$,
\dkcomment{I'm not sure if we have defined the notation for components
  of vectors in the \emph{superscript}. Let's make sure not to forget it.}
renormalized to have standard deviation $1/2$.
\dkcomment{Is this the correct scaling.}
The $S_n$ define an unbiased half-subgaussian random walk,
and we can therefore apply Lemma~\ref{lem:ACI-inequality} to them.
Specifically, we set $b(n) = \frac{n x^2}{16 \sigma^2}$,
and obtain that

\begin{align*}
\Prob{J < \infty \mbox{ and } S_J \geq 
\sqrt{0.6 J \cdot \log (\log_{1.1}(J) + 1) + \frac{J^2 x^2}{16 \sigma^2}}}
& \leq 12 \exp \left( \frac{-1.8 J x^2}{16 \sigma^2} \right).
\end{align*}

Applying Lemma~\ref{lem:ACI-inequality} to $-S_n$ with the same choice
of $b(n)$, and taking a union bound over both cases, we obtain that

\begin{align*}
\Prob{J < \infty \mbox{ and } |S_J| \geq 
\sqrt{0.6 J \cdot \log (\log_{1.1}(J) + 1) + \frac{J^2 x^2}{16 \sigma^2}}}
& \leq 24 \exp \left( \frac{-1.8 J x^2}{16 \sigma^2} \right).
\end{align*}

In the high-probability case
$|S_J| \leq
\sqrt{0.6 J \cdot \log (\log_{1.1}(J) + 1) + \frac{J^2 x^2}{16 \sigma^2}}$,
we first use that $\sqrt{\cdot}$ is sublinear to bound
$\sqrt{0.6 J \cdot \log (\log_{1.1}(J) + 1) + \frac{J^2 x^2}{16 \sigma^2}}
\leq \sqrt{0.6 J \cdot \log (\log_{1.1}(J) + 1)} + \frac{J x}{4 \sigma}$.

\bccomment{If $b(n)$ is a constant, here we know $\sqrt{0.6 J \cdot \log (\log_{1.1}(J) + 1) + \frac{J^2 x^2}{16 \sigma^2}}
\leq \sqrt{0.6 J \cdot \log (\log_{1.1}(J) + 1)} + \frac{\sqrt{J} x}{4 \sigma}\leq \sqrt{0.6 J \cdot \log (\log_{1.1}(J) + 1)} + \frac{J x}{4 \sigma}$.}

Then, we use the following technical Lemma~\ref{lem:n0-inequality}
(whose proof is given in the appendix, and whose precondition we verify
momentarily) to bound the first term,
giving us an overall upper bound of
$|S_J| \leq \frac{J x}{2 \sigma}$.

\begin{lemma} \label{lem:n0-inequality}
For any $n \geq \LatePhase = \max (50, \frac{92.16 \sigma^4}{x^4})$, 
\begin{align*}
\sqrt{0.6 n \log (\log_{1.1}(n) + 1)} & \leq \frac{n x}{4 \sigma}. 
\end{align*}
\end{lemma}

Substituting the definition of $S_J$ and canceling common terms,
the inequality implies that
$|\ArmE{k_J}{i}{j} - \Arm{i}{j}| \leq x$.
By the assumptions of the lemma,
the stopping time $\tau$ is such that almost surely, each arm --- and
in particular arm $i$ --- has been pulled at least
$s \geq \LatePhase$ times at time $\tau$.
Defining $J$ to be the number of times that $i$ has been pulled at
time $\tau$, we obtain that $J$ satisfies the precondition of
Lemma~\ref{lem:n0-inequality}.
Furthermore, with probability 1, $J$ will be finite, meaning that
\begin{align*}
& \Prob{S_J \geq 
\sqrt{0.6 J \cdot \log (\log_{1.1}(J) + 1) + \frac{J^2 x^2}{16
    \sigma^2}}}\\
= & \Prob{J < \infty \mbox{ and } S_J \geq 
\sqrt{0.6 J \cdot \log (\log_{1.1}(J) + 1) + \frac{J^2 x^2}{16
    \sigma^2}}}.
\end{align*}

Finally, this choice of $J$ ensures that
$\ArmE{k_J}{i}{j} = \ArmE{\tau}{i}{j}$,
and we have thus shown that
$|\ArmE{\tau}{i}{j} - \Arm{i}{j}| \leq x$ for all $i,j$.
The lemma now follows by a union bound over all \ARMNUM arms and $d$
attributes.
\end{extraproof}


Lemma~\ref{lem:round-prob} implies that errors from myopic play
decrease exponentially in the phase number $s$.

\begin{corollary} \label{cor:right-choice}
Let $x > 0$ be arbitrary.
Let $\LatePhase \geq \max (50, \frac{92.16 \sigma^4}{x^4})$.

Let $\tau$ be any stopping time that is almost surely \dkcomment{What
  exactly do we mean by this here?} in phase $s$,
with respect to the filtration
$\mathcal{F}_{t} = \sigma (\Pull{1}, \ldots, \Pull{t},
                          \PayA{1}, \ldots, \PayA{t},
                          \ObsV{1}, \ldots, \ObsV{t})$.
Then, for all $s \geq \LatePhase$,
with probability (over the past history) at least 
$1 - 24 \ARMNUM d \exp\left(-\frac{1.8 s x^2}{16 \sigma^2} \right)$,
\dkcomment{We should clarify what the probability is over. It seems to
  be over the past history, but not the current agent.}
an agent of type $\AgV = \AgentV{\tau}$ 
will not pull a highly suboptimal arm, i.e., an arm $i$ with 
$\AgV \cdot (\ArmV{\Best{\AgV}} - \ArmV{i}) > 2\Diam d x$.
\end{corollary}

\begin{proof}
Let $\tau$ be a (random) time.
By Lemma~\ref{lem:round-prob},
with probability at least
$1 - 24 \ARMNUM d \cdot \exp \left( \frac{-1.8 s x^2}{16\sigma^2} \right)$,
for all arms $i$ and attributes $j = 1, \ldots, d$ simultaneously,
the estimated attribute $j$ for arm $i$ is close enough to the true attribute: 
$|\ArmE{\tau}{i}{j} - \Arm{i}{j}| \leq x$.
Condition on this high-probability event.

Consider any agent type $\AgV = \AgentV{\tau}$ at the (random) time
$\tau$.
Let $i \neq \Best{\AgV}$ be any arm
with much smaller true reward than the best arm:
$\AgV \cdot (\ArmV{\Best{\AgV}} - \ArmV{i}) > 2\Diam d x$.
Because each coordinate of \ArmEV{\tau}{\Best{\AgV}} and of
\ArmEV{\tau}{i} is estimated accurately to within $x$, 
we get that 
$\AgV \cdot (\ArmEV{\tau}{\Best{\AgV}} - \ArmV{\Best{\AgV}})
\geq - \Diam d x$
and
$\AgV \cdot (\ArmEV{\tau}{i} - \ArmV{i}) \leq \Diam d x$.
Hence, 

\begin{align}
\AgV \cdot (\ArmEV{\tau}{\Best{\AgV}} - \ArmEV{\tau}{i})
& =
\AgV \cdot (\ArmEV{\tau}{\Best{\AgV}} - \ArmV{\Best{\AgV}})
+ \AgV \cdot (\ArmV{\Best{\AgV}} - \ArmV{i})
+ \AgV \cdot (\ArmV{i} - \ArmEV{\tau}{i}) \nonumber\\
& > -\Diam d x + 2 \Diam d x - \Diam d x
\; = \; 0, \label{equ:choice}
\end{align}
which means that the agent with type \AgV will not pull arm $i$
under the high-probability event.
\end{proof}

\subsection{Bounding the Total Payment}

Towards bounding the algorithm's total payment, we first bound the
\emph{number} of rounds in which the algorithm makes a payment by a
constant.
This bound also turns out to be useful for bounding the total regret.

\begin{lemma} \label{lem:numP}
The expected number of time steps in which
Algorithm~\ref{alg:basic-incentivizing}
makes any payment is at most $O(\ARMNUM^2)$.
\end{lemma}

\begin{proof}
We partition phases into early and late phases.
For each of the early phases,
we crudely bound the number of payments by \ARMNUM,
using that each arm is incentivized at most once per phase.
For later phases,
we use that arms are payment-eligible only if their myopic pulling
probability is less than $1/\log(s)$.
Since at least a \MinProb fraction of agents would prefer arm $i$ if
given accurate estimates, this would require misestimates that make a
significant fraction of agents pull the wrong arm.
But in later phases, such misestimates are unlikely by
Corollary~\ref{cor:right-choice}.

To make this intuition precise, we set
$\delta = \frac{\MinProb}{2 \TieDensity}$,
and $x = \frac{\delta}{2 \Diam d}$.
The cutoff point between early and late phases is now set to
$\EvenLaterPhase = \max(50, \frac{92.16 \sigma^4}{x^4}, \exp(\frac{2}{\MinProb}))$.
Consider the \Kth{s} pull of an arm $i$, for $s \geq \EvenLaterPhase$.
If this pull occurs before phase $s$, it was definitely not incentivized.
So we condition on the pull occurring in phase $s$,
and let $\tau$ be the time step when this pull occurred; notice that
this means that this must be the first pull of arm $i$ in phase $s$.
We can now apply Corollary~\ref{cor:right-choice}
with this choice of $x$,
and obtain that with probability at least 
$1 - 24 \ARMNUM d \cdot \exp\left(-\frac{1.8 s x^2}{16 \sigma^2} \right)$,
all agent types \AgV with
$\AgV \cdot (\ArmV{\Best{\AgV}} - \ArmV{\Second{\AgV}})
> 2\Diam d x = \delta$
will pull their best arm \Best{\AgV}.
\dkcomment{We are losing a factor \ARMNUM here.
We already took a union bound over all arms in the lemma, and
we now apply it only to one arm, then sum over all arms.}

By Lemma~\ref{lem:sdelta}, the \emph{total} measure of agents (across
all arms) whose best and second-best arm differ in utility by less
than $\delta$ is at most $\TieDensity \delta = \frac{\MinProb}{2}$.
In particular, this bound holds for agents whose best arm is $i$.
By Assumption~\ref{A3}, at least a measure \MinProb of agents has $i$
as their best arm, and thus, at least a measure $\frac{\MinProb}{2}$
has arm $i$ as their preferred arm by at least $\delta$ over any other
arm $i'$.
In the high-probability event, all such agents would myopically pull
the arm $i$, and as a result, in the high-probability event,
arm $i$ is not payment-eligible.

Thus, for any phase $s \geq \EvenLaterPhase$
Algorithm~\ref{alg:basic-incentivizing} incentivizes any given arm $i$
with probability at most
$24 \ARMNUM d \cdot \exp \left(
  -\frac{1.8 s \cdot \MinProb^2}{256 \TieDensity^2 \Diam^2 d^2 \sigma^2}
\right)$.
Summing over all arms and phases $s$,
and adding the at most $\ARMNUM \EvenLaterPhase$ incentivizations in
the first \EvenLaterPhase phases, 
the expected total number of arm incentivizations is at most
\begin{align*}
\ARMNUM \EvenLaterPhase
  + 24 \ARMNUM^2 d \cdot \frac{1}{1 - \exp \left(
    \frac{-1.8 s \cdot \MinProb^2}{256 \TieDensity^2 \Diam^2 d^2 \sigma^2}
  \right)}
& = O\left( \max\{\frac{\ARMNUM \TieDensity^4 \Diam^4 d^4 \sigma^4}{\MinProb^4}, N\exp(\frac{2}{p})\}
  + \frac{\ARMNUM^2 d}{1 - \exp \left(
    \frac{-1.8 s \cdot \MinProb^2}{256 \TieDensity^2 \Diam^2 d^2 \sigma^2}
  \right)} \right).
\end{align*}
\dkcomment{It seems like with our choice of $x$ (which may be
  unavoidable), we get quite large factors, in particular with the
  fourth power.
  Also, it would be nice if we could simplify the $\frac{1}{1-\exp()}$
  term. Unfortunately, I think that the inequality goes the wrong way
  for applying the standard $\exp(-x) \geq 1-x$ bound.}

\dkcomment{I'm leaving in all the dependencies on ``constant''
  parameters for now. We can decide later which ones to absorb into
  big-$O$.}
\end{proof}

It would be desirable to simply identify a constant upper bound on the
payment made each time.
Unfortunately, while the agent types are drawn from a bounded support,
the noise in arm locations is not;
hence, with small probability, arm locations may be grossly
misestimated, resulting in high incentive payments.
As a result, the actual analysis of the total payment is significantly more
intricate, and hence,
the proof of Theorem~\ref{rst:budget} is given in the appendix.


\subsection{Bounding the Total Regret}
In bounding the total regret, because agents' types are from a compact
set by Assumption~\ref{A2}, the maximum regret in any one round is
bounded by a constant.
We use $\MAXR = \max_{\AgV, i,i'} \AgV \cdot (\ArmV{i} - \ArmV{i'})$
to denote this constant upper bound on the maximum regret that can be
incurred in any one time step. 


\dkcomment{The following paragraph is from Bangrui's version. It may
  move to an earlier section, go away, or become a part somewhere.}
As we can see later in our proof, we only need
$\max_{i,j}\limsup_{y\rightarrow 0^{+}}\frac{F_{i,j}(y)}{y}$ to be
finite.
Intuitively, Assumption~\ref{A1} states that there are not many agents
who are indifferent between their best arm and the second best arm.


\begin{proof}
Regret can arise in two ways:
(1) an agent was incentivized to pull a suboptimal arm, or
(2) an agent myopically pulled a suboptimal arm.
By Lemma~\ref{lem:numP}, Algorithm~\ref{alg:basic-incentivizing}
incentivizes agents at most 
$O \left( \max\{\frac{\ARMNUM \TieDensity^4 \Diam^4 d^4 \sigma^4}{\MinProb^4}, N\exp(\frac{2}{p})\}
  + \frac{\ARMNUM^2 d}{1 - \exp \left(
    \frac{-1.8 s \cdot \MinProb^2}{256 \TieDensity^2 \Diam^2 d^2 \sigma^2}
  \right)} \right)$
times in expectation, each time causing regret at most \MAXR,
for a total expected regret of at most
$O \left(\MAXR\cdot\max\{\frac{\ARMNUM \TieDensity^4 \Diam^4 d^4 \sigma^4}{\MinProb^4}, N\exp(\frac{2}{p})\}
  + \frac{\MAXR \ARMNUM^2 d}{1 - \exp \left(
    \frac{-1.8 s \cdot \MinProb^2}{256 \TieDensity^2 \Diam^2 d^2 \sigma^2}
  \right)} \right)$.
For the rest of the proof, we focus on the regret incurred when agents
pull arms myopically and make mistakes.

We distinguish between agents incurring large regret
(which requires severe misestimates of arm locations;
such misestimated are exponentially unlikely to occur), 
and agents incurring small positive regret,
which requires these agents to be almost tied in their preference for
the best arm.
To be more precise, we will define (with hindsight) a phase-dependent
cutoff $g(s)$, and consider a regret exceeding $g(s)$ large.

We first consider agents \AgV incurring positive regret less than $g(s)$.
The second-best arm \Second{\AgV} of \AgV must satisfy
$\AgV \cdot (\ArmV{\Best{\AgV}} - \ArmV{\Second{\AgV}}) \leq g(s)$.
By Lemma~\ref{lem:sdelta}, the total measure of such agents
is $\AlmostTied{g(s)} \leq \TieDensity \cdot g(s)$,
and each incurs regret at most $g(s)$.
Summing over all time steps, the expected total regret from such agents is at most
\begin{align}
&E[\sum_{s=1}^{T}\sum_{t=t_{s-1}+1}^{t_{s}} \TieDensity \cdot g^2(s)] \leq \sum_{s=1}^{T}\TieDensity\cdot g^2(s)N\log(s). \nonumber 
\end{align}

To bound the regret incurred in time steps with large regret,
let the random variable $t_s$ be the final time step in phase $s$,
for each $s$.
\dkcomment{Is $t_s$ better or $T_s$?}
Define the stopping times $\tau_{s}^{k}$ to be the \Kth{k}
time step in the \Kth{s} phase,
with $\tau_{s}^{k} = \infty$ when phase $s$ has fewer than $k$ steps,
i.e., $k > t_{s}-t_{s-1}$.
For $t = \tau_{s}^{k}$, we know
\begin{align}
&\Prob{\text{incur regret exceeding g(s)}} \nonumber \\
\leq & \Prob{\exists i,j, s.t. |\ArmE{t}{i}{j} - \Arm{i}{j}|\geq \frac{g(s)}{2\Diam d}}.\nonumber 
\end{align}

Because the regret never exceeds \MAXR,
the expected (large) regret for the \Kth{k} time step of phase $s$
is bounded above by
$\Prob{\exists i,j, s.t. |\ArmE{t}{i}{j} - \Arm{i}{j}|\geq \frac{g(s)}{2\Diam d}} \cdot \MAXR$.
Summing over all time steps $t$, we get that the total regret from
large-regret steps is at most

\begin{align*}
\lefteqn{\sum_{s = 1}^T \sum_{t=t_s}^{t_{s+1}-1}
  \Prob{\exists i,j, s.t. |\ArmE{t}{i}{j} - \Arm{i}{j}|\geq \frac{g(s)}{2\Diam d}} \cdot \MAXR}
\\ & =
\sum_{s = 1}^T 
  \Prob{\exists i,j, s.t. |\ArmE{t}{i}{j} - \Arm{i}{j}|\geq \frac{g(s)}{2\Diam d}} \cdot \MAXR
  \cdot \Expect{\mbox{Length of phase } s}
\\ & \leq
\sum_{s = 1}^T 
 \Prob{\exists i,j, s.t. |\ArmE{t}{i}{j} - \Arm{i}{j}|\geq \frac{g(s)}{2\Diam d}} \cdot \MAXR
  \cdot \ARMNUM \log(s),
\end{align*}
where the inequality was an application of Lemma~\ref{lem:phase-length}.
Adding all three terms,
the cumulative regret up to time $T$ is at most

\begin{align}
&O \left( \MAXR\cdot \max\{\frac{\ARMNUM \TieDensity^4 \Diam^4 d^4 \sigma^4}{\MinProb^4}, N\exp(\frac{2}{p})\}
  + \frac{\MAXR \ARMNUM^2 d}{1 - \exp \left(
    \frac{-1.8 s \cdot \MinProb^2}{256 \TieDensity^2 \Diam^2 d^2 \sigma^2}
  \right)} \right)
\\
+ & \sum_{s = 1}^T 
  \Prob{\exists i,j, s.t. |\ArmE{t}{i}{j} - \Arm{i}{j}|\geq \frac{g(s)}{2\Diam d}} \cdot \MAXR
  \cdot \ARMNUM \log(s),
+ \sum_{s=1}^{T}\TieDensity \cdot g^2(s)\cdot N\log(s). \label{thm:equ}
\end{align}

The first line is independent of $T$, and we now choose
$g^2(s) = \frac{288\log(T) \cdot \Diam^2 d^2 \sigma^2}{1.8 s}$
with the goal of minimizing the second line.
With this choice and same definition of $S_{s}$ as in the proof of Lemma~\ref{lem:round-prob}, 
\begin{align*}
&\Prob{\exists i,j, s.t. |\ArmE{t}{i}{j} - \Arm{i}{j}|\geq \frac{g(s)}{2\Diam d}} \\
\leq &\sum_{i,j}\Prob{|S_s|\geq 3\sqrt{\frac{2}{1.8}s\log(T)}} \\
\leq &\sum_{i,j}\Prob{|S_s|\geq \sqrt{0.6s\log(\log_{1.1}(s)+1)+\frac{2}{1.8}\log(T)s}} \leq 24\ARMNUM d \cdot \exp(-2\log(T)),
\end{align*}
where the second to last inequality is because the following technical inequality, whose proof we include in the appendix.

\begin{lemma} \label{lem:n1-inequality}
For $2\leq s\leq T$, we have $3\sqrt{\frac{2}{1.8}s\log(T)} \geq \sqrt{0.6s\log(\log_{1.1}(s)+1)+\frac{2}{1.8}\log(T)s}$.
\end{lemma}

\bccomment{I believe the calculation up to this line should be correct now. The final equation we want to minimize is $\sum_{s=1}^{T}[\exp(-s*g^2(s))+g^2(s)*s]$, what should be our choice of $g(s)$?}


Therefore, the second line becomes

\begin{align*}
\lefteqn{\sum_{s=1}^{T} 24\ARMNUM^2 d \log(s) \MAXR \exp\left(-2\log(T)\right)  +
          \frac{288\ARMNUM\Diam^2 d^2\sigma^2
          \TieDensity\log(T)}{1.8}\sum_{s=1}^{T}\frac{\log(s)}{s}}
\\ & \leq
24\ARMNUM^2 d R\frac{T(T-1)}{2T^2}  + 160 \ARMNUM \Diam^2 d^2\sigma^2
          \TieDensity(\log(T))^2(\log(T)+1)
  \\ & \leq
12 \ARMNUM^2 d R  + 160 \ARMNUM\Diam^2 d^2\sigma^2 \TieDensity(\log(T))^2(\log(T)+1). 
\end{align*}

Adding all terms gives an upper bound of 

\begin{multline*}
O \left(\MAXR\cdot \max\{\frac{\ARMNUM \TieDensity^4 \Diam^4 d^4 \sigma^4}{\MinProb^4}, N\exp(\frac{2}{p})\}
  + \frac{\MAXR \ARMNUM^2 d}{1 - \exp \left(
    \frac{-1.8 s \cdot \MinProb^2}{256 \TieDensity^2 \Diam^2 d^2 \sigma^2}
  \right)} \right) +
O(\ARMNUM\Diam^2 d^2\sigma^2 \TieDensity(\log(T))^3) 
\end{multline*}

on the cumulative expected regret.
\dkcomment{I see a problem here: What's the value of $x$ in the
  definition of \LatePhase? Presumably, we need to make $x$ small
  enough that any $x$ we use in any application of Lemma 7 (or
  Corollary 10) works. But notice that we use $x =
  \Theta(g^2(\tau_s^k))$,
  and in particular for the last step, I don't see how we can make
  $\tau_s^k$ much smaller than $T$.
  But then, we get that $x = O(\frac{\log T}{T})$, which would give
  linear regret. How to handle this?}
\end{proof}

\dkcomment{Edited up to here.}

\begin{corollary} \label{cor:constant-regret}
If $\exists \delta>0$ such that $F_{i,j}(\delta)=0$ for all $i,j$, then the cumulative expected regret is bounded by $O \left(\MAXR\cdot \max\{\frac{\ARMNUM \TieDensity^4 \Diam^4 d^4 \sigma^4}{\MinProb^4}, N\exp(\frac{2}{p})\}
  + \frac{\MAXR \ARMNUM^2 d}{1 - \exp \left(
    \frac{-1.8 s \cdot \MinProb^2}{256 \TieDensity^2 \Diam^2 d^2 \sigma^2}
  \right)} \right) $.
\end{corollary}

\begin{proof}
The proof of this corollary is similar to the proof of Theorem~\ref{rst:regret}. If we set $g^2(s) = \frac{288\log(T) \cdot \Diam^2 d^2 \sigma^2}{1.8 s}$, then there exists a $s^{0}$ such that for $s>s_{0}$, $q(g(s))=0$. Thus, similar to equation~\eqref{thm:equ}, we know the cumulative expected regret when we let the agents play myopically is bounded above by
\begin{align}
\sum_{s=1}^{T} 24\ARMNUM^2 d s \MAXR \exp\left(-2\log(T)\right)+ \sum_{s=1}^{s^{0}}\TieDensity g(s)^2\ARMNUM\log(s) \nonumber
\end{align}
Therefore, based on the same analysis of Theorem~\ref{rst:regret}, we know the cumulative regret is bounded by $O \left(\MAXR\cdot \max\{\frac{\ARMNUM \TieDensity^4 \Diam^4 d^4 \sigma^4}{\MinProb^4}, N\exp(\frac{2}{p})\}
  + \frac{\MAXR \ARMNUM^2 d}{1 - \exp \left(
    \frac{-1.8 s \cdot \MinProb^2}{256 \TieDensity^2 \Diam^2 d^2 \sigma^2}
  \right)} \right) $.
\end{proof}

