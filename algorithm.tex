% move to colt.tex eventually

\newcommand{\e}{\mathrm{e}}
\providecommand{\Kth}[1]{\ensuremath{{#1}^{\rm th}}}

\def\QED{{\phantom{x}} \hfill \ensuremath{\rule{1.3ex}{1.3ex}}}

\newcommand{\extraproof}[1]{\rm \trivlist \item[\hskip \labelsep{\bf Proof of #1. }]}
\def\endextraproof{\QED \endtrivlist}

\def\emptyproof{\rm \trivlist \item[\hskip \labelsep{\bf Proof. }]}
\def\endemptyproof{\endtrivlist}

\section{Main Algorithm and Analysis}
\label{sec:ub}

The algorithm achieving the claimed bounds of
Theorem~\ref{thm:main-overview} is fairly simple.
It mostly allows agents to exploits, but when an arm is estimated to
be extremely unlikely to be pulled by any agents,
the algorithm incentivizes such an arm with a payment that is so high
that the next agent is guaranteed to pull the arm.
This way, the algorithm ensures that each arm is pulled at least a
certain minimum number of times.

More precisely, the algorithm divides time into \emph{phases}
$s = 1, 2, 3, \ldots$.
Phase $s$ ends when each arm has been pulled at least $s$ times.
An arm $i$ is \emph{payment-eligible} at time $t$ (in phase $s$)
if both of the following hold:

\begin{itemize}
\item $i$ has been pulled at most%
\footnote{in fact: exactly, since the algorithm entered phase $s$}
$s-1$ times up to time $t$, i.e., $\NumPull{t}{s} \leq s-1$.
\item Based on the current estimates \ArmEV{t}{i'} of all arms'
attribute vectors, the probability of pulling arm $i$ is less than $1/s$.
Let $\PullProb{t}{i} = \Prob[\AgV \sim \AgentDist]{\AgV \cdot \ArmEV{t}{i} > \AgV
  \cdot \ArmEV{t}{i'} \mbox{ for all } i' \neq i}$
be the probability that arm $i$ will be pulled
by the next (random) agent based on the current estimates. 
Then, the second criterion for payment-eligibility is that
$\PullProb{t}{i} < 1/s$.
\end{itemize}

When multiple arms are payment-eligible, the algorithm picks one
uniformly at random to incentivize.
When the algorithm decides to incentivize an arm $i$,
it offers ``whatever it takes,'' i.e., offers a payment of
$\Pay{t}{i} = \max_{\AgV,i'} \AgV \cdot (\ArmEV{t}{i'} - \ArmEV{t}{i})$.
The maximum for \AgV is taken over the support of \AgentDist;
recall that we assumed this support to be compact.
The payment \Pay{t}{i} may appear unnecessarily high.
Indeed, we show in Section~\ref{sec:pi} that it suffices to
incentivize only a $1/s$ fraction of the agents.
\dkcomment{Given that we don't prove anything there, is ``show'' the
  right word?}
Here, we focus on the higher payments for simplicity of analysis.

Notice that \PullProb{t}{i} depends on the estimates for \emph{all}
arms; thus, by pulling some other arm $i'$, an arm $i$ may become
payment-eligible, or cease to be so.
Algorithm~\ref{alg:basic-incentivizing} gives the full details.


\begin{algorithm}
\caption{Algorithm: Incentivizing Exploration \label{alg:basic-incentivizing}}
\begin{algorithmic}
\STATE Set the current phase number $s = 1$.
\FOR{time steps $t = 1, 2, 3, \ldots$}
\IF{$\NumPull{t}{i} \geq s$ for all arms $i$}
\STATE Increment the phase $s = s + 1$.
\ENDIF
\IF{there is a payment-eligible arm $i$}
\STATE Let $i$ be a uniformly random payment-eligible arm.
\STATE Offer payment
$\Pay{t}{i} = \max_{\AgV,i'} \AgV \cdot (\ArmEV{t}{i'} - \ArmEV{t}{i})$
for pulling arm $i$
(and payment 0 for all other arms).
\ELSE
\STATE Let agent $t$ play myopically, i.e., offer payments 0 for all arms.
\ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}

The high-level idea of the proof of Theorem~\ref{thm:main-overview}
is the following.
Because the algorithm ensures that each arm is pulled
``frequently enough,''
the estimates \ArmEV{t}{i} become exponentially more accurate in the
phase number $s$.
Thus, the fraction of agents who misidentify their best arm decreases
exponentially as well.
Because for each arm, enough agents have a true preference for this
arm, once the arms' attribute vectors are learned well enough,
the algorithm will not need to incentivize any arm pulls any more,
resulting in a payment bound independent of $T$.
Similarly, the regret will decrease exponentially, and mostly accrue
due to agents close to the border making mistakes in their arm choices.
The detailed analysis following this outline is somewhat subtle, though,
due to the dependencies between the agents' arm pulls and the
estimates which in turn are based on past arm pulls.
We begin with several technical lemmas that are used for both the
payment and regret bounds.

To capture the probability masses of problematic and non-problematic
agents, we define the following:
$\AlmostTied{\delta} = \Prob[\AgV \sim \AgentDist]{%
\AgV \cdot (\ArmV{\Best{\AgV}} - \ArmV{\Second{\AgV}}) \leq \delta}$
is the total probability mass (across all arms) of agents
whose (actual) preference for their best arm over their second-best
arm is at most $\delta$.
The minimum probability mass of users who prefer their best arm
sufficiently strongly over their second-best arm is denoted by
$\ClearPref{\delta} = \min_{i}
\Prob[\AgV \sim \AgentDist]{\Best{\AgV}=i,%
\AgV \cdot (\ArmV{\Best{\AgV}} - \ArmV{\Second{\AgV}}) > \delta}$.
Notice that while \AlmostTied{\delta} (being a measure of problematic
agents) is defined as a sum over all arms,
while  \ClearPref{\delta} (being a measure of non-problematic agents)
is defined as a minimum over arms.
Also notice that $\ClearPref{0} = \MinProb$.
\dkcomment{We should stress somewhere that we assume that actual ties
  have measure 0.}


\dkcomment{I moved the assumptions to an earlier section.
I am saving here some text that was in that subsection.}

We use $R = \max_{\AgV, i,j} \AgV \cdot (u_i - u_j)$ to denote the
maximum regret that can be incurred at each time.
Assumption~\ref{A2} implies that the maximum regret or payment
incurred in any one round is finite.

As we can see later in our proof, we only need $\max_{i,j}\limsup_{y\rightarrow 0^{+}}\frac{F_{i,j}(y)}{y}$ to be finite. Intuitively, assumption~\ref{A1} states that there are not many agents who are indifferent between their best arm and the second best arm. 


In this section, we prove Algorithm~\ref{alg:basic-incentivizing} achieves $O(\ARMNUM^2+\TieDensity(\log(T))^2)$ cumulative regret with $O(\ARMNUM^2)$ payment budget.  This is stated in the following pair of theorems, which together constitute our main results.

\begin{theorem}
The payment budget for Algorithm~\ref{alg:basic-incentivizing} is bounded above by $O(\ARMNUM^2)$. 
\label{rst:budget}
\end{theorem}


\begin{theorem}
The cumulative regret for Algorithm~\ref{alg:basic-incentivizing}
is bounded above by $O(\ARMNUM^2 d + \TieDensity d^2(\log(T))^2)$.
\label{rst:regret}
\end{theorem}


With this additional notation, we now prove several lemmas.
First, based on Assumption~\ref{A1}, we have the following bound for $S(\delta)$.


Notation: $t_s$ is end time for phase $s$.

\subsection{General Lemmas}

We begin by showing that under our assumptions, the measure of
problematic arms cannot be too large.

\begin{lemma} \label{lem:sdelta}
$\AlmostTied{\delta} \leq \TieDensity \cdot \delta$ for all $\delta$.
\end{lemma}

\begin{emptyproof}
Using the upper bound from Assumption~\ref{A1}, we can bound

\begin{align*}
\AlmostTied{\delta}
& = \sum_{i,i'} \ProbC{\AgV \cdot (\ArmV{i} - \ArmV{i'}) \leq \delta}%
    {\AgV \in \FirstTwo{i}{i'}}
  \cdot \Prob{\AgV \in \FirstTwo{i}{i'}}\\
& \leq \sum_{i,i'} \TieDensity \cdot \delta
    \cdot \Prob{\AgV \in \FirstTwo{i}{i'}}
\; = \; \TieDensity \cdot \delta. \QED
\end{align*}
\end{emptyproof}

We next bound the length of any phase, which will be useful to bound
the cumulative regret of the early rounds
(before tail bounds have kicked in).

\begin{lemma} \label{lem:phase-length}
For any $s$, the expected length of phase $s$ is at most
$\ARMNUM \cdot s$ time steps.
\end{lemma}

\begin{proof}
Let $S$ be the set of arms that have been pulled at most
$s-1$ times at the start of phase $s$.
Thus, phase $s$ ends when each $i \in S$ has been pulled at least once.
Consider any time step $t$.
If $S$ contains at least one payment-eligible arm $i$,
then some payment-eligible arm will be pulled with probability 1,
and every payment-eligible arm is in $S$.
In particular, the size of $S$ decreases with probability 1.
If $S$ contains no payment-eligible arm,
then by definition of payment-eligibility,
each arm $i \in S$ has probability at least $1/s$ of being pulled.
In particular, the size of $S$ decreases with probability at least $1/s$.

In either case, the expected number of time steps until
$|S|$ decreases is at most $s$,
so the expected number of steps until $S$ is empty is at most
$\ARMNUM \cdot s$.
\dkcomment{If we wanted, this bound could probably be improved using a
  coupon collector argument. But it would look uglier.}
\end{proof}
                  
The key technical lemma bounds the probability of mistakes in myopic play.
Intuitively, because estimates of the arms' attribute vectors become
exponentially more accurate with increasing phases $s$,
errors from myopic play should decrease exponentially.
That intuition is captured by the following lemma.

\begin{lemma} \label{lem:round-prob}
Let $x > 0$ be arbitrary.
Recall that $\sigma$ is the standard deviation of the Gaussian noise.
Let $\LatePhase \geq \max \SET{50, \frac{92.16 \sigma^4}{x^4}}$.

\dkcomment{Is it problematic that we use $\sigma$ for two things here?}
Let $\tau$ be any stopping time that is almost surely \dkcomment{What
  exactly do we mean by this here?} in phase $s$,
with respect to the filtration
$\mathcal{F}_{t} = \sigma (\Pull{1}, \ldots, \Pull{t},
                          \PayA{1}, \ldots, \PayA{t},
                          \ObsV{1}, \ldots, \ObsV{t})$.
Then, for all $s \geq \LatePhase$,
\begin{align*}
  \ProbC[\AgV \sim \AgentDist]{%
  \argmax \SET{\AgentV{\tau} \cdot \ArmEV{\tau}{i}} \neq \Best{\AgentV{\tau}}}{%
  \AgentV{\tau} \cdot (\ArmV{\Best{\AgentV{\tau}}} - \ArmV{\Second{\AgentV{\tau}}})
  > 2\Diam d x}
& \leq 24 \ARMNUM d \exp\left(-\frac{1.8 s x^2}{16 \sigma^2} \right).
\end{align*}
\end{lemma}

The proof of Lemma~\ref{lem:round-prob} is based on an adaptive
concentration inequality due to \cite{zhao2016adaptive},
given as Lemma~\ref{lem:ACI-inequality}.

\begin{lemma}[Corollary 1 of \cite{zhao2016adaptive}]
\label{lem:ACI-inequality}
Let $X_i$ be zero-mean $1/2$-subgaussian random variables,
and $\SET{S_n = \sum_{i=1}^n X_i, n \geq 1}$ the corresponding random walk.
Let $J$ be any stopping time with respect to $\SET{X_1, X_2, \ldots}$.
(We allow $J$ to take the value $\infty$,
defining $\Prob{J = \infty} = 1 - \lim_{n \to \infty} \Prob{J \leq n}$.)
Define \dkcomment{I'm making the dependence of $b$ on $n$ explicit,
  since that's how we're using it.}
\begin{align*}
g(n) & = \sqrt{0.6 n \cdot \log (\log_{1.1}(n) + 1) + n \cdot b(n)}.
\end{align*}
Then, 
\begin{align*}
\Prob{J < \infty \mbox{ and } S_J \geq g(J)} \leq 12 \e^{-1.8 b(n)}.
\end{align*}
\end{lemma}

\begin{extraproof}{Lemma~\ref{lem:round-prob}}
Consider any agent type \AgentV{\tau} at the (random) time $\tau$ such that
$\AgentV{\tau} \cdot (\ArmV{\Best{\AgentV{\tau}}} - \ArmV{\Second{\AgentV{\tau}}})
> 2\Diam d x$.
We will use Lemma~\ref{lem:ACI-inequality} to show that
for any arm $i$ and attribute $j = 1, \ldots, d$,
with probability at least
$1 - 24 \exp \left( \frac{-1.8 s x^2}{16\sigma^2} \right)$,
the estimated attribute $j$ for arm $i$ is close enough to the true attribute: 
$|\ArmE{\tau}{i}{j} - \Arm{i}{j}| \leq x$.
By a union bound over all $i, j$, this bound holds simultaneously for all
arms and coordinates with probability at least
$1 - 24 \ARMNUM d \cdot \exp \left( \frac{-1.8 s x^2}{16\sigma^2} \right)$.

Then, consider any type $\AgV = \AgentV{\tau}$ in the high-probability event,
and let $i \neq \Best{\AgentV{\tau}}$ be any other arm.
By the conditioning of \AgentV{\tau}, the true reward from arm $i$ is
much smaller than from the best arm:
$\AgV \cdot (\ArmV{\Best{\AgV}} - \ArmV{i}) > 2\Diam d x$.
Because each coordinate of \ArmEV{\tau}{\Best{\AgV}} and of
\ArmEV{\tau}{i} is estimated accurately to within $x$, 
we get that 
$\AgV \cdot (\ArmEV{\tau}{\Best{\AgV}} - \ArmV{\Best{\AgV}})
\geq - \Diam d x$
and
$\AgV \cdot (\ArmEV{\tau}{i} - \ArmV{i}) \leq \Diam d x$.
Hence, 

\begin{align*}
\AgV \cdot (\ArmEV{\tau}{\Best{\AgV}} - \ArmEV{\tau}{i})
& =
\AgV \cdot (\ArmEV{\tau}{\Best{\AgV}} - \ArmV{\Best{\AgV}})
+ \AgV \cdot (\ArmV{\Best{\AgV}} - \ArmV{i})
+ \AgV \cdot (\ArmV{i} - \ArmEV{\tau}{i})\\
& > -\Diam d x + 2 \Diam d x - \Diam d x
\; = \; 0,
\end{align*}
which means that the agent with type \AgV will in fact choose
the optimal arm in the high-probability event.

It remains to prove that with high probability, all arms' coordinates
are estimated sufficiently accurately for type \AgentV{\tau}.
Thereto, fix an arm $i$ and attribute $j$.
For any $n \geq 1$, let $k_s$ be the time step right after arm $i$ has
been pulled for the \Kth{n} time.
\dkcomment{I'm writing $n$ instead of $s$ here because $S_s$ would
  look ugly, and $n$ is consistent with the lemma.
  Or we could rename $S$ to something else.}
Define
$S_n := \frac{n \cdot (\ArmE{k_n}{i}{j} - \Arm{i}{j})}{2 \sigma}$
to be the sum of all attribute-$j$ noise components up to and
including the \Kth{n} pull of arm $i$,
\dkcomment{I'm not sure if we have defined the notation for components
  of vectors in the \emph{superscript}. Let's make sure not to forget it.}
renormalized to have standard deviation $1/2$.
\dkcomment{Is this the correct scaling.}
The $S_n$ define an unbiased half-subgaussian random walk,
and we can therefore apply Lemma~\ref{lem:ACI-inequality} to them.
Specifically, we set $b(n) = \frac{n x^2}{16 \sigma^2}$,
and obtain that

\begin{align*}
\Prob{J < \infty \mbox{ and } S_J \geq 
\sqrt{0.6 J \cdot \log (\log_{1.1}(J) + 1) + \frac{J^2 x^2}{16 \sigma^2}}}
& \leq 12 \exp \left( \frac{-1.8 J x^2}{16 \sigma^2} \right).
\end{align*}

Applying Lemma~\ref{lem:ACI-inequality} to $-S_n$ with the same choice
of $b(n)$, and taking a union bound over both cases, we obtain that

\begin{align*}
\Prob{J < \infty \mbox{ and } |S_J| \geq 
\sqrt{0.6 J \cdot \log (\log_{1.1}(J) + 1) + \frac{J^2 x^2}{16 \sigma^2}}}
& \leq 24 \exp \left( \frac{-1.8 J x^2}{16 \sigma^2} \right).
\end{align*}

In the high-probability case
$|S_J| \leq
\sqrt{0.6 J \cdot \log (\log_{1.1}(J) + 1) + \frac{J^2 x^2}{16 \sigma^2}}$,
we first use that $\sqrt{\cdot}$ is sublinear to bound
$\sqrt{0.6 J \cdot \log (\log_{1.1}(J) + 1) + \frac{J^2 x^2}{16 \sigma^2}}
\leq \sqrt{0.6 J \cdot \log (\log_{1.1}(J) + 1)} + \frac{J x}{4 \sigma}$.
Then, we use the following technical Lemma~\ref{lem:n0-inequality}
(whose proof is given in the appendix, and whose precondition we verify
momentarily) to bound the first term,
giving us an overall upper bound of
$|S_J| \leq \frac{J x}{2 \sigma}$.

\begin{lemma} \label{lem:n0-inequality}
For any $n \geq \LatePhase = \max (50, \frac{92.16 \sigma^4}{x^4})$, 
\begin{align*}
\sqrt{0.6 n \log (\log_{1.1}(n) + 1)} & \leq \frac{n x}{4 \sigma}. 
\end{align*}
\end{lemma}

Substituting the definition of $S_J$ and canceling common terms,
the inequality implies that
$|\ArmE{k_J}{i}{j} - \Arm{i}{j}| \leq x$.
By the assumptions of the lemma,
the stopping time $\tau$ is such that almost surely, each arm --- and
in particular arm $i$ --- has been pulled at least
$s \geq \LatePhase$ times at time $\tau$.
Defining $J$ to be the number of times that $i$ has been pulled at
time $\tau$, we obtain that $J$ satisfies the precondition of
Lemma~\ref{lem:n0-inequality}.
Furthermore, with probability 1, $J$ will be finite, meaning that
\begin{align*}
& \Prob{S_J \geq 
\sqrt{0.6 J \cdot \log (\log_{1.1}(J) + 1) + \frac{J^2 x^2}{16
    \sigma^2}}}\\
= & \Prob{J < \infty \mbox{ and } S_J \geq 
\sqrt{0.6 J \cdot \log (\log_{1.1}(J) + 1) + \frac{J^2 x^2}{16
    \sigma^2}}}.
\end{align*}

Finally, this choice of $J$ ensures that
$\ArmE{k_J}{i}{j} = \ArmE{\tau}{i}{j}$,
and we have thus shown that
$|\ArmE{\tau}{i}{j} - \Arm{i}{j}| \leq x$ for all $i,j$.
\end{extraproof}

\subsection{Bounding the Total Payment}

Using the above lemmas, we are ready to prove Theorem~\ref{rst:budget}. We include the proof of Theorem~\ref{rst:budget} in the appendix. Below, we bound the expected number of payments for our algorithm.
\begin{lemma}
The expected number of payments for Algorithm~\ref{Alg1} is bounded above by $O(\ARMNUM^2)$.
\label{lemma:numP}
\end{lemma}
\begin{proof}
If $|u_{i}^{j}-u_{i,t}^{j}|\leq \lambda$ is true $\forall i$, $\forall j$, then we know for those $\AgV\in \{\AgV:\AgV\cdot u_{B(\AgV)}-\max_{j\neq B(\AgV)}\{\AgV \cdot u_{j}\}> 2mW\lambda\}$, they will correctly identify their best arm. Thus we know, in the $n^{th}$ round, if $|u_{i}^{j}-u_{i,t}^{j}|\leq \frac{p^{-1}(\frac{p}{2})}{2 \Diam d}$ $\forall i$ and $\forall j$, and $n^{-1}\leq p/2$, we do not need to incentivize any arms. In order to have $n^{-1}\leq \frac{p}{2}$, we need $n\geq \frac{2}{p}$. Denote $n_1=\max\{n_{0}, \frac{2}{p}\}$. Denote $\delta_{0}=p^{-1}(\frac{p}{2})>0$ (because of Assumption~\ref{A1}).

Define $\tau_{n}^{i}$ to be the first time we pull arm $i$ in the $n^{th}$ round. Then
\begin{align}
\sum_{t=1}^{\infty}\mathbbm{1}\{c(t)>0\} =\sum_{n=1}^{\infty}\sum_{i=1}^{N}\mathbbm{1}\{c(\tau_{n}^{i})>0\}. \nonumber
\end{align}

The cumulative expected number of payments is bounded above by:
\begin{align}
&E\left[\sum_{t=1}^{\infty}\mathbbm{1}\{c(t)>0\}\right] \nonumber \\
=&\sum_{n=1}^{\infty}\sum_{i=1}^{N}P(c(\tau_{n}^{i})>0) \nonumber \\
\leq &\sum_{n=n_{1}}^{\infty}\sum_{i=1}^{N}P\left(\exists i,j:|u_{i}^{j}-u_{i,\tau_{n}^{i}}^{j}|>\frac{p^{-1}(\frac{p}{2})}{2 \Diam d}\right)+\sum_{n=1}^{n_1}\ARMNUM \nonumber \\
\leq &\sum_{n=n_{1}}^{\infty}\sum_{i=1}^{N}24 \ARMNUM d
       \exp\left(\frac{-1.8 n\delta_{0}^2}{64 \Diam^2 d^2\sigma^2}\right) +\sum_{n=1}^{n_1}\ARMNUM \nonumber \\
\leq & \sum_{n=n_{1}}^{\infty}24Nm \exp\left(\frac{-1.8 n\delta_{0}^2}{64\Diam^2 d^2\sigma^2}\right)\times \ARMNUM+\sum_{n=1}^{n_1}\ARMNUM \nonumber  \\
\leq&24 \ARMNUM^2 d \frac{1}{\exp(\frac{1.8\delta_{0}}{64\Diam^2 d^2\sigma^2})-1} + Nn_1, \nonumber
\end{align}

Thus, we know the expected number of payments is bounded above by $O(\ARMNUM^2)$.

\end{proof}

\subsection{Bounding the Total Regret}

Now we are ready to prove our second main result, Theorem~\ref{rst:regret}.

\begin{proof}
For regret incurred in the first $n_0$ round, it is bounded above by $\sum_{n=1}^{n_{0}}NRn$.

For regret incurred after the first $n_0$ round, it has two different components: the regret incurred when we let the agents play myopically and the regret incurred when we incentivize the agents. Using Lemma~\ref{lemma:numP}, the expected regret incurred when we incentivize the agents is bounded above by: $\left[24\ARMNUM^2 d \frac{1}{\exp(\frac{1.8\delta_{0}}{64\Diam^2 d^2\sigma^2})-1} + Nn_1\right]R$.

For the regret incurred when we let the agents play myopically at time $t\geq t_{n_0}$, it consists of the following two components:
\begin{itemize}
\item For those users whose utility difference between their best and the second best arm is greater than $f(t)$: we define a sequence of stopping time $\tau_{n}^{k}$ to be the $k^{th}$ time period in the $n^{th}$ round. For $k>t_{n+1}-t_{n}$, we define $\tau_{n}^{k}=\infty$. For $\tau_{n}^{k}=t$, the probability of these users making a mistake is bounded above by $24Nm\exp\left(-\frac{1.8n f(\tau_{n}^{k})^2}{64 \Diam^2 d^2\sigma^2}\right)$ and the expected regret is bounded above by $24Nm\exp\left(-\frac{1.8n f(\tau_{n}^{k})^2}{64 \Diam^2 d^2\sigma^2}\right)\times R$. We denote the regret incurred by these agents as $r_1(\tau_{n}^{k})$. For $k>t_{n+1}-t_{n}$, we define $r_1(\tau_{n}^{k})=0$.
\item For those user whose utility difference between their best and the second best arm is smaller than $f(t)$: this happens with probability $S(f(t))$ at each time and regret is bounded above by $S(f(t)) \times f(t)=Mf(t)^2$. We denote the regret incurred by these agents as $r_2(t)$.
\end{itemize}

Thus, the cumulative expected regret incurred up to time $T$ when we let the agent play myopically is bounded above by:
\begin{align}
&E\left[\sum_{t=1}^{T}r(t)\right] \nonumber \\
=&E\left[\sum_{t=1}^{t_{n_{0}}} r(t) + \sum_{t=t_{n_{0}}}^{T}(r_1(t)+r_2(t))\right]  \nonumber \\
\leq & \sum_{n=1}^{n_{0}}NRn + E\left[\sum_{n=n_{0}}^{T}\sum_{t=t_{n}}^{t_{n+1}-1}r_1(t)\right]+ E\left[\sum_{t=1}^{T}r_2(t)\right] \nonumber \\
=& \sum_{n=1}^{n_{0}}NRn + E\left[\sum_{n=n_{0}}^{T}\sum_{k=1}^{\infty}r_1(\tau_{n}^{k})\right]+ E\left[\sum_{t=1}^{T}r_2(t)\right]. \label{chap5:equ:r1}
\end{align}
Since
\begin{align}
& E\left[\sum_{n=n_{0}}^{T}\sum_{k=1}^{\infty}r_1(\tau_{n}^{k})\right] \nonumber\\
= &  \sum_{n=n_{0}}^{T}\sum_{k=1}^{\infty}E[r_1(\tau_{n}^{k})] \nonumber \\
= &  \sum_{n=n_{0}}^{T}\sum_{k=1}^{\infty}(E[r_1(\tau_{n}^{k})|\tau_{n}^{k}<\infty]\times P(\tau_{n}^{k}<\infty)+E[r_1(\tau_{n}^{k})|\tau_{n}^{k}=\infty]\times P(\tau_{n}^{k}=\infty)) \nonumber \\
= & \sum_{n=n_{0}}^{T}\sum_{k=1}^{\infty}E[r_1(\tau_{n}^{k})|\tau_{n}^{k}<\infty]\times P(\tau_{n}^{k}<\infty), \nonumber
\end{align}

we have
\begin{align}
\eqref{chap5:equ:r1}= & \sum_{n=1}^{n_{0}}NRn + \sum_{n=n_{0}}^{T}\sum_{k=1}^{\infty}E[r_1(\tau_{n}^{k})|\tau_{n}^{k}<\infty]\times P(\tau_{n}^{k}<\infty) + E\left[\sum_{t=1}^{T}r_2(t)\right] \nonumber \\
\leq & \sum_{n=1}^{n_{0}}NRn + \sum_{n=n_{0}}^{T}\left[\sum_{k=1}^{\infty}24Nm\exp\left(-\frac{1.8n f(\tau_{n}^{k})^2}{64 \Diam^2 d^2\sigma^2}\right) R\times P(\tau_{n}^{k}<\infty) \right]+ \sum_{k=1}^{T}Mf(t)^2 \nonumber \\
\leq & \sum_{n=1}^{n_{0}}NRn + \sum_{n=1}^{T} 24Nm\exp\left(-\frac{1.8n f(n)^2}{64 \Diam^2 d^2\sigma^2}\right) R \times Nn+ \sum_{t=1}^{T}Mf(t)^2. \label{chap5:equ:regret}
\end{align}

Thus the cumulative regret at time $T$ is bounded above by
\begin{align}
&\sum_{n=1}^{n_{0}}NRn + \sum_{n=1}^{T} 24Nm\exp\left(-\frac{1.8n f(n)^2}{64 \Diam^2 d^2\sigma^2}\right)\times R \times Nn+ \sum_{t=1}^{T}Mf(t)^2 \nonumber \\
+ & 24\ARMNUM^2 d \frac{1}{e^{\frac{1.8\delta_{0}}{64\Diam^2 d^2\sigma^2}}-1}R+\ARMNUM\left(\max\left\{n_{0},\frac{2}{p}\right\}\right)R. \nonumber
\end{align}

For a fixed $T$, we only need to minimize the following two terms since all others are constant:

\begin{align}
\sum_{n=1}^{T} 24Nm\exp\left(-\frac{1.8n f(n)^2}{64 \Diam^2 d^2\sigma^2}\right)\times R \times Nn+ \sum_{t=1}^{T}Mf(t)^2. \label{equ:regret}
\end{align}

If we set $f^2(t)=\frac{2\log(T)\times 64\Diam^2 d^2\sigma^2}{1.8t}$, then
\begin{align}
&\sum_{n=1}^{T} 24Nm\exp\left(-\frac{1.8n f(n)^2}{64 \Diam^2 d^2\sigma^2}\right)\times R \times Nn+ \sum_{t=1}^{T}Mf(t)^2 \nonumber \\ 
\leq & \sum_{n=1}^{T} 24\ARMNUM^2 mnR \exp\left(-2\log(T)\right)  + \frac{128\Diam^2 d^2\sigma^2 \TieDensity\log(T)}{1.8}\sum_{t=1}^{T}\frac{1}{n} \nonumber \\
\leq &  24\ARMNUM^2 d R\frac{T(T-1)}{2T^2}  + 71.12 \Diam^2 d^2\sigma^2 \TieDensity\log(T)(\log(T)+1) \nonumber \\
\leq &  12 \ARMNUM^2 d R  + 71.12 \Diam^2 d^2\sigma^2 \TieDensity\log(T)(\log(T)+1). \nonumber
\end{align}

Thus, the cumulative expected regret is bounded by $O(\ARMNUM^2 d + \TieDensity d^2(\log(T))^2)$.
\end{proof}

\begin{corollary}
If $\exists \delta>0$ such that $F_{i,j}(\delta)=0$ for all $i,j$, then the cumulative expected regret is bounded by $O(\ARMNUM^2)$.
\end{corollary}

\begin{proof}
The proof of this corollary is similar to the proof of Theorem~\ref{rst:regret}. If we set $f^2(t)=\frac{2\log(T)\times 64\Diam^2 d^2\sigma^2}{1.8t}$, then there exists a $t_{0}$ such that for $t>t_{0}$, $S(f(t))=0$. Thus, similar to equation~\eqref{equ:regret}, we know the cumulative expected regret when we let the agents play myopically is bounded above by
\begin{align}
\sum_{n=1}^{n_{0}}NRn + \sum_{n=1}^{T} 24Nm\exp\left(-\frac{1.8n f(n)^2}{64 \Diam^2 d^2\sigma^2}\right)\times R \times Nn+ \sum_{t=1}^{t_{0}}Mf(t)^2 \nonumber
\end{align}
Therefore, based on the same analysis of Theorem~\ref{rst:regret}, we know the cumulative regret is bounded by $O(\ARMNUM^2)$.
\end{proof}

\subsection{Decreasing the Payment} \label{sec:pi}
\dkcomment{Does this really need a whole subsection now?}

In Algorithm~\ref{alg:basic-incentivizing},
the payment \Pay{t}{i} offered for pulling payment-eligible arms $i$
is high enough to ensure that arm $i$ will be pulled with probability 1.
However, as can be seen in the proof of Lemma~\ref{lem:phase-length},
we then lower-bound this probability by $1/s$.
Thus, the bounds we prove do not deteriorate as long as the payment is
high enough such that arm $i$ is pulled with probability at least $1/s$.
A sufficient payment would be defined as
$\Pay{t}{i} = \sup \Set{c \geq 0}{%
  \Prob[\AgV \sim \AgentDist]{c + \AgV \cdot \ArmEV{t}{i} \leq \AgV \cdot \ArmEV{t}{i'}}}$.
Except for offering different payments, the algorithm remains unchanged.
Then, the exact same analysis shows that the modified algorithm
provides the same guarantees.
