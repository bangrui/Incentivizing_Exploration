
\section{Algorithm and Upper Bound}
\label{sec:ub}

In this section, we propose a simple policy that mostly exploits, and occasionally incentivizes exploration when the probability of an arm would be pulled by all agent types below a time-varying threshold given the current posterior. We prove that with the help of heterogeneous preferences, we can get a certain amount of exploration for free via heterogeneity. 

\subsection{Our Algorithm}
Our algorithm incentivizes pulling an arm $i$ at a time $t$ in round $n$ if and only if both of the following criteria are met:
\begin{itemize}
\item the probability of pulling arm $i$ would be below $n^{-1}$ without incentives; 
\item arm $i$ has not been played previously in the current round.
\end{itemize}
Ties are broken randomly.  This algorithm does not need to know the horizon $T$ in advance. 

If our algorithm decides to incentivize an arm $i$, it uses the ``pay whatever it takes'' strategy in which the payment offered is $\max_{\theta,j} \theta \cdot (u_{j,t} - u_{i,t})$. This maximum over $\theta$ is taken over the support of $F$, which we recall is assumed compact.  (We use this ``pay whatever it takes'' strategy for its simplicity, and in Section~\ref{sec:pi} we provide an alternate and smaller incentive payment that achieves the same payment budget bound and regret bound). 

We describe our algorithm in detail as follows:


\begin{algorithm}
\caption{Algorithm: Incentivizing Exploration}
\label{Alg1}
\begin{algorithmic}
\STATE Set n = 1 to denote the round number; Let $V=\emptyset$ be the set of arms that were pulled in the current round;
\FOR{ $t = 1, 2, 3, \cdots$}{
  \STATE Let $S = \{ i : P( \theta \cdot u_{i,t} > \theta \cdot u_{j,t}\ \forall j\ne i | u_{j,t}\ \forall j) < n^{-1}\}$ be the set of arms with unincentivized probability of being pulled below $n^{-1}$.
    \IF {$S\setminus V$ is non-empty}
  \STATE{Choose an arm $i$ uniformly at random from $S\setminus V$}
  \STATE{Pay whatever it takes to incentivize pulling arm $i$, i.e., offer payment 
    $c_{i,t} = \max_{\theta,j} \theta \cdot (u_{j,t} - u_{i,t})$ and $c_{j,t} = 0$ for $j \ne i$.}
  \ELSE
    \STATE {Let agents play myopically, i.e., offer payment $c_{j,t} = 0$ for all $j$}
  \ENDIF
    \STATE Denote $A_t$ as the pulled arm, update $V=V\cup\{A_t\}$, $u_{A_t,t}$ and $N(A_t,t)$
    \IF {$n\neq \min_{i}N(i,t)$} 
  \STATE $V=\emptyset$
    \ENDIF
    \STATE Update the round number, $n = \min_{i} N(i,t)$
}\ENDFOR

\end{algorithmic}
\end{algorithm}


\subsection{Assumptions}
In this section, we state several assumptions assumed by our analysis.  First define
\begin{align}
\Omega_{i,j}=\{\theta:B(\theta)=i, \hat{B}(\theta)=j\}, \nonumber 
\end{align}
which is the set of agent preferences whose best arm is arm $i$ and second best arm is arm $j$. With this definition, our analysis makes the following assumptions:

\begin{assumption} Let $F_{i,j}(y)$ be the marginal cumulative density function (or cumulative mass function if $F(\cdot)$ is a discrete distribution) of $(u_i-u_j)\cdot\theta$ conditioned on $\theta \in \Omega_{i,j}$. We assume $F_{i,j}(y)\leq My$ for all $y\in R^{+}$, $\forall i,j$. 
\label{A1}
\end{assumption}

As we can see later in our proof, we only need $\max_{i,j}\limsup_{y\rightarrow 0^{+}}\frac{F_{i,j}(y)}{y}$ to be finite. Intuitively, assumption~\ref{A1} states that there are not many agents who are indifferent between their best arm and the second best arm. 

\begin{assumption} We assume $F$ has a compact support set. Without loss of generality, we assume $\theta\in [0,W]^m$.
\label{A2}
\end{assumption}

We use $R = \max_{\theta, i,j} \theta \cdot (u_i - u_j)$ to denote the maximum regret that can be incurred at each time.  Assumption~\ref{A2} shows that $R<\infty$.

\begin{assumption}
Denote $p=\min_{i}P(\{\theta: B(\theta)=i\})$. We assume $p>0$.
\label{A3}
\end{assumption}

Assumption~\ref{A3} means each arm $i$ has a strictly positive proportion of users for which that arm is best. 

\subsection{General Results}

In this section, we prove Algorithm~\ref{Alg1} achieves $O(N^2+M(\log(T))^2)$ cumulative regret with $O(N^2)$ payment budget.  This is stated in the following pair of theorems, which together constitute our main results.

\begin{theorem}
The payment budget for Algorithm~\ref{Alg1} is bounded above by $O(N^2)$. 
\label{rst:budget}
\end{theorem}


\begin{theorem}
The cumulative regret for Algorithm~\ref{Alg1} is bounded above by $O(N^2 m + M m^2(\log(T))^2)$.
\label{rst:regret}
\end{theorem}



Before we prove these two theorems, we must first introduce two additional pieces of notation, which will be used in preliminary lemmas.  Let $S(\delta)$ be the proportion of users whose utility difference between their best and second best arm is less than $\delta$. Formally, $S(\delta)=P(\theta: \theta \cdot u_{B(\theta)}-\theta\cdot u_{\hat{B}(\theta)}\leq \delta)$. Then, let $p(\delta)=\min_{i}P(\{\theta:B(\theta)=i,\theta\cdot u_{B(\theta)}-\theta\cdot u_{\hat{B}(\theta)}>\delta\})$. We know $p(0)=p$. 

With this additional notation, we now prove several lemmas.
First, based on Assumption~\ref{A1}, we have the following bound for $S(\delta)$.

\begin{lemma}
$S(\delta)\leq M\delta$.
\label{lemma:sdelta}
\end{lemma}

\begin{proof}
\begin{align*}
S(\delta)&=\sum_{i,j}P(\theta\cdot(u_{i}-u_{j})\le \delta|\theta\in \Omega_{i,j})P(\theta\in \Omega_{i,j}) \\
&\leq \sum_{i,j}M\delta \times P(\theta\in \Omega_{i,j}) \\
&=M\delta.
\end{align*}
\end{proof}

The following lemma bounds the probability of making a mistake if we let the agents play myopically in the $n^{th}$ round, given that the utility difference between his/her best and second best arm is bounded below by a constant. 

\begin{lemma}
Define $\tau$ to be any stopping time that is almost surely between $t_n$ and $t_{n+1}-1$ with respect to the filtration $\mathcal{F}_{t}=\sigma(A_1,\cdots,A_t,c_1,\cdots,c_t,O_1,\cdots,O_t)$, we have 
\begin{align}
P(\arg\max\{\theta_{\tau}\cdot u_{i,\tau}\}\neq B(\theta_{\tau})|\theta_{\tau}\cdot(u_{B(\theta_{\tau})}-u_{\hat{B}(\theta_{\tau})})> 2Wm\lambda)\leq 24Nm\exp\left(-\frac{1.8n\lambda^2}{16\sigma^2}\right), \nonumber
\end{align}
for $n\geq n_{0}=\max\{50, \frac{92.16\sigma^4}{\lambda^4}\}$.
\label{round:prob}
\end{lemma}


We need the following lemma in order to prove Lemma~\ref{round:prob} and we include the proof in the appendix.

\begin{lemma}
For $n\geq n_{0}=\max\{50, \frac{92.16\sigma^4}{\lambda^4}\}$, we have
\begin{align}
\frac{n\lambda}{4\sigma}\geq \sqrt{0.6n\log(\log_{1.1}(n)+1)}. \nonumber
\end{align}
\label{n0-inequality}
\end{lemma}

To prove Lemma~\ref{round:prob}, we also need to use an adaptive concentration inequality due to \cite{zhao2016adaptive}. For reference, we state it here as a Lemma.

\begin{lemma}[Corollary 1 in \cite{zhao2016adaptive}]
Let $X_{i}$ be zero mean $1/2$-subgaussian random variables. $\{S_{n}=\sum_{i=1}^{n}X_{i},n\geq 1\}$ be a random walk. Let $J$ be any stopping time with respect to $\{X_1,X_2,\cdots\}$. We allow $J$ to take the value of $\infty$ where $P(J=\infty)=1-\lim_{n\rightarrow \infty}P(J\leq n)$. If
\begin{align}
f(n)=\sqrt{0.6n\log(\log_{1.1}(n)+1)+bn}, \nonumber
\end{align}
then
\begin{align}
Pr[\{S_{J}\geq f(J)\}\cap \{J<\infty\}]\leq 12e^{-1.8b}. \nonumber
\end{align}
\label{ACI-inequality}
\end{lemma}


We now prove Lemma~\ref{round:prob}.

\begin{proof}[Proof of Lemma~\ref{round:prob}]
In the $n^{th}$ round, we know all arms have been pulled at least $n$ times. For all the agents $\theta$ whose utility difference between their best and second best arm is greater than $2mW\lambda$, denote $K(\theta)=\max_{i\neq B(\theta)}\{\theta\cdot u_{i,t}\}$. If $|u_{i,t}^{j}-u_{i}^{j}|\leq \lambda$ for all $i,j$, then
\begin{align}
&\theta\cdot(u_{B(\theta),t}-u_{K(\theta),t}) \nonumber \\
\geq & \theta\cdot(u_{B(\theta),t}-u_{B(\theta)}) + \theta\cdot(u_{K(\theta)}-u_{K(\theta),t}) + \theta\cdot(u_{B(\theta)}-u_{K(\theta)}) \nonumber \\
> & -Wm\lambda - Wm\lambda + 2Wm\lambda = 0,\nonumber
\end{align}
which means their myopic action would incur no regret.


Define $\epsilon_{i,\tau}=u_{i,\tau}-u_i$ and $\epsilon_{i,\tau}^{j}$ to be the $j^{th}$ component of $\epsilon_{i,\tau}$. Thus, we have
\begin{align}
&P(\arg\max\{\theta_{\tau}\cdot u_{i,\tau}\}\neq B(\theta_{\tau})|\theta_{\tau}\cdot(u_{B(\theta_{\tau})}-u_{\hat{B}(\theta_{\tau})})> 2Wm\lambda)\nonumber \\
\leq &P(\exists i, \exists j, |u_{i,\tau}^{j}-u_{i}^{j}|\geq\lambda |\theta_{\tau}\cdot(u_{B(\theta_{\tau})}-u_{\hat{B}(\theta_{\tau})})> 2Wm\lambda) \nonumber \\
\leq & \sum_{i}\sum_{j} P(|u_{i,\tau}^{j}-u_{i}^{j}|\geq\lambda|\theta_{\tau}\cdot(u_{B(\theta_{\tau})}-u_{\hat{B}(\theta_{\tau})})> 2Wm\lambda) \nonumber \\
= &  \sum_{i}\sum_{j} P(|\epsilon_{i,\tau}^{j}|\geq\lambda|\theta_{\tau}\cdot(u_{B(\theta_{\tau})}-u_{\hat{B}(\theta_{\tau})})> 2Wm\lambda). \label{ACI}
\end{align}

To bound equation~(\ref{ACI}), we use Lemma~\ref{ACI-inequality}. Define
\begin{align}
S_{N(i,\tau)}^{i,j}=\frac{\epsilon_{i,\tau}^{j}}{2\sigma}. \nonumber
\end{align}

Based on Lemma~\ref{n0-inequality}, for $n_{0}=\max\{50,\frac{92.16\sigma^2}{\lambda^2}\}$ and $n\geq n_{0}$, we have
\begin{align}
\frac{n\lambda}{4\sigma}\geq \sqrt{0.6n\log(\log_{1.1}(n)+1)}. \nonumber
\end{align}

Thus, if we set $b=\frac{n\lambda^2}{16\sigma^2}$ in Lemma~\ref{ACI-inequality}, for any $N(i,\tau)\geq n\geq n_{0}$, we have
\begin{align}
\frac{N(i,\tau)\lambda}{2\sigma}\geq & \sqrt{0.6N(i,\tau)\log(\log_{1.1}(N(i,\tau))+1)}+\frac{\lambda}{4\sigma}\sqrt{n N(i,\tau)} \nonumber \\
\geq & \sqrt{0.6N(i,\tau)\log(\log_{1.1}(N(i,\tau))+1)+bN(i,\tau)}, \nonumber 
\end{align}
where the last inequality is because $\sqrt{x}+\sqrt{y}\geq \sqrt{x+y}$. Thus, we have
\begin{align}
&P(\epsilon_{i,\tau}^{j}\geq\lambda|\theta_{\tau}\cdot(u_{B(\theta_{\tau})}-u_{\hat{B}(\theta_{\tau})})> 2Wm\lambda) \nonumber \\
=&P\left(S_{N(i,\tau)}^{i,j}\geq \frac{N(i,\tau)\lambda}{2\sigma}\bigg|\theta_{\tau}\cdot(u_{B(\theta_{\tau})}-u_{\hat{B}(\theta_{\tau})})> 2Wm\lambda\right) \nonumber \\
\leq & P\left(S_{N(i,\tau)}^{i,j}\geq \sqrt{0.6 N_{i,\tau}\log(\log_{1.1}(N(i,\tau))+1)+b N(i,\tau)}\bigg|\theta_{\tau}\cdot(u_{B(\theta_{\tau})}-u_{\hat{B}(\theta_{\tau})})> 2Wm\lambda\right) \nonumber \\
\leq & 12\exp( -1.8b) = 12\exp\left(\frac{-1.8 n\lambda^2}{16\sigma^2}\right). \nonumber
\end{align}
Similarily, we can bound 
\begin{align}
&P(\epsilon_{i,\tau}^{j}\leq-\lambda|\theta_{\tau}\cdot(u_{B(\theta_{\tau})}-u_{\hat{B}(\theta_{\tau})})> 2Wm\lambda) \nonumber \\
=&P(-\epsilon_{i,\tau}^{j}\geq \lambda|\theta_{\tau}\cdot(u_{B(\theta_{\tau})}-u_{\hat{B}(\theta_{\tau})})> 2Wm\lambda) \nonumber \\
\leq & 12\exp\left(\frac{-1.8 n\lambda^2}{16\sigma^2}\right). \nonumber 
\end{align}

Therefore, we know $P(|\epsilon_{i,\tau}^{j}|\geq \lambda|\theta_{\tau}\cdot(u_{B(\theta_{\tau})}-u_{\hat{B}(\theta_{\tau})})> 2Wm\lambda)\leq 24\exp\left(\frac{-1.8 n\lambda^2}{16\sigma^2}\right)$. Thus, we know
\begin{align}
&\sum_{i}\sum_{j} P(|\epsilon_{i,\tau}^{j}|\geq \lambda|\theta_{\tau}\cdot(u_{B(\theta_{\tau})}-u_{\hat{B}(\theta_{\tau})})> 2Wm\lambda)  \nonumber \\
\leq& 24Nm \exp\left(\frac{-1.8 n\lambda^2}{16\sigma^2}\right). \nonumber
\end{align}
\end{proof}

Before we start analyzing the cumulative regret, we first prove the following lemma which bounds the expected length of each round.
\begin{lemma}
Using our algorithm, we have $\mathbb{E}[t_{n+1}-t_{n}]\leq Nn$, $\forall n\geq 1$.
\label{round:length}
\end{lemma}
\begin{proof}
A round completes when each arm is pulled at least once in that round. Let $X_{i}$ be the number of agents who come to the system between the time after the $(i-1)^{th}$ unique arm was pulled, up to and including the time when the $i^{th}$ unique arm was pulled. Then we know 
\begin{equation*}
\mathbb{E}[t_{n+1}-t_{n}]=\sum_{i=1}^{N}E[X_{i}].
\end{equation*}


Fix $i$. In bounding $X_i$, we think of agents as ``trials'', where each trial can result in a new unique arm being pulled (which we call a ``successful'' trial), or not.  There are two ways a trial can be successful:
\begin{itemize}
\item If there is at least one arm that has not been pulled and the probability of an agent utility function that would pull this arm without incentives is less than $n^{-1}$, then the principal will offer an incentive that causes this arm to be pulled (or one of these arms if there is more than one). In this case, the probability that the trial is succesful is $1$.  
\item The probability of an agent utility function that would pull each un-pulled arm without incentives is at least $n^{-1}$. In this case, the probability that the trial is successful is at least $n^{-1}$.
\end{itemize}

Thus, $X_{i}$ is stochastically dominated below by a geometric random variable with success probability $n^{-1}$, the expected number of trials up to and including the first success, $E[X_i]$, is bounded above by $n$.  Thus,
\begin{align}
E[t_{n+1}-t_{n}]\leq Nn. \nonumber
\end{align}
\end{proof}
                  
Using the above lemmas, we are ready to prove Theorem~\ref{rst:budget}. We include the proof of Theorem~\ref{rst:budget} in the appendix. Below, we bound the expected number of payments for our algorithm.
\begin{lemma}
The expected number of payments for Algorithm~\ref{Alg1} is bounded above by $O(N^2)$.
\label{lemma:numP}
\end{lemma}
\begin{proof}
If $|u_{i}^{j}-u_{i,t}^{j}|\leq \lambda$ is true $\forall i$, $\forall j$, then we know for those $\theta\in \{\theta:\theta\cdot u_{B(\theta)}-\max_{j\neq B(\theta)}\{\theta \cdot u_{j}\}> 2mW\lambda\}$, they will correctly identify their best arm. Thus we know, in the $n^{th}$ round, if $|u_{i}^{j}-u_{i,t}^{j}|\leq \frac{p^{-1}(\frac{p}{2})}{2Wm}$ $\forall i$ and $\forall j$, and $n^{-1}\leq p/2$, we do not need to incentivize any arms. In order to have $n^{-1}\leq \frac{p}{2}$, we need $n\geq \frac{2}{p}$. Denote $n_1=\max\{n_{0}, \frac{2}{p}\}$. Denote $\delta_{0}=p^{-1}(\frac{p}{2})>0$ (because of Assumption~\ref{A1}).

Define $\tau_{n}^{i}$ to be the first time we pull arm $i$ in the $n^{th}$ round. Then
\begin{align}
\sum_{t=1}^{\infty}\mathbbm{1}\{c(t)>0\} =\sum_{n=1}^{\infty}\sum_{i=1}^{N}\mathbbm{1}\{c(\tau_{n}^{i})>0\}. \nonumber
\end{align}

The cumulative expected number of payments is bounded above by:
\begin{align}
&E\left[\sum_{t=1}^{\infty}\mathbbm{1}\{c(t)>0\}\right] \nonumber \\
=&\sum_{n=1}^{\infty}\sum_{i=1}^{N}P(c(\tau_{n}^{i})>0) \nonumber \\
\leq &\sum_{n=n_{1}}^{\infty}\sum_{i=1}^{N}P\left(\exists i,j:|u_{i}^{j}-u_{i,\tau_{n}^{i}}^{j}|>\frac{p^{-1}(\frac{p}{2})}{2Wm}\right)+\sum_{n=1}^{n_1}N \nonumber \\
\leq &\sum_{n=n_{1}}^{\infty}\sum_{i=1}^{N}24Nm \exp\left(\frac{-1.8 n\delta_{0}^2}{64W^2 m^2\sigma^2}\right) +\sum_{n=1}^{n_1}N \nonumber \\
\leq & \sum_{n=n_{1}}^{\infty}24Nm \exp\left(\frac{-1.8 n\delta_{0}^2}{64W^2 m^2\sigma^2}\right)\times N+\sum_{n=1}^{n_1}N \nonumber  \\
\leq&24N^2 m \frac{1}{\exp(\frac{1.8\delta_{0}}{64W^2 m^2\sigma^2})-1} + Nn_1, \nonumber
\end{align}

Thus, we know the expected number of payments is bounded above by $O(N^2)$.

\end{proof}

Now we are ready to prove our second main result, Theorem~\ref{rst:regret}.
\begin{proof}
For regret incurred in the first $n_0$ round, it is bounded above by $\sum_{n=1}^{n_{0}}NRn$.

For regret incurred after the first $n_0$ round, it has two different components: the regret incurred when we let the agents play myopically and the regret incurred when we incentivize the agents. Using Lemma~\ref{lemma:numP}, the expected regret incurred when we incentivize the agents is bounded above by: $\left[24N^2 m \frac{1}{\exp(\frac{1.8\delta_{0}}{64W^2 m^2\sigma^2})-1} + Nn_1\right]R$.

For the regret incurred when we let the agents play myopically at time $t\geq t_{n_0}$, it consists of the following two components:
\begin{itemize}
\item For those users whose utility difference between their best and the second best arm is greater than $f(t)$: we define a sequence of stopping time $\tau_{n}^{k}$ to be the $k^{th}$ time period in the $n^{th}$ round. For $k>t_{n+1}-t_{n}$, we define $\tau_{n}^{k}=\infty$. For $\tau_{n}^{k}=t$, the probability of these users making a mistake is bounded above by $24Nm\exp\left(-\frac{1.8n f(\tau_{n}^{k})^2}{64 W^2 m^2\sigma^2}\right)$ and the expected regret is bounded above by $24Nm\exp\left(-\frac{1.8n f(\tau_{n}^{k})^2}{64 W^2 m^2\sigma^2}\right)\times R$. We denote the regret incurred by these agents as $r_1(\tau_{n}^{k})$. For $k>t_{n+1}-t_{n}$, we define $r_1(\tau_{n}^{k})=0$.
\item For those user whose utility difference between their best and the second best arm is smaller than $f(t)$: this happens with probability $S(f(t))$ at each time and regret is bounded above by $S(f(t)) \times f(t)=Mf(t)^2$. We denote the regret incurred by these agents as $r_2(t)$.
\end{itemize}

Thus, the cumulative expected regret incurred up to time $T$ when we let the agent play myopically is bounded above by:
\begin{align}
&E\left[\sum_{t=1}^{T}r(t)\right] \nonumber \\
=&E\left[\sum_{t=1}^{t_{n_{0}}} r(t) + \sum_{t=t_{n_{0}}}^{T}(r_1(t)+r_2(t))\right]  \nonumber \\
\leq & \sum_{n=1}^{n_{0}}NRn + E\left[\sum_{n=n_{0}}^{T}\sum_{t=t_{n}}^{t_{n+1}-1}r_1(t)\right]+ E\left[\sum_{t=1}^{T}r_2(t)\right] \nonumber \\
=& \sum_{n=1}^{n_{0}}NRn + E\left[\sum_{n=n_{0}}^{T}\sum_{k=1}^{\infty}r_1(\tau_{n}^{k})\right]+ E\left[\sum_{t=1}^{T}r_2(t)\right]. \label{chap5:equ:r1}
\end{align}
Since
\begin{align}
& E\left[\sum_{n=n_{0}}^{T}\sum_{k=1}^{\infty}r_1(\tau_{n}^{k})\right] \nonumber\\
= &  \sum_{n=n_{0}}^{T}\sum_{k=1}^{\infty}E[r_1(\tau_{n}^{k})] \nonumber \\
= &  \sum_{n=n_{0}}^{T}\sum_{k=1}^{\infty}(E[r_1(\tau_{n}^{k})|\tau_{n}^{k}<\infty]\times P(\tau_{n}^{k}<\infty)+E[r_1(\tau_{n}^{k})|\tau_{n}^{k}=\infty]\times P(\tau_{n}^{k}=\infty)) \nonumber \\
= & \sum_{n=n_{0}}^{T}\sum_{k=1}^{\infty}E[r_1(\tau_{n}^{k})|\tau_{n}^{k}<\infty]\times P(\tau_{n}^{k}<\infty), \nonumber
\end{align}

we have
\begin{align}
\eqref{chap5:equ:r1}= & \sum_{n=1}^{n_{0}}NRn + \sum_{n=n_{0}}^{T}\sum_{k=1}^{\infty}E[r_1(\tau_{n}^{k})|\tau_{n}^{k}<\infty]\times P(\tau_{n}^{k}<\infty) + E\left[\sum_{t=1}^{T}r_2(t)\right] \nonumber \\
\leq & \sum_{n=1}^{n_{0}}NRn + \sum_{n=n_{0}}^{T}\left[\sum_{k=1}^{\infty}24Nm\exp\left(-\frac{1.8n f(\tau_{n}^{k})^2}{64 W^2 m^2\sigma^2}\right) R\times P(\tau_{n}^{k}<\infty) \right]+ \sum_{k=1}^{T}Mf(t)^2 \nonumber \\
\leq & \sum_{n=1}^{n_{0}}NRn + \sum_{n=1}^{T} 24Nm\exp\left(-\frac{1.8n f(n)^2}{64 W^2 m^2\sigma^2}\right) R \times Nn+ \sum_{t=1}^{T}Mf(t)^2. \label{chap5:equ:regret}
\end{align}

Thus the cumulative regret at time $T$ is bounded above by
\begin{align}
&\sum_{n=1}^{n_{0}}NRn + \sum_{n=1}^{T} 24Nm\exp\left(-\frac{1.8n f(n)^2}{64 W^2 m^2\sigma^2}\right)\times R \times Nn+ \sum_{t=1}^{T}Mf(t)^2 \nonumber \\
+ & 24N^2 m \frac{1}{e^{\frac{1.8\delta_{0}}{64W^2 m^2\sigma^2}}-1}R+N\left(\max\left\{n_{0},\frac{2}{p}\right\}\right)R. \nonumber
\end{align}

For a fixed $T$, we only need to minimize the following two terms since all others are constant:

\begin{align}
\sum_{n=1}^{T} 24Nm\exp\left(-\frac{1.8n f(n)^2}{64 W^2 m^2\sigma^2}\right)\times R \times Nn+ \sum_{t=1}^{T}Mf(t)^2. \label{equ:regret}
\end{align}

If we set $f^2(t)=\frac{2\log(T)\times 64W^2 m^2\sigma^2}{1.8t}$, then
\begin{align}
&\sum_{n=1}^{T} 24Nm\exp\left(-\frac{1.8n f(n)^2}{64 W^2 m^2\sigma^2}\right)\times R \times Nn+ \sum_{t=1}^{T}Mf(t)^2 \nonumber \\ 
\leq & \sum_{n=1}^{T} 24N^2 mnR \exp\left(-2\log(T)\right)  + \frac{128W^2 m^2\sigma^2 M\log(T)}{1.8}\sum_{t=1}^{T}\frac{1}{n} \nonumber \\
\leq &  24N^2 m R\frac{T(T-1)}{2T^2}  + 71.12 W^2 m^2\sigma^2 M\log(T)(\log(T)+1) \nonumber \\
\leq &  12 N^2 m R  + 71.12 W^2 m^2\sigma^2 M\log(T)(\log(T)+1). \nonumber
\end{align}

Thus, the cumulative expected regret is bounded by $O(N^2 m + M m^2(\log(T))^2)$.
\end{proof}

\begin{corollary}
If $\exists \delta>0$ such that $F_{i,j}(\delta)=0$ for all $i,j$, then the cumulative expected regret is bounded by $O(N^2)$.
\end{corollary}

\begin{proof}
The proof of this corollary is similar to the proof of Theorem~\ref{rst:regret}. If we set $f^2(t)=\frac{2\log(T)\times 64W^2 m^2\sigma^2}{1.8t}$, then there exists a $t_{0}$ such that for $t>t_{0}$, $S(f(t))=0$. Thus, similar to equation~\eqref{equ:regret}, we know the cumulative expected regret when we let the agents play myopically is bounded above by
\begin{align}
\sum_{n=1}^{n_{0}}NRn + \sum_{n=1}^{T} 24Nm\exp\left(-\frac{1.8n f(n)^2}{64 W^2 m^2\sigma^2}\right)\times R \times Nn+ \sum_{t=1}^{t_{0}}Mf(t)^2 \nonumber
\end{align}
Therefore, based on the same analysis of Theorem~\ref{rst:regret}, we know the cumulative regret is bounded by $O(N^2)$.
\end{proof}

\subsection{Practical Issues}
\label{sec:pi}
In Algorithm~\ref{Alg1}, we use ``pay whatever it takes'' strategy when we decide to incentivize the agent. However, ''pay whatever it takes'' only shows up in the proof of Lemma~\ref{round:length}. Without loss of generality, suppose we want to incentivize arm $i$ at time $t$ at the $n^{th}$ round. Based on the proof of Lemma~\ref{round:length}, as long as we offer a payment $c_{i,t}$ such that arm $i$ has at least $n^{-1}$ probability being pulled at time $t$, our results still hold true. We could compute this $c_{i,t}$ dynamically based on $F(\cdot)$ as well as our current estimate $u_{i,t}$. Here is the revised algorithm which would work well in practice:

\begin{algorithm}
\caption{Algorithm: Incentivizing Exploration}
\label{Alg2}
\begin{algorithmic}
\STATE Set n = 1 to denote the round number; Let $V=\emptyset$ be the set of arms that were pulled in the current round;
\FOR{ $t = 1, 2, 3, \cdots$}{
\STATE Let $S = \{ i : P( \theta \cdot u_{i,t} > \theta \cdot u_{j,t}\ \forall j\ne i | u_{j,t}\ \forall j) < n^{-1}\}$ be the set of arms with unincentivized probability of being pulled below $n^{-1}$.
\IF {$S\setminus V$ is non-empty}
\STATE{Choose an arm $i$ uniformly at random from $S\setminus V$}
\STATE{Offer payment $c_{i,t}=\inf\{c: P(\theta\sim F: \theta\cdot u_{i,t}+c>max_{j}\theta\cdot u_{j,t})>n^{-1}\}$}
\ELSE
\STATE {Let agents play myopically, i.e., offer payment $c_{j,t} = 0$ for all $j$}
\ENDIF
\STATE Denote $A_t$ as the pulled arm, update $V=V\cup\{A_t\}$, $u_{A_t,t}$ and $N(A_t,t)$
\IF {$n\neq \min_{i}N(i,t)$} 
\STATE $V=\emptyset$
\ENDIF
\STATE Update the round number, $n = \min_{i} N(i,t)$
}\ENDFOR

\end{algorithmic}
\end{algorithm}


The same proof would work and we can get the exact same results as Algorithm~\ref{Alg1}. 


