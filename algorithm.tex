\section{Main Algorithm and Analysis}
\label{sec:ub}
The algorithm achieving the claimed bounds of
Theorems~\ref{rst:budget} and~\ref{rst:regret} is fairly simple.
It mostly allows agents to exploit, but when an arm is sufficiently
unlikely to be pulled, 
the algorithm incentivizes this arm with a payment high enough
to guarantee that the next agent pulls it.
This way the algorithm ensures that each arm is pulled often enough.

More precisely, the algorithm divides time into \emph{phases}
$s = 1, 2, 3, \ldots$.
Phase $s$ starts when each arm has been pulled at least $s$ times.
We indicate the start time of phase $s$ by $t_s$. An arm $i$ is \emph{payment-eligible} at time $t$ (in phase $s$)
if both of the following hold:

\begin{itemize}
\item $i$ has been pulled at most%
\footnote{in fact: exactly, since the algorithm entered phase $s$}
$s$ times up to time $t$, i.e., $\NumPull{t}{s} \leq s$.
\item Based on the current estimates \ArmEV{t}{i'} of all arms'
attribute vectors, the probability of pulling arm $i$ is less than
$1/\log(s)$. Let $\PullProb{t}{i} = \Prob[\AgV \sim \AgentDist]{\AgV \cdot \ArmEV{t}{i} > \AgV
  \cdot \ArmEV{t}{i'} \mbox{ for all } i' \neq i}$
be the probability that arm $i$ will be pulled
by the next (random) agent based on the current estimates. 
Then, the second criterion for payment-eligibility is that
$\PullProb{t}{i} < 1/\log(s)$.
\end{itemize}

When multiple arms are payment-eligible, the algorithm picks one arbitarily to incentivize.
When the algorithm decides to incentivize an arm $i$,
it offers ``whatever it takes,'' i.e., offers a payment of
$\Pay{t}{i} = \max_{\AgV,i'} \AgV \cdot (\ArmEV{t}{i'} - \ArmEV{t}{i})$.
The maximum for \AgV is taken over the support of \AgentDist;
recall that we assumed this support to be compact.
The payment \Pay{t}{i} may appear unnecessarily high.
Indeed, it suffices to
incentivize only a $1/\log(s)$ fraction of the agents,
and our bounds also hold for an alternate version of our algorithm that 
offers payment
$\Pay{t}{i} = \min \Set{c \geq 0}{%
\Prob[\AgV \sim \AgentDist]{c + \AgV \cdot \ArmEV{t}{i} \geq \max_{i'\ne i} \AgV \cdot \ArmEV{t}{i'}} \ge 1/\log(s)}$.
However, we focus on the higher payments for simplicity of presentation.

Notice that \PullProb{t}{i} depends on the estimates for \emph{all}
arms; thus, by pulling some other arm $i'$, an arm $i$ may become
payment-eligible, or cease to be so.
Algorithm~\ref{alg:basic-incentivizing} gives the full details.


\begin{algorithm}
\caption{Algorithm: Incentivizing Exploration \label{alg:basic-incentivizing}}
\begin{algorithmic}
\STATE Set the current phase number $s = 1$.
\COMMENT{Each arm is pulled once initially ``for free''.}
\FOR{time steps $t = 1, 2, 3, \ldots$}
\IF{$\NumPull{t}{i} \geq s+1$ for all arms $i$}
\STATE Increment the phase $s = s + 1$.
\ENDIF
\IF{there is a payment-eligible arm $i$}
\STATE Let $i$ be an arbitary payment-eligible arm.
\STATE Offer payment
$\Pay{t}{i} = \max_{\AgV,i'} \AgV \cdot (\ArmEV{t}{i'} - \ArmEV{t}{i})$
for pulling arm $i$
(and payment 0 for all other arms).
\ELSE
\STATE Let agent $t$ play myopically, i.e., offer payments 0 for all arms.
\ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}

The high-level idea in the proofs of our main results, Theorems~\ref{rst:budget} and~\ref{rst:regret},
is the following.
Because the algorithm ensures that each arm is pulled
``frequently enough,''
the estimates \ArmEV{t}{i} become gradually more accurate in the
phase number $s$.
Thus, the fraction of agents who misidentify their best arm decreases.
Because for each arm, enough agents have a true preference for this
arm, once the arms' attribute vectors are learned well enough,
the algorithm will not need to incentivize any more,
resulting in a payment bound independent of $T$.
Similarly, the regret will decrease, and mostly accrue
due to ``problematic'' agents who are nearly tied in their preferences
between their top two arm choices.
The detailed analysis following this outline is somewhat subtle, though,
due to the dependencies between the agents' arm pulls and the
estimates which in turn are based on past arm pulls.
We begin with several technical lemmas that are used for both the
payment and regret bounds.

To capture the probability masses of problematic agents,
we define the quantity
$\AlmostTied{\delta} = \Prob[\AgV \sim \AgentDist]{%
\AgV \cdot (\ArmV{\Best{\AgV}} - \ArmV{\Second{\AgV}}) \leq \delta}$
to be the total probability mass (across all arms) of agents
whose (actual) preference for their best arm over their second-best
arm is at most $\delta$.
\dkcomment{We should stress somewhere that we assume that actual ties
  have measure 0.}

To formally reason about the event that the estimates of arms'
attributes vectors are accurate enough --- or fail to be so ---
we define the events
$\AccE{t}{i}{j}{x} := [|\ArmE{t}{i}{j} - \Arm{i}{j}| \leq x]$
that attribute $j$ of arm $i$ at time $t$ is estimated to
within accuracy $x$ or better.
Then, 
$\AccEU{t}{x} = \bigcap_{i,j} \AccE{t}{i}{j}{x}$
is the event that at time $t$, all arm attribute
estimates are accurate to within $x$ simultaneously.
We will then show that for suitable choices of $t, x$,
the events \AccE{t}{i}{j}{x}
(and hence, by union bound, \AccEU{t}{x})
have high probability,
and that when they do, myopic agents do not make large mistakes.

%We now prove our main results, Theorems~\ref{rst:budget} and~\ref{rst:regret}.

\subsection{General Lemmas}

% We begin by showing that under our assumptions, the measure of
% problematic arms cannot be too large.

% \begin{lemma} \label{lem:sdelta}
% $\AlmostTied{\delta} \leq \TieDensity \cdot \delta$ for all $\delta$.
% \end{lemma}

% \begin{emptyproof}
% Using the upper bound from Assumption~\ref{A1}, we can bound

% \begin{align*}
% \AlmostTied{\delta}
% & = \sum_{i,i'} \ProbC{\AgV \cdot (\ArmV{i} - \ArmV{i'}) \leq \delta}%
%     {\AgV \in \FirstTwo{i}{i'}}
%   \cdot \Prob{\AgV \in \FirstTwo{i}{i'}}\\
% & \leq \sum_{i,i'} \TieDensity \cdot \delta
%     \cdot \Prob{\AgV \in \FirstTwo{i}{i'}}
% \; = \; \TieDensity \cdot \delta. \QED
% \end{align*}
% \end{emptyproof}

We begin by bounding the length of any phase, which will be useful to bound
the cumulative regret of the early rounds
(before tail bounds have kicked in). We prove Lemma~\ref{lem:phase-length} in the Appendix~\ref{sec:lemma4-proof}.


\begin{lemma} \label{lem:phase-length}
For any $s\geq 3$, the expected length of phase $s$ is at most
$\ARMNUM \cdot \log(s)$ time steps.
\end{lemma}
                  
We now state the key technical lemma captures the intuition that
the estimates of the arms' attribute vectors become
more accurate with increasing phases $s$.

\begin{lemma} \label{lem:round-prob}
Recall that $\sigma$ is the standard deviation of the Gaussian noise.
Let \LatePhase be a phase cutoff, 
and let $x_n, x'_n > 0$ be functions satisfying that
$\sqrt{0.6 n \cdot \log (\log_{1.1}(n) + 1) + \frac{n x_n^2}{16 \sigma^2}}
\leq \frac{n x'_n}{2 \sigma}$,
for all $n \geq \LatePhase$.
Let $\tau_s$ be a stopping time
(which may depend on the entire past history)
which is almost surely in phase $s$,
i.e., satisfying $\tau \in [t_s, t_{s+1})$ almost surely.

Then, for any arm $i$, attribute $j$, and phase $s \geq \LatePhase$,
we have that
$\Prob{\AccE{\tau_s}{i}{j}{x'_s}}
\geq 1 - 24 \exp\left(-\frac{1.8 s x_s^2}{16 \sigma^2} \right)$.
\end{lemma}

The proof of Lemma~\ref{lem:round-prob} is based on an adaptive
concentration inequality due to \cite{zhao2016adaptive},
given as Lemma~\ref{lem:ACI-inequality}.

\begin{lemma}[Corollary 1 of \cite{zhao2016adaptive}]
\label{lem:ACI-inequality}
Let $X_i$ be zero-mean $1/2$-subgaussian random variables,
and $\SET{S_n = \sum_{i=1}^n X_i, n \geq 1}$ the corresponding random walk.
Let $J$ be any stopping time with respect to $\SET{X_1, X_2, \ldots}$.
(We allow $J$ to take the value $\infty$,
defining $\Prob{J = \infty} = 1 - \lim_{n \to \infty} \Prob{J \leq n}$.)
Define 
\begin{align*}
g(n) & = \sqrt{0.6 n \cdot \log (\log_{1.1}(n) + 1) + n \cdot b}.
\end{align*}
Then, 
\begin{align*}
\Prob{J < \infty \mbox{ and } S_J \geq g(J)} \leq 12 \e^{-1.8 b}.
\end{align*}
\end{lemma}

We leave the proof of Lemma~\ref{lem:round-prob} in the Appendix~\ref{sec:lemma5-proof}. Next, we show the complementary part:
when \AccEU{t}{x} happens, no myopic agent incurs large regret. We prove Lemma~\ref{lem:right-choice} in the Appendix~\ref{sec:lemma7-proof}.

\begin{lemma} \label{lem:right-choice}
Let $x > 0$ be arbitrary.
When \AccEU{t}{x} happens,
no agent \AgV will pull a highly suboptimal arm, i.e., an arm $i$ with 
$\AgV \cdot (\ArmV{\Best{\AgV}} - \ArmV{i}) > 2\Diam d x$.
\end{lemma}

\subsection{Bounding the Total Payment}

As a first step towards bound the total payment (and also regret),
we show that for sufficiently late phases,
under the event \AccEU{t}{x} for suitably small $x$,
the algorithm does not offer any payments.

\begin{lemma} \label{lem:no-incentives}
Fix an arm $i$.
Let $s \geq \exp(2/\MinProb)$, and let $\tau_s$ be the (random)
time when arm $i$ is pulled for the \Kth{s} time.
Under \AccEU{\tau_s}{\frac{p}{4\Diam d \TieDensity}},
this pull of arm $i$ is not incentivized.
\end{lemma}

\begin{proof}
By Lemma~\ref{lem:right-choice},
under the event \AccEU{\tau_s}{\frac{\MinProb}{4\Diam d \TieDensity}},
all agent types \AgV with
$\AgV \cdot (\ArmV{\Best{\AgV}} - \ArmV{\Second{\AgV}})
> \frac{\MinProb}{2\TieDensity}$
will pull their best arm \Best{\AgV}.

By Assmption~\ref{A1},
%Lemma~\ref{lem:sdelta},
the measure of agents (across all arms)
whose best and second-best arm differ in utility by less
than $\frac{\MinProb}{2\TieDensity}$ is at most $\frac{\MinProb}{2}$.
In particular, this bound holds for agents whose best arm is $i$.
By Assumption~\ref{A3}, at least a measure \MinProb of agents has $i$
as their best arm, and thus, at least a measure $\frac{\MinProb}{2}$
will myopically pull arm $i$.
Because $1/\log(s) < \frac{\MinProb}{2}$ for
$s \geq \exp(2/\MinProb)$, arm $i$ is not payment-eligible at time
$\tau_s$.
\end{proof}

Towards bounding the algorithm's total payment, we now bound the
\emph{number} of rounds in which the algorithm makes a payment by a
constant.
This bound also turns out to be useful for bounding the total regret.

\begin{lemma} \label{lem:numP}
The expected number of time steps in which
Algorithm~\ref{alg:basic-incentivizing}
makes any payment is at most $O\left( N\exp\left(\frac{2}{p}\right) \right)$.
\end{lemma}

\begin{proof}
We partition phases into early and late phases.
For each of the early phases,
we crudely bound the number of payments by \ARMNUM,
using that each arm is incentivized at most once per phase.
For later phases,
we use Lemma~\ref{lem:no-incentives},
which rules out any incentives unless large misestimates of the arm
locations occur, which is exponentially unlikely by 
Lemma~\ref{lem:round-prob}.

To make this intuition precise, we set
$\delta = \frac{\MinProb}{2 \TieDensity}$,
and $x = x' = \frac{\delta}{2 \Diam d}$
(which is independent of the phase number $s$).
The cutoff point between early and late phases is now set to
$\EvenLaterPhase = \max(2, \frac{30 \sigma^3}{x^3}, \exp(\frac{2}{\MinProb}))$.
We will verify below that
$\sqrt{0.6 n \cdot \log (\log_{1.1}(n) + 1) + \frac{n x^2}{16 \sigma^2}}
\leq \frac{n x}{2 \sigma}$
for all $n \geq \max(2, \frac{30 \sigma^3}{x^3})$.

Fix an arm $i$, and consider the \Kth{s} pull of an arm $i$,
for $s \geq \EvenLaterPhase$.
If this pull occurs before phase $s$, it was definitely not incentivized.
So we condition on the pull occurring in phase $s$,
and let $\tau_s$ be the time step when this pull occurred;
notice that this means that this must be the first pull of arm $i$ in
phase $s$.

By Lemma~\ref{lem:round-prob} (with our choice of $x = x'$),
and a union bound over all $i,j$, we bound the probability
$\Prob{\AccEU{\tau_s}{x}}
\geq 1 - 24 \ARMNUM d \cdot \exp\left(-\frac{1.8 s x^2}{16 \sigma^2} \right)$.
And by Lemma~\ref{lem:no-incentives},
under the event ${\mathcal E}_{\tau_s}(x)$,
arm $i$ is not payment-eligible.

Thus, for any $s \geq \EvenLaterPhase$,
the \Kth{s} pull of arm $i$ is incentivized  
with probability at most
$24 \ARMNUM d \cdot \exp \left(
  -\frac{1.8 s \cdot \MinProb^2}{256 \TieDensity^2 \Diam^2 d^2 \sigma^2}
\right)$.
Summing over all arms and phases $s$,
and adding the at most $\ARMNUM \EvenLaterPhase$ incentivizations in
the first \EvenLaterPhase phases, 
the expected total number of arm incentivizations is at most
\begin{align*}
&\ARMNUM \EvenLaterPhase
  + 24 \ARMNUM^2 d \cdot \frac{1}{1 - \exp \left(
    \frac{-1.8 s \cdot \MinProb^2}{256 \TieDensity^2 \Diam^2 d^2 \sigma^2}
  \right)} \nonumber \\
=& O\left( \max\left(\frac{\ARMNUM \TieDensity^3 \Diam^3 d^3 \sigma^3}{\MinProb^3}, N\exp(\frac{2}{p})\right)
  + \frac{\ARMNUM^2 d}{1 - \exp \left(
    \frac{-1.8 \cdot \MinProb^2}{256 \TieDensity^2 \Diam^2 d^2 \sigma^2}
  \right)} \right) \nonumber  \\
=& O\left( N\exp\left(\frac{2}{p}\right) \right).
\end{align*}

It remains to show that
$\sqrt{0.6 n \cdot \log (\log_{1.1}(n) + 1) + \frac{n x^2}{16 \sigma^2}}
\leq \frac{n x}{2 \sigma}$
for all $n \geq \max(2, \frac{30 \sigma^3}{x^3}$.
We first use that $\sqrt{\cdot}$ is sublinear to bound
\[
  \sqrt{0.6 n \cdot \log (\log_{1.1}(n) + 1) + \frac{n x^2}{16 \sigma^2}}
\; \leq \; \sqrt{0.6 n \cdot \log (\log_{1.1}(n) + 1)} + \frac{n x}{4 \sigma}
\]
(here we use the fact $\sqrt{n} \leq n$).
Next, we show that
$\sqrt{0.6 n \log (\log_{1.1}(n) + 1)} \leq \frac{n x}{4 \sigma}$;
then, adding the two terms gives the desired bound.

By squaring the claimed statement and rearranging,
it is equivalent to show that
$\frac{n}{\log(\log_{1.1}(n)+1)} \geq \frac{9.6\sigma^2}{x^2}$.
Because $n \geq 2$,
a numerical calculation and derivative test shows that
$\frac{n}{\log(\log_{1.1}(n)+1)} \geq n^{2/3}$,
and because $n \geq \frac{30 \sigma^3}{x^3}$,
we get that
$n^{2/3} \geq \frac{9.6 \sigma^2}{x^2}$, completing the proof.

\end{proof}

It would be desirable to simply identify a constant upper bound on the
payment made each time.
Unfortunately, while the agent types are drawn from a bounded support,
the noise in arm locations is not;
hence, with small probability, arm locations may be grossly
misestimated, resulting in high incentive payments.
As a result, the actual analysis of the total payment is significantly more
intricate;
the proof of Theorem~\ref{rst:budget} is given in
Appendix~\ref{sec:payment-proof}.


\subsection{Bounding the Total Regret}
In bounding the total regret, because agents' types are from a compact
set by Assumption~\ref{A2}, the maximum regret in any one round is
bounded by a constant.
We use $\MAXR = \max_{\AgV, i,i'} \AgV \cdot (\ArmV{i} - \ArmV{i'})$
to denote this constant upper bound on the maximum regret that can be
incurred in any one time step. 


\begin{extraproof}{Theorem~\ref{rst:regret}}
Regret can arise in two ways:
(1) an agent was incentivized to pull a suboptimal arm, or
(2) an agent myopically pulled a suboptimal arm.
By Lemma~\ref{lem:numP}, Algorithm~\ref{alg:basic-incentivizing}
incentivizes agents at most 
$O\left( N\exp\left(\frac{2}{p}\right) \right)$
times in expectation, each time causing regret at most \MAXR,
for a total expected regret of at most
$O\left(\MAXR\cdot N\exp\left(\frac{2}{p}\right) \right)$.
For the rest of the proof, we focus on the regret incurred when agents
pull arms myopically and make mistakes.

We distinguish between agents incurring large regret
(which requires severe misestimates of arm locations;
such misestimated are exponentially unlikely to occur), 
and agents incurring small positive regret,
which requires these agents to be almost tied in their preference for
the best arm.
To be more precise, we will define (with foresight) a phase-dependent
cutoff
$g(s) = \sqrt{\frac{128\log(T) \cdot \Diam^2 d^2 \sigma^2}{1.8 s}}$,
and consider a regret exceeding $g(s)$ large.

We first consider agents \AgV incurring positive regret less than $g(s)$.
The second-best arm \Second{\AgV} of such an agent \AgV must satisfy
$\AgV \cdot (\ArmV{\Best{\AgV}} - \ArmV{\Second{\AgV}}) \leq g(s)$.
By Assumption~\ref{A1},
%Lemma~\ref{lem:sdelta},
the total measure of such agents
is $\AlmostTied{g(s)} \leq \TieDensity \cdot g(s)$,
and each incurs regret at most $g(s)$.
Summing over all time steps,
and using Lemma~\ref{lem:phase-length} to bound the number of time
steps in phase $s$, 
the expected total regret from such agents is at most
\begin{align}
\Expect{\sum_{s=1}^{T} \sum_{t=t_{s-1}+1}^{t_{s}} \TieDensity \cdot g^2(s)}
& \leq \sum_{s=1}^{T} \TieDensity \cdot g^2(s) \ARMNUM \log(s).
\label{equ:small_regret_bound}
\end{align}

We next bound the regret incurred in time steps with large regret.
Let the random variable $t_s$ be the final time step in phase $s$,
for each $s$. Define the stopping times $\tau_{s}^{k}$ to be the \Kth{k}
time step in the \Kth{s} phase,
with $\tau_{s}^{k} = \infty$ when phase $s$ has fewer than $k$ steps,
i.e., $k > t_{s}-t_{s-1}$.
By Lemma~\ref{lem:right-choice},
under \AccEU{\tau_s^k}{\frac{g(s)}{2\Diam d}}, no agent at time $t$ will incur
regret more than $g(s)$.

To bound the probability of \AccEU{\tau_s^k}{\frac{g(s)}{2\Diam d}},
we use Lemma~\ref{lem:round-prob}
(with $x_s = x'_s = \frac{g(s)}{2 \Diam d}$
and a stopping time of $\tau_s^k$).
We first verify that this choice of $x_s, x'_s$
satisfies the assumption of Lemma~\ref{lem:round-prob} for all
$s \geq 2$,
i.e., that
$\sqrt{0.6 n \cdot \log (\log_{1.1}(n) + 1) + \frac{n x_n^2}{16 \sigma^2}}
\leq \frac{n x_n}{2 \sigma}$
for all $n \geq 2$.
Substituting that
$x_n = \frac{g(n)}{2 \Diam d} = \sqrt{\frac{32\log(T) \cdot \sigma^2}{1.8 n}}$,
the left-hand side is

\[
  \sqrt{0.6 n \cdot \log (\log_{1.1}(n) + 1) + \frac{2 \log T}{1.8}}
  \; \leq \;
  \sqrt{0.6 n \cdot \log (\log_{1.1}(n) + 1) + \frac{2 n \log T}{1.8}},
\]
while the right-hand side is $\sqrt{\frac{8 n \log(T)}{1.8}}$.
Squaring both sides and canceling out common terms,
the desired inequality is equivalent to
$\frac{50}{9} \log T \geq \log(\log_{1.1}(n) + 1)$.
Because $n \leq T$, it is sufficient to show that
$\frac{50}{9} \log T \geq \log(\log_{1.1}(n) + 1)$,
which can be verified by numerical calculation for $T=2$ and a
derivative test.

Now, by Lemma~\ref{lem:round-prob} and a union bound over all arms $i$
and attributes $j$, we get that 
\begin{align*}
\Prob{\AccEU{\tau_s^k}{x_s}}
& \leq 24 \ARMNUM d \cdot \exp \left( \frac{-1.8 s x^2}{16 \sigma^2} \right)
\; = \; \frac{24 \ARMNUM d}{T^2}.
\end{align*}

In the low-probability event, we simply 
bound the maximum regret by \MAXR.
Now, applying Lemma~\ref{lem:phase-length},
the total expected regret from large-regret steps is at most

\begin{align*}
\sum_{s = 1}^T \sum_{t=t_s}^{t_{s+1}-1}
  \Prob{\mbox{incur regret exceeding } g(s) \mbox{ at step } t} \cdot \MAXR
& \leq
 \sum_{s = 1}^T \frac{24 \ARMNUM d}{T^2} \cdot \MAXR \cdot \ARMNUM \log(s)
\\ & \leq 12 \ARMNUM^2 d \MAXR.
\end{align*}

Adding all three types of regret terms,
and substituting that 
$g^2(s) = \frac{128\log(T) \cdot \Diam^2 d^2 \sigma^2}{1.8 s}$,
the cumulative regret up to time $T$ is at most

\begin{align*}
\lefteqn{
O \left(\ARMNUM \MAXR \cdot \exp \left( \frac{2}{p} \right)
+ \ARMNUM^2 \MAXR d
+ \ARMNUM \TieDensity \cdot \sum_{s=1}^{T} g^2(s) \log(s) \right)}
\\ & = 
O \left(\ARMNUM \MAXR \cdot \exp \left( \frac{2}{p} \right)
+ \ARMNUM^2 \MAXR d
+ \ARMNUM \TieDensity \Diam^2 d^2 \sigma^2 \cdot \log^3(T) \right).
\end{align*}
\end{extraproof}
