\documentclass[serif]{beamer}

%set the theme to Cornell and set options.
%navbar=true shows the navigation bar in the footline. navbar=false hides it
%colorblocks=true makes the block (and theorem) environment appear as a colored box. colorblocks=false makes the block (and theorem) environment very plain.
\mode<presentation>
{
\usetheme
[navbar=true,colorblocks=true,pagenumbers=true]{Cornell}
}

%these packages are essential for compiling
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{mathrsfs}
\usepackage{biblatex}
\usepackage{enumitem}


\usepackage[]{algorithm}
\usepackage[noend]{algorithmic}

%short title appears in headline, long title appears on title page, subtitle appears on title page
\title[Incentivizing Exploration by Heterogeneous Users]{Incentivizing Exploration by Heterogeneous Users}
\subtitle{COLT 2018}

%the author only appears in the headline of slides
\author[]{Chen, Frazier \& Kempe}

%the institute contains all information, including author name. only appears on title page
\institute
{
\begin{tabular}[h]{c}
\normalsize Bangrui Chen, Peter Frazier  \\
~\\
Cornell University            \\
Operations Research and Information Engineering        \\
{\tt bc496@cornell.com, pf98@cornell.edu}    \\
~\\
\normalsize David Kempe \\
~\\
University of Southern California           \\
Department of Computer Science       \\
{\tt david.m.kempe@gmail.com}   
\end{tabular}
}

\date[]{July 8, 2018}

%uncomment the following lines to change major colors in the theme. they are currently set to their defaults.

%\setbeamercolor*{structure}{fg=cblue} %misc. elements, like toc pages and itemize
%\setbeamercolor{palette secondary}{fg=cred} %footline
%\setbeamercolor{palette tertiary}{fg=white,bg=cgray} %headline
%\setbeamercolor{palette quaternary}{fg=cred} %title
%\setbeamercolor{high stripe}{bg=cred} %stripe in title
%\definecolor{block color}{named}{cblue} %normal block colors


\begin{document}

%#################################################
\begin{frame}[plain]
\titlepage
\end{frame}
%#################################################
%#################################################
\begin{frame}{Motivation}
\includegraphics[scale=0.4]{example}
\end{frame}
%#################################################

%#################################################
\begin{frame}{Previous Work}
\textbf{Without Money Transfer:}
\small{
\begin{itemize}[label=\textbullet]
\item Implementing the “Wisdom of the Crowd”, Kremer et al. 2014;
\item Bayesian incentive-compatible bandit exploration, Mansour et al. 2015;
\item $\cdots$
\end{itemize}
}
\vspace{1cm}
\textbf{With Money Transfer}
\small{
\begin{itemize}[label=\textbullet]
\item Incentivizing exploration, Frazier et al. 2014;
\item Incentivizing exploration with heterogeneous value of money, Han et al. 2015;
\item $\cdots$
\end{itemize}
}
\end{frame}
%#################################################

\begin{frame}{Our Contribution}
\begin{itemize} [label=\textbullet]
\item First algorithm and analysis for incentivizing exploration when users have heterogeneous preferences over arms;
\item We proved \textbf{“heterogeneity provides free exploration”}.
\end{itemize}
\end{frame}

%#################################################
\begin{frame}{Problem Setting}

\textbf{$N$ arms}
\begin{itemize}[label=\textbullet]
\item Each arm is associated with a feature vector $u_i\in R^{d}$;
\item Both the agents and the principal can observe the current estimate of $u_i$, denoted as $\hat{u}_{i,t}$, which equals to the average of all past observations;
\end{itemize}
\vspace{1cm}
\textbf{Myopic Agents}
\begin{itemize}[label=\textbullet]
\item Agents arrive sequentially and their preference $\theta_t\in R^{d}$ follows known distribution $F(\cdot)$.
\item Without any incentives, agent $\theta_t$ would choose the arm $i_t = \arg\max_{i}\{\theta_{t}\cdot \hat{u}_{i,t}\}$.
\end{itemize}

\end{frame}
%#################################################

%#################################################
\begin{frame}{Problem Setting}

\textbf{Agents behavior and feedback}
\begin{itemize}[label=\textbullet]
\item Principal chooses payment $c_{t,i}$ for arm $i$ at time $t$;
\item Agent $\theta_t$ would choose the arm $i_t = \arg\max_{i}\{\theta_{t}\cdot \hat{u}_{i,t}+c_{t,i}\}$;
\item Each pull provides vector-valued outcomes equal to $u_{i}$, perturbed by independent noise.
\end{itemize}
\vspace{1cm}
\textbf{Principal's goal}
\begin{itemize}[label=\textbullet]
\item Regret $r_t = (\max_{i} \theta_t \cdot u_i) - \theta_t \cdot u_{i_t}$ and payment $c_t = c_{t,i_t}$;
\item Incentivize to minimize the cumulative regret while making a small cumulative payment;
\end{itemize}

\end{frame}
%#################################################

%#################################################
\begin{frame}{Key Assumptions}
\begin{itemize}[label=\textbullet]
\item (\textbf{Every arm is someone's best}) We use $p$ to denote the minimum (over all arms) fraction of users that prefer any particular arm.
\vspace{0.2cm}
\item (\textbf{Not too many near-ties}) Let $q(z)$ be the cumulative distribution function of those agents whose utility difference between their best and second best arm is less than or equal to $z$, then there exists a $\hat{z}>0$, $L$ such that $q(z)\leq L\cdot z$ for all $z\leq \hat{z}$.
\vspace{0.2cm}
\item (\textbf{Compact Support}) $\theta$ has a compact support set contained in $[0,D]^{d}$.\
\end{itemize}

\end{frame}
%#################################################

%#################################################
\begin{frame}{Main Result}
\begin{block}{Theorem 1}

With the previously stated assumptions, there is a policy that achieves expected 
cumulative regret $O (N e^{2/p} + L N \log^3(T))$,
using expected cumulative payments of $O(N^2 e^{2/p})$.
\newline
\newline
In particular, when agents who are close to tied between two arms have measure $0$,
both the expected regret and expected payment are bounded by constants
(with respect to $T$). 

\end{block}

\end{frame}
%#################################################
%#################################################
\begin{frame}{Notations}

\textbf{Phase}
\begin{itemize}[label=\textbullet]
\item Phase $s$ starts when each arm has been pulled at least $s$ times.
\end{itemize}

\vspace{0.7cm}
\textbf{Payment-eligible}
\begin{itemize}[label=\textbullet]
\item Arm $i$ has been pulled at most $s$ times up to time $t$;
\item The conditional probability of pulling arm $i$ is less than $\frac{1}{\log(s)}$ given the current estimates $\hat{u}_{i,t}$.
\end{itemize}

\end{frame}

%#################################################
\begin{frame}{Algorithm}

\begin{algorithmic}
\STATE Set the current phase number $s = 1$.
\COMMENT{Each arm is pulled once initially ``for free.''}
\FOR{time steps $t = 1, 2, 3, \ldots$} {
\IF {$m_{t,i} \geq s+1$ for all arms $i$}
\STATE Increment the phase $s = s + 1$.
\ENDIF
\IF {there is a payment-eligible arm $i$} 
    \STATE Let $i$ be an arbitrary payment-eligible arm.
    \STATE Offer payment $c_{t,i} = \max_{\theta,i'} \theta \cdot (\hat{\mu}_{t,i'} - \hat{\mu}_{t,i})$ for pulling arm $i$ (and payment 0 for all other arms).
\ELSE
    \STATE Let agent $t$ play myopically, i.e., offer payments 0 for all arms.
\ENDIF 
}\ENDFOR
\end{algorithmic}

\end{frame}
%#################################################
%#################################################
\begin{frame}{Payment Analysis}
\textbf{Key technical lemma}: an adaptive concentration inequality (Zhao et al. 2016);
\vspace{1cm}

\textbf{Early Phases}: bound by $N$;
\vspace{1cm}

\textbf{Later Phases}: exponentially unlikely as the phases advances;
\end{frame}
%#################################################

%#################################################
\begin{frame}{Regret Analysis}

\textbf{When principal incentivizes:} similar to the payment proof;
\vspace{0.5cm}

\textbf{When agents pull myopically:} We define a phase-dependent cutoff $\gamma(s(t))$ to further distinguish agents based on the regret.
\begin{itemize}[label=\textbullet]
\item $r(t)\geq \gamma(s(t))$:
\begin{itemize}[label=$\star$]
\item this happens with exponentially decreasing probability;
\item since $\theta_t$ has a compact support, the maximum regret is bounded by a constant;
\end{itemize}
\item $r(t)\leq \gamma(s(t))$:
\begin{itemize}[label=$\star$] 
\item not so many agents have near-ties preferences; 
\item the maximum regret is bounded above by $\gamma(s(t))$;
\end{itemize}
\end{itemize}

\end{frame}
%#################################################
\begin{frame}{Question?}
\begin{center}
\Huge{Thanks for your time!}
\end{center}
\end{frame}
%#################################################




\end{document}

