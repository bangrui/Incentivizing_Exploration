\subsection{Tighter Bounds Under Additional Assumptions}
%: Discrete Preferences and Known \MinProb}

The proofs of Theorem~\ref{rst:budget} and Theorem~\ref{rst:regret} incur exponential (in \ARMNUM) payment and regret in the initial phases because the threshold required for incentivization $1/\log(s)$ decreases slowly.
This slow decrease is needed to bound the regret in the later phases when the concentration inequality kicks in as in Equation~(\ref{equ:small_regret_bound}).
In this section, we discuss two restrictions on the problem setting under which  we can modify the algorithm slightly and provide stronger bounds, avoiding this exponential dependence.
Both problem settings are special cases of the more general problem setting previously considered.

\subsubsection{Discrete Preferences}
Discrete preferences by agents are captured by the following assumption (which states that the agents who are close to tied between two arms have measure 0):

\begin{assumption}[Discrete Preferences]
\label{a:discrete}
There is a positive $\delta_0$ such that
$\AlmostTied{\delta_0} = 0$.
\end{assumption}

When Assumption~\ref{a:discrete} holds, we restrict the payment-eligibility criterion by only incentivizing arms with much smaller probability to be pulled: an arm $i$ is \emph{payment-eligible} at time $t$ in phase $s$ when both of the following are true:
\begin{itemize}
\item $i$ has been pulled at most
$s$ times up to time $t$, i.e., $\NumPull{t}{s} \leq s$.
\item Based on the current estimates \ArmEV{t}{i'} of all arms' attribute vectors, the probability of pulling arm $i$ is less than $1/s$ (compared to $1/\log(s)$ in the general algorithm).
\end{itemize}

We refer to this modified version of Algorithm~\ref{alg:basic-incentivizing} as the \emph{Discrete-Preference Algorithm.}
We outline a proof of the following result for this algortithm under Assumption~\ref{a:discrete} in Appendix~\ref{sec:discussion-proof1}.

\begin{theorem}
\label{rst:discrete}
Under Assumption~\ref{a:discrete}, the Discrete-Preference Algorithm has expected payment budget bounded above by 
\begin{align*}
O \left(\frac{\ARMNUM^2 d \cdot (\MAXR + \Diam d \sigma)\sigma^{5/2}L^{5/2}\Diam^{5/2}d^{5/2}}{\MinProb^{5/2}} \right),
\end{align*}
and expected regret bounded above by 
\begin{align}
O \left(\MAXR\cdot \frac{\ARMNUM \TieDensity^4 \Diam^4 d^4 \sigma^4}{\MinProb^4}
  + \frac{\MAXR \ARMNUM^2 d}{1 - \exp \left(
    \frac{-1.8 \cdot \MinProb^2}{256 \TieDensity^2 \Diam^2 d^2 \sigma^2}
  \right)} \right).  \nonumber
\end{align}
\end{theorem}



\subsubsection{Known \MinProb}
An alternative useful assumption is that \MinProb (or a strictly positive lower bound on \MinProb) is known.

\begin{assumption}[Known $\MinProb$]
\label{a:known-p}
A strictly positive lower bound on \MinProb is known in advance.
\end{assumption}

When this assumption holds, we choose a different modification in the definition of payment eligibility.
Let $s_0 = \exp(2/\MinProb)$.
An arm $i$ is \emph{payment-eligible} at time $t$ (in phase $s$)
if both of the following hold:
\begin{itemize}
\item $i$ has been pulled at most
$s$ times up to time $t$, i.e., $\NumPull{t}{s} \leq s$.
\item Based on the current estimates \ArmEV{t}{i'} of all arms' attribute vectors, the probability of pulling arm $i$ is less than $1/\log(s+s_0)$.
\end{itemize}

This, knowledge of \MinProb allows the algorithm to incentivize significantly fewer arm pulls.
We refer to this modified version of Algorithm~\ref{alg:basic-incentivizing} as the \emph{Known-\MinProb Algorithm.}
We outline a proof of the following result for this algortithm under Assumption~\ref{a:known-p} in the Appendix~\ref{sec:discussion-proof2}.

\begin{theorem}
\label{rst:known-p}
Under Assumption~\ref{a:known-p}, the Known-\MinProb Algorithm has an expected payment budget bounded above by 
\begin{align*}
O \left(\frac{\ARMNUM^2 d \cdot (\MAXR + \Diam d \sigma)\sigma^{5/2}L^{5/2}\Diam^{5/2}d^{5/2}}{\MinProb^{5/2}} \right),
\end{align*}
and an expected regret bounded above by
\begin{align*}
O \left(\frac{\MAXR \ARMNUM \TieDensity^3 \Diam^3 d^3 \sigma^3}{\MinProb^3}
  + \frac{\MAXR \ARMNUM^2 d}{1 - \exp \left(
    \frac{-1.8 \cdot \MinProb^2}{256 \TieDensity^2 \Diam^2 d^2 \sigma^2}
  \right)} +\frac{\ARMNUM^2 d\MAXR}{\MinProb}+\frac{\ARMNUM\Diam^2 d^2\sigma^2 \TieDensity(\log(T))^3}{\MinProb}\right).
\end{align*}
\end{theorem}

