\subsection{Special Case}

As we see in the proof of Theorem~\ref{rst:budget} and Theorem~\ref{rst:regret}, we occurred an exponential payment/regret at the initial phases simplily because we decrease the probability threshold too slowly, which is needed when bounding the regret after the concentration probability kicks in as in Equation~(\ref{equ:small_regret_bound}). In this section, we are going to discuss two special cases where we can get rid of the exponential dependency on the number of arms in the payment and regret bound. 

\textbf{Case 1: If $\exists \delta_0>0$ such that $F_{i,j}(\delta)=0$ for all $i,j$.}

In this case, we can simplily modify the payment-eligible criteria as follows: An arm $i$ is \emph{payment-eligible} at time $t$ (in phase $s$)
if both of the following hold:

\begin{itemize}
\item $i$ has been pulled at most%
\footnote{in fact: exactly, since the algorithm entered phase $s$}
$s$ times up to time $t$, i.e., $\NumPull{t}{s} \leq s$.
\item Based on the current estimates \ArmEV{t}{i'} of all arms'
attribute vectors, the probability of pulling arm $i$ is less than $1/s$. Let $\PullProb{t}{i} = \Prob[\AgV \sim \AgentDist]{\AgV \cdot \ArmEV{t}{i} > \AgV
  \cdot \ArmEV{t}{i'} \mbox{ for all } i' \neq i}$
be the probability that arm $i$ will be pulled
by the next (random) agent based on the current estimates. 
Then, the second criterion for payment-eligibility is that
$\PullProb{t}{i} < 1/s$.
\end{itemize}

The algorithm remains the same as Algorithm~\ref{alg:basic-incentivizing}. Under this new payment-eligible criteria, we are able to prove the following results:

\begin{theorem}
The payment budget for Algorithm~\ref{alg:basic-incentivizing} is bounded above by 
\begin{align*}
O \left(\frac{\ARMNUM^2 d \cdot (\MAXR + \Diam d \sigma)\sigma^{5/2}L^{5/2}\Diam^{5/2}d^{5/2}}{p^{5/2}} \right). 
\end{align*}
\end{theorem}

\begin{proof}
The proof is exact the same as the proof for Theorem~\ref{rst:budget} except we now define $h(\ell) := \max \left( \frac{2}{\MinProb},
\left( \frac{48 \sigma \ell \TieDensity \Diam d}{\MinProb} \right)^{5/2}
\right)$. The rest of the proof remains the same.
\end{proof}


\begin{lemma}  \label{lem:NumP1}
The expected number of time steps in which
Algorithm~\ref{alg:basic-incentivizing}
makes any payment is at most 
\begin{align}
O\left(\frac{\ARMNUM \TieDensity^3 \Diam^3 d^3 \sigma^3}{\MinProb^3}
  + \frac{\ARMNUM^2 d}{1 - \exp \left(
    \frac{-1.8 s \cdot \MinProb^2}{256 \TieDensity^2 \Diam^2 d^2 \sigma^2}
  \right)} \right). \nonumber 
\end{align}
\end{lemma}

\begin{proof}
The proof is exact the same as the proof for Lemma~\ref{lem:numP} except we now define $\EvenLaterPhase = \max (2, \frac{30 \sigma^3}{x^3}, \frac{2}{p})$.
\end{proof}


\begin{theorem}
The payment budget for Algorithm~\ref{alg:basic-incentivizing} is bounded above by 
\begin{align}
O \left(\MAXR\cdot \frac{\ARMNUM \TieDensity^4 \Diam^4 d^4 \sigma^4}{\MinProb^4}
  + \frac{\MAXR \ARMNUM^2 d}{1 - \exp \left(
    \frac{-1.8 s \cdot \MinProb^2}{256 \TieDensity^2 \Diam^2 d^2 \sigma^2}
  \right)} \right).  \nonumber
\end{align}
\end{theorem}

\begin{proof}
The proof of this Theorem is similar as Theorem~\ref{rst:regret}, except
\begin{itemize}
\item We can use Lemma~\ref{lem:NumP1} to bound the regret when we decide to incentivize;
\item The expected length of phase $s$ is bounded by $\ARMNUM s$;
\item The regret in Equation~\ref{equ:small_regret_bound} can now be bounded by a constant based on our assumption.
\end{itemize}
The rest of the proof remains the same.
\end{proof}




\textbf{Case 2: If we know $\MinProb$ in advance.}

If we know $\MinProb$ in advance, we can modify the payment-eligible criteria as follows: Denote $s_0=\min_{s=1,2,\cdots,}\{\frac{1}{\log(s_0)}\leq \frac{\MinProb}{2}\}$. An arm $i$ is \emph{payment-eligible} at time $t$ (in phase $s$)
if both of the following hold:

\begin{itemize}
\item $i$ has been pulled at most%
\footnote{in fact: exactly, since the algorithm entered phase $s$}
$s$ times up to time $t$, i.e., $\NumPull{t}{s} \leq s$.
\item Based on the current estimates \ArmEV{t}{i'} of all arms'
attribute vectors, the probability of pulling arm $i$ is less than $1/\log(s+s_0)$. Let $\PullProb{t}{i} = \Prob[\AgV \sim \AgentDist]{\AgV \cdot \ArmEV{t}{i} > \AgV
  \cdot \ArmEV{t}{i'} \mbox{ for all } i' \neq i}$
be the probability that arm $i$ will be pulled
by the next (random) agent based on the current estimates. 
Then, the second criterion for payment-eligibility is that
$\PullProb{t}{i} < 1/\log(s+s_0)$.
\end{itemize}

The algorithm remains the same as Algorithm~\ref{alg:basic-incentivizing}. Under this new algorithm, we can prove:

\begin{theorem}
The payment budget for Algorithm~\ref{alg:basic-incentivizing} is bounded above by 
\begin{align*}
O \left(\frac{\ARMNUM^2 d \cdot (\MAXR + \Diam d \sigma)\sigma^{5/2}L^{5/2}\Diam^{5/2}d^{5/2}}{p^{5/2}} \right). 
\end{align*}
\end{theorem}

\begin{proof}
The proof is exact the same as the proof for Lemma~\ref{rst:budget} except we define $h(\ell) := 
\left( \frac{48 \sigma \ell \TieDensity \Diam d}{\MinProb} \right)^{5/2}$. The rest of the proof remains the same.
\end{proof}

\begin{lemma}  \label{lem:NumP2}
The expected number of time steps in which
Algorithm~\ref{alg:basic-incentivizing}
makes any payment is at most 
\begin{align}
O\left(\frac{\ARMNUM \TieDensity^3 \Diam^3 d^3 \sigma^3}{\MinProb^3}
  + \frac{\ARMNUM^2 d}{1 - \exp \left(
    \frac{-1.8 s \cdot \MinProb^2}{256 \TieDensity^2 \Diam^2 d^2 \sigma^2}
  \right)} \right). \nonumber 
\end{align}
\end{lemma}

\begin{proof}
The proof is exact the same as the proof for Lemma~\ref{lem:numP} except we now define $\EvenLaterPhase = \max(2, \frac{30 \sigma^3}{x^3})$ since $\frac{1}{\log(s+s_0)}\leq \frac{p}{2}$ is true for all $s$.
\end{proof}

\begin{theorem}
The payment budget for Algorithm~\ref{alg:basic-incentivizing} is bounded above by 
\begin{align}
O \left(\MAXR\cdot \frac{\ARMNUM \TieDensity^3 \Diam^3 d^3 \sigma^3}{\MinProb^3}
  + \frac{\MAXR \ARMNUM^2 d}{1 - \exp \left(
    \frac{-1.8 s \cdot \MinProb^2}{256 \TieDensity^2 \Diam^2 d^2 \sigma^2}
  \right)} +\frac{\ARMNUM^2 d\MAXR}{p}+\frac{\ARMNUM\Diam^2 d^2\sigma^2 \TieDensity(\log(T))^3}{p}\right).  \nonumber
\end{align}
\end{theorem}

\begin{proof}
The proof of this Theorem is similar as Theorem~\ref{rst:regret}, except
\begin{itemize}
\item We can use Lemma~\ref{lem:NumP2} to bound the regret when we decide to incentivize;
\item The expected length of phase $s$ is bounded by $\ARMNUM \log(s+s_0)$;
\item With the same choice of $g(s)$, the regret in Equation~\ref{equ:small_regret_bound} can now be bounded by $71.11 \ARMNUM\Diam^2 d^2\sigma^2 \TieDensity\log(T)(\log(T)+1)\log(T+s_0)=O(\frac{\ARMNUM\Diam^2 d^2\sigma^2 \TieDensity(\log(T))^3}{p})$.
\end{itemize}
The rest of the proof remains the same.
\end{proof}


\subsection{Decreasing the Payment} \label{sec:pi}
\dkcomment{Does this really need a whole subsection now?}

In Algorithm~\ref{alg:basic-incentivizing},
the payment \Pay{t}{i} offered for pulling payment-eligible arms $i$
is high enough to ensure that arm $i$ will be pulled with probability 1.
However, as can be seen in the proof of Lemma~\ref{lem:phase-length},
we then lower-bound this probability by $1/\log(s)$.
Thus, the bounds we prove do not deteriorate as long as the payment is
high enough such that arm $i$ is pulled with probability at least $1/\log(s)$.
A sufficient payment would be defined as
$\Pay{t}{i} = \sup \Set{c \geq 0}{%
  \Prob[\AgV \sim \AgentDist]{c + \AgV \cdot \ArmEV{t}{i} \leq \AgV \cdot \ArmEV{t}{i'}}}$.
Except for offering different payments, the algorithm remains unchanged.
Then, the exact same analysis shows that the modified algorithm
provides the same guarantees.
