\subsection{Tighter Bounds Under Additional Assumptions: Discrete Preferences and Known $p$}

The proofs of Theorem~\ref{rst:budget} and Theorem~\ref{rst:regret} incur exponential payment and regret in the initial phases because the threshold required for incentivization $1/\log(s)$ decreases slowly.  This slow decrease is needed to bound the regret in the later phases when the concentration inequality kicks in as in Equation~(\ref{equ:small_regret_bound}). In this section, we discuss two problem settings where we can modify our algorithm slightly and provide new bounds that remove this exponential dependence.  Both problem settings are special cases of the more general problem setting previously considered.

We first discuss an assumption that holds when $\theta$ follows a finite discrete distribution.
\begin{assumption}
\label{a:discrete}
(Discrete Preferences) 
$\exists \delta_0>0$ such that $F_{i,j}(\delta_0)=0$ for all $i,j$.
\end{assumption}

When Assumption~\ref{a:discrete} holds, we modify our algorithm to use a new payment-eligibility criterion: an arm $i$ is \emph{payment-eligible} at time $t$ in phase $s$ when both of the following are true:
\begin{itemize}
\item $i$ has been pulled at most%
\footnote{in fact: exactly, since the algorithm entered phase $s$}
$s$ times up to time $t$, i.e., $\NumPull{t}{s} \leq s$.
\item Based on the current estimates \ArmEV{t}{i'} of all arms'
attribute vectors, the probability of pulling arm $i$ is less than $1/s$. Let $\PullProb{t}{i} = \Prob[\AgV \sim \AgentDist]{\AgV \cdot \ArmEV{t}{i} > \AgV
  \cdot \ArmEV{t}{i'} \mbox{ for all } i' \neq i}$
be the probability that arm $i$ will be pulled
by the next (random) agent based on the current estimates. 
Then, the second criterion for payment-eligibility is that
$\PullProb{t}{i} < 1/s$.
\end{itemize}

We refer to this modified version of Algorithm~\ref{alg:basic-incentivizing} as the {\it Discrete-Preference Algorithm.} We prove the following result for this algortithm under Assumption~\ref{a:discrete}.

\pfcomment{I collapsed these results together into a single theorem, and consolidated the proofs.  I tweaked some wording in the proofs, but I didn't check them in detail.}

\begin{theorem}
\label{rst:discrete}
Under Assumption~\ref{a:discrete}, the Discrete-Preference Algorithm has an expected payment budget bounded above by 
\begin{align*}
O \left(\frac{\ARMNUM^2 d \cdot (\MAXR + \Diam d \sigma)\sigma^{5/2}L^{5/2}\Diam^{5/2}d^{5/2}}{p^{5/2}} \right). 
\end{align*}
and an expected regret bounded above by 
\begin{align}
O \left(\MAXR\cdot \frac{\ARMNUM \TieDensity^4 \Diam^4 d^4 \sigma^4}{\MinProb^4}
  + \frac{\MAXR \ARMNUM^2 d}{1 - \exp \left(
    \frac{-1.8 s \cdot \MinProb^2}{256 \TieDensity^2 \Diam^2 d^2 \sigma^2}
  \right)} \right).  \nonumber
\end{align}
\end{theorem}

\begin{proof}
The proof of the expected payment bound is the same as the proof of Theorem~\ref{rst:budget}, except that we now define $h(\ell) := \max \left( \frac{2}{\MinProb},
\left( \frac{48 \sigma \ell \TieDensity \Diam d}{\MinProb} \right)^{5/2}
\right)$.

The proof of the expected regret bound first uses a lemma bounding the expectedt number of time steps in which a payment is made by 
\begin{align}
O\left(\frac{\ARMNUM \TieDensity^3 \Diam^3 d^3 \sigma^3}{\MinProb^3}
  + \frac{\ARMNUM^2 d}{1 - \exp \left(
    \frac{-1.8 s \cdot \MinProb^2}{256 \TieDensity^2 \Diam^2 d^2 \sigma^2}
  \right)} \right). \nonumber 
\end{align}
The proof follows that of Lemma~\ref{lem:numP}, except that it defines $\EvenLaterPhase = \max (2, \frac{30 \sigma^3}{x^3}, \frac{2}{p})$.

The proof of the expected regret bound then follows the proof of Theorem~\ref{rst:regret}, with these modifications:
\begin{itemize}
\item We use the lemma stated above to bound the regret on pulls that incentivize;
\item The expected length of phase $s$ is bounded by $\ARMNUM s$;
\item The regret in Equation~\ref{equ:small_regret_bound} can now be bounded by a constant.
\end{itemize}
\end{proof}


We now consider an alternate assumption, that $\MinProb$ (or a strictly positive lower bound) is known.
\begin{assumption}
\label{a:known-p}
(Known $\MinProb$) 
A strictly positive lower bound on $\MinProb$ is known in advance.
\end{assumption}
\pfcomment{I wrote the assumption allowing a lower bound, but in the eligibility criterion we just use $\MinProb$.  We can modify the criterion.}

When this assumption holds, we modify Algorithm~\ref{alg:basic-incentivizing} by modifying its payment eligibility criterion as follows.  Let $s_0=\min_{s=1,2,\cdots,}\{\frac{1}{\log(s_0)}\leq \frac{\MinProb}{2}\}$. An arm $i$ is \emph{payment-eligible} at time $t$ (in phase $s$)
if both of the following hold:
\begin{itemize}
\item $i$ has been pulled at most%
\footnote{in fact: exactly, since the algorithm entered phase $s$}
$s$ times up to time $t$, i.e., $\NumPull{t}{s} \leq s$.
\item Based on the current estimates \ArmEV{t}{i'} of all arms'
attribute vectors, the probability of pulling arm $i$ is less than $1/\log(s+s_0)$. Let $\PullProb{t}{i} = \Prob[\AgV \sim \AgentDist]{\AgV \cdot \ArmEV{t}{i} > \AgV
  \cdot \ArmEV{t}{i'} \mbox{ for all } i' \neq i}$
be the probability that arm $i$ will be pulled
by the next (random) agent based on the current estimates. 
Then, the second criterion for payment-eligibility is that
$\PullProb{t}{i} < 1/\log(s+s_0)$.
\end{itemize}

We refer to this modified version of Algorithm~\ref{alg:basic-incentivizing} as the {\it Known-$\MinProb$ Algorithm.} We prove the following result for this algortithm under Assumption~\ref{a:known-p}.

\begin{theorem}
\label{rst:known-p}
Under Assumption~\ref{a:known-p}, the Known-$\MinProb$ Algorithm has an expected payment budget bounded above by 
\begin{align*}
O \left(\frac{\ARMNUM^2 d \cdot (\MAXR + \Diam d \sigma)\sigma^{5/2}L^{5/2}\Diam^{5/2}d^{5/2}}{p^{5/2}} \right). 
\end{align*}
and an expected regret bounded above by
\begin{align}
O \left(\MAXR\cdot \frac{\ARMNUM \TieDensity^3 \Diam^3 d^3 \sigma^3}{\MinProb^3}
  + \frac{\MAXR \ARMNUM^2 d}{1 - \exp \left(
    \frac{-1.8 s \cdot \MinProb^2}{256 \TieDensity^2 \Diam^2 d^2 \sigma^2}
  \right)} +\frac{\ARMNUM^2 d\MAXR}{p}+\frac{\ARMNUM\Diam^2 d^2\sigma^2 \TieDensity(\log(T))^3}{p}\right).  \nonumber
\end{align}
\end{theorem}

\begin{proof}
The proof of the expected payment bound follows Lemma~\ref{rst:budget} except we define $h(\ell) := 
\left( \frac{48 \sigma \ell \TieDensity \Diam d}{\MinProb} \right)^{5/2}$.

The proof of the expected regret bound first establishes the following upper bound on the number of time steps in which a payment is made:
\begin{align}
O\left(\frac{\ARMNUM \TieDensity^3 \Diam^3 d^3 \sigma^3}{\MinProb^3}
  + \frac{\ARMNUM^2 d}{1 - \exp \left(
    \frac{-1.8 s \cdot \MinProb^2}{256 \TieDensity^2 \Diam^2 d^2 \sigma^2}
  \right)} \right). \nonumber 
\end{align}
The proof of this result follows that of Lemma~\ref{lem:numP}, except it defines $\EvenLaterPhase = \max(2, \frac{30 \sigma^3}{x^3})$ since $\frac{1}{\log(s+s_0)}\leq \frac{p}{2}$ is true for all $s$.

The proof of the expected regret bound then follows that of Theorem is similar as Theorem~\ref{rst:regret}, except:
\begin{itemize}
\item We use the result above to bound the regret when we incentivize;
\item The expected length of phase $s$ is bounded by $\ARMNUM \log(s+s_0)$;
\item With the same choice of $g(s)$, the regret in Equation~\ref{equ:small_regret_bound} can now be bounded by $71.11 \ARMNUM\Diam^2 d^2\sigma^2 \TieDensity\log(T)(\log(T)+1)\log(T+s_0)=O(\frac{\ARMNUM\Diam^2 d^2\sigma^2 \TieDensity(\log(T))^3}{p})$.
\end{itemize}
\end{proof}
