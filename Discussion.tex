\subsection{Tighter Bounds Under Additional Assumptions}
%: Discrete Preferences and Known \MinProb}

The proofs of Theorem~\ref{rst:budget} and Theorem~\ref{rst:regret} incur exponential (in \ARMNUM) payment and regret in the initial phases because the threshold required for incentivization $1/\log(s)$ decreases slowly.
This slow decrease is needed to bound the regret in the later phases when the concentration inequality kicks in as in Equation~(\ref{equ:small_regret_bound}).
In this section, we discuss two restrictions \pfdelete{on the problem setting} under which  we can modify the algorithm slightly and provide stronger bounds, avoiding this exponential dependence.
Both problem settings are special cases of the more general \pfdelete{problem} setting previously considered.

\subsubsection{Discrete Preferences}
\label{subsec:discrete}
Discrete preferences by agents are captured by the following assumption (which states that the agents who are close to tied between two arms have measure 0):

\begin{assumption}[Discrete Preferences]
\label{a:discrete}
There is a positive \pfedit{$\hat{z}$} such that
$\AlmostTied{\pfedit{\hat{z}}} = 0$.
\end{assumption}

When Assumption~\ref{a:discrete} holds, we restrict the payment-eligibility criterion by only incentivizing arms with much smaller probability to be pulled: an arm $i$ is \emph{payment-eligible} at time $t$ in phase $s$ when both of the following are true:
\begin{itemize}
\item $i$ has been pulled at most
$s$ times up to time $t$, i.e., $\NumPull{t}{i} \leq s$.
\item Based on the current estimates \ArmEV{t}{i'} of all arms' attribute vectors, the probability of pulling arm $i$ is less than $1/s$ (compared to $1/\log(s)$ in the general algorithm).
\end{itemize}

We refer to this modified version of Algorithm~\ref{alg:basic-incentivizing} as the \emph{Discrete-Preference Algorithm.}
We outline a proof of the following result for this algortithm under Assumption~\ref{a:discrete} in Appendix~\ref{sec:discussion-proof1}.

\begin{theorem}
\label{rst:discrete}
Under Assumption~\ref{a:discrete}, the Discrete-Preference Algorithm has expected payment \pfdelete{budget} bounded above by 
\begin{align*}
\pfedit{
O \left(\ARMNUM^2  d \cdot (\MAXR + \Diam d \sigma)\cdot
    \max\bigg\{\frac{2}{p},
    \left( \frac{\sigma \Diam d}{\TieCutoff} \right)^{5/2}
    \bigg\}
\right)
}
\end{align*}
and expected regret bounded above by 
\begin{align}
\pfedit{
O \left(
\ARMNUM \MAXR \left(
	\frac2{\MinProb}
       + \frac{\log^2(T) \Diam^4 d^4 \sigma^4}{\hat{z}^4}\right) \right)
}
\end{align}
\end{theorem}



\subsubsection{Known \MinProb}
An alternative useful assumption is that \MinProb (or a strictly positive lower bound on \MinProb) is known.

\begin{assumption}[Known $\MinProb$]
\label{a:known-p}
A strictly positive lower bound on \MinProb is known in advance.
\end{assumption}

When this assumption holds, we choose a different modification in the definition of payment eligibility.
Let $s_0 = \exp(2/\MinProb)$.
An arm $i$ is \emph{payment-eligible} at time $t$ (in phase $s$)
if both: \pfdelete{of the following hold:}
\begin{itemize}
\item $i$ has been pulled at most
$s$ times up to time $t$, i.e., $\NumPull{t}{i} \leq s$.
\item Based on the current estimates \ArmEV{t}{i'} of all arms' attribute vectors, the probability of pulling arm $i$ is less than $1/\log(s+s_0)$.
\end{itemize}

This knowledge of \MinProb allows the algorithm to incentivize
significantly fewer arm pulls.
We refer to this modified version of Algorithm~\ref{alg:basic-incentivizing} as the \emph{Known-\MinProb Algorithm.}
We outline a proof of the following result for this algortithm under Assumption~\ref{a:known-p} in Appendix~\ref{sec:discussion-proof2}.

\begin{theorem}
\label{rst:known-p}
Under Assumption~\ref{a:known-p}, the Known-\MinProb Algorithm has expected payment \pfdelete{budget} bounded above by 
    \bcedit{
\begin{align*}
    O\left(\ARMNUM^2d\cdot(R+\Diam d\sigma)\cdot\max\bigg\{\left(\frac{\sigma \Diam d}{\TieCutoff}\right)^{5/2},
    \left(\frac{\sigma \TieDensity\Diam d}{\MinProb}\right)^{5/2}\bigg\}\right)
\end{align*}
}
and expected regret bounded above by
\bcedit{
\begin{align*}
O \left(\frac{\MAXR \ARMNUM \TieDensity^3 \Diam^3 d^3 \sigma^3}{\MinProb^3}
  + \frac{\MAXR \ARMNUM^2 d^3 \TieDensity^2 \Diam^2 \sigma^2}{\MinProb^2}
  + \frac{\ARMNUM\Diam^2 d^2\sigma^2 \TieDensity(\log(T))^3}{\MinProb}\right).
\end{align*}
}
\end{theorem}

