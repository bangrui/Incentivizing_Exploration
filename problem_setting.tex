\section{Preliminaries}
\label{sec:prob}

We consider a MAB setting with \ARMNUM arms.
Arm payoffs are determined by $d$ \emph{attributes} or \emph{features};
hence, arms can be identified with vectors $\ArmV{i} \in \R^d$.
The \ArmV{i} are (adversarially) fixed, but unknown to the agents
or the principal.
Whenever arm $i$ is pulled, its current utility-relevant features are
determined as $\ArmV{i} + \NoiseV$, where \NoiseV is a mean-zero \pfedit{independent} 
Gaussian%
\footnote{For simplicity of notation, we assume that the variance
  $\sigma^2$ is uniform across time steps. This is not material for
  the analysis.}
noise vector $\NoiseV \sim \Normal{\vc{0}}{\sigma^2 I_{d}}$.
\dkcomment{Clarify if we can handle more general noise.}
Here, $I_d$ denotes the $d \times d$ identity matrix.

At each time $t$, a new user (or \emph{agent}) arrives,
whose feature vector (which we also call his%
\footnote{We use male pronouns to refer to users and female pronouns
  to refer to the principal.}
\emph{type}) $\AgentV{t} \in \R^d$ is drawn from a known distribution
\AgentDist.
Depending on context, we will identify agents with their arrival time
$t$ or their type \AgentV{t}.
When agent $t$ pulls arm $\Pull{t} := i$,
he and all future agents observe a vector
$\ObsV{t} = \ArmV{i} + \NoiseV[t]$ for arm $i$,
and his reward is $\AgentV{t} \cdot \ObsV{t}$,
i.e., agents have linear preferences.

For each time $t$ and arm $i$, let \NumPull{t}{i} be the number of
times that arm $i$ has been pulled (strictly) before time $t$.
An agent at time $t$ estimates arm $i$'s attribute vector as the
average of vectors observed during past pulls of the arm:
$\ArmEV{t}{i} = \frac{1}{\NumPull{t}{i}+1} \cdot
(\ArmEV{0}{i} + \sum_{t'<t: \Pull{t'} = i} \ObsV{t'})$;
here, \ArmEV{i}{0} is a single draw $\ArmV{i} + \NoiseV$ for arm $i$.
(In other words, we assume that each arm is pulled once for free at time 0.)

Since each user only pulls an arm once, users are \emph{myopic}:
in the absence of incentives, user $t$ will pull an arm from
$\argmax_i \AgentV{t} \cdot \ArmEV{t}{i}$.
To incentivize users to explore more, the principal can offer
\emph{payments} $\Pay{t}{i}$ to user $t$ for pulling arm $i$.
Then, user $t$ will pull an\footnote{We assume that ties are broken in
  favor of an arm with largest payment \Pay{t}{i}; this assumption is
  only for notational convenience, and can of course be avoided by
  raising payments infinitesimally.
  \pfcomment{In the finite preference setting this is clear, but it's a little bit delicate when there are continuum preferences, since regardless of the payment we offer there may always be support within our preference distribution for a $\theta$ that causes a tie.  In this continuum setting it should instead be possible to perturb infinitessimally so that we avoid ties with probability 1.}  
  } arm $i$ maximizing
$\argmax_i (\Pay{t}{i} + \AgentV{t} \cdot \ArmEV{t}{i})$.
The principal cannot observe \AgentV{t},
and only knows the distribution \AgentDist from which it is drawn.
Her goal is to achieve a good tradeoff between the cumulative
\emph{regret} experienced by all users up to time $T$,
and the total \emph{payment} she makes to the users.

We define the regret at time $t$ as
$\Regret{t} = (\max_{i} \AgentV{t} \cdot \ArmV{i}) - \AgentV{t} \cdot \ArmV{\Pull{t}}$,
and the cumulative regret up to time $T$ as
$\TotalRegret{T} = \sum_{t=1}^{T} \Regret{t}$.
Similarly, $\PayA{t} = \Pay{t}{\Pull{t}}$ is the actual incentive
payment at time $t$,
and the cumulative payment up to time $T$ is
$\TotalPay{T} = \sum_{t=1}^{T} \PayA{t}$.
More formally, the principal's goal is to find a policy
\POLICY for offering payments under which both the cumulative expected
regret
$\Expect[\POLICY]{\TotalRegret{T}}$ the cumulative expected payment
$\Expect[\POLICY]{\TotalPay{T}}$ are small.

To support the formulation of our results and the analysis,
we define the following additional notation.
We let
$\Best{\AgV} \in \argmax_i \AgV \cdot \ArmV{i}$
and
$ \Second{\AgV} \in \argmax_{i \neq \Best{\AgV}} \AgV \cdot \ArmV{i}$
denote the (indices of) the best and second-best arms for an agent
with attribute vector \AgV;
\dkdelete{breaking ties uniformly at random}.
\dkcomment{What would it mean to break ties at random in a
  definition?}
\pfcomment{Perhaps we should say instead ``breaking ties arbitrarily'' if our results don't depend on how we break ties.}

\dkdelete{This behavior may be recovered if agents are Bayesian and
  share a common non-informative prior distribution that is constant
  over $\mathbb{R}^m$ and know $\sigma^2$.  In this case, the
  posterior distribution on $u_{i}$ at time $t$ is multivariate normal
  with mean $u_{i,t}$, and the expected value of $\theta_t \cdot u_i$
  under this posterior conditioned on $\theta_t$ is $\theta_t \cdot
  u_{i,t}$ (see Equation 2.13 in Section 2.5, \cite{Ge04}).
  Alternatively, one may simply take our assumption that agents use
  the average as their estimate of an attribute value directly without
  such a Bayesian justification.}
\dkcomment{I wonder what's the best way to discuss this.}


\subsection{Properties of \AgentDist}
Our algorithms rely on a few assumptions about the agent distribution
\AgentDist.

\begin{assumption}[Compact Support] \label{A2}
\AgentDist has a compact support set contained in $[0,\Diam]^d$.
\end{assumption}

Let $\MinProb = \min_{i} \Prob[\AgentDist]{\Set{\AgV}{\Best{\AgV} = i}}$
denote the minimum (over all arms) fraction of users that prefer any
particular arm.

\begin{assumption}[Every arm is someone's best] \label{A3}
Each arm $i$ has a strictly positive proportion of users for whom $i$
is the best arm; that is, $\MinProb > 0$.
\end{assumption}

Let $\FirstTwo{i}{i'} = \Set{\AgV}{\Best{\AgV} = i, \Second{\AgV} = i'}$
be the set of agent attribute vectors whose best arm is $i$ and
second-best arm is $i'$.
Let $F_{i,i'}$ be the marginal cumulative density function
(or cumulative mass function if $\AgentDist(\cdot)$ is a discrete distribution)
\pfcomment{Maybe this should just be the conditional cumulative distribution function, 
$F_{i,i'}(z) = \Prob[\AgentDist]{(\ArmV{i}-\ArmV{i'}) \cdot \AgV \le z \mid \AgV \in \Omega_{i,i^{'}}}$?}
of $(\ArmV{i}-\ArmV{i'}) \cdot \AgV$,
conditioned on $\AgV \in \Omega_{i,i^{'}}$.
In words, $F_{i,i'}$ is the distribution of the \emph{strength} of the
preference of a random agent for arm $i$ over arm $i'$, given that
$i$ and $i'$ are the agent's first and second choices.
\pfcomment{We should say something about what $F_{i,i'}$ is when $\Prob[\AgentDist]{\AgV \in \Omega_{i,i^{'}}}=0$.  We know that arm $i$ is someone's best arm, but it doesn't have to be possible for $i'$ to be someone's second best arm when $i$ is best.  Maybe we can just set $F_{i,i'}$ to be $0$ in that case.}

\begin{assumption}[Not too many near-ties] \label{A1}
Near-ties have vanishing probability; 
that is, there exists a constant \TieDensity such that
for all pairs $i,i'$ of arms and all $z \in \R^+$,
we have $F_{i,i'}(z) \leq \TieDensity \cdot z$.
\end{assumption}






