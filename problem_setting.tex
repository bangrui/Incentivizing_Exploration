\section{Preliminaries}
\label{sec:prob}

We consider a multi-armed bandit setting with \ARMNUM arms.
Arm payoffs are determined by $d$ \emph{attributes} or \emph{features};
hence, arms are identified with vectors $\ArmV{i} \in \R^d$.
The \ArmV{i} are (adversarially) fixed, and unknown to the agents
or the principal.
When arm $i$ is pulled, its current utility-relevant features are
determined as $\ArmV{i} + \NoiseV$, where \NoiseV is a mean-zero independent Gaussian%
\footnote{For simplicity of notation, we assume that the variance
  $\sigma^2$ is uniform across time steps.
In fact, the analysis extends straightforwardly to any
mean-zero \pfedit{sub-Gaussian($\sigma^2$)} noise \pfedit{that is independent but not necessarily identically distributed across time}.}
noise vector $\NoiseV \sim \Normal{\vc{0}}{\sigma^2 I_{d}}$.
Here, $I_d$ denotes the $d \times d$ identity matrix.

At each time $t$, a new user (or \emph{agent}) arrives,
whose feature vector (which we also call his \emph{type})
$\AgentV{t} \in \R^d$ is drawn from a known distribution \AgentDist.
Depending on context, we will identify agents with their arrival time
$t$ or their type \AgentV{t}.
When agent $t$ pulls arm $\Pull{t} := i$,
he and all future agents observe a vector
$\ObsV{t} = \ArmV{i} + \NoiseV[t]$ for arm $i$,
and his reward is $\AgentV{t} \cdot \ObsV{t}$,
i.e., agents have linear preferences.%

For each time $t$ and arm $i$, let \NumPull{t}{i} be the number of
times that arm $i$ has been pulled (strictly) before time $t$.
An agent at time $t$ estimates arm $i$'s attribute vector as the
average of vectors observed during the arm's past pulls:
$\ArmEV{t}{i} = \frac{1}{\NumPull{t}{i}+1} \cdot
(\ArmEV{0}{i} + \sum_{t'<t: \Pull{t'} = i} \ObsV{t'})$;
here, \ArmEV{i}{0} is a single draw $\ArmV{i} + \NoiseV$ for arm $i$.
(In other words, we assume each arm is pulled once for free at time 0.)

Since each user only pulls an arm once, users are \emph{myopic}:
in the absence of incentives, user $t$ will pull an arm from
$\argmax_i \AgentV{t} \cdot \ArmEV{t}{i}$.
To incentivize users to explore more, the principal can offer
\emph{payments} $\Pay{t}{i}$ to user $t$ for pulling arm $i$.
Then, user $t$ will pull an\footnote{We assume that ties are broken in
  favor of an arm with largest payment \Pay{t}{i}.}
% ; this assumption is
%   only for notational convenience, and can of course be avoided by
%   raising payments infinitesimally.
%   \pfcomment{In the finite preference setting this is clear, but it's a little bit delicate when there are continuum preferences, since regardless of the payment we offer there may always be support within our preference distribution for a $\theta$ that causes a tie.  In this continuum setting it should instead be possible to perturb infinitessimally so that we avoid ties with probability 1.}  
% }
arm $i$ maximizing $\argmax_i (\Pay{t}{i} + \AgentV{t} \cdot \ArmEV{t}{i})$.
The principal cannot observe \AgentV{t},
and only knows the distribution \AgentDist from which it is drawn.
Her goal is to reduce both the cumulative
\emph{regret} experienced by all users up to time $T$,
and the total \emph{payment} she makes to the users.

We define the regret at time $t$ as
$\Regret{t} = (\max_{i} \AgentV{t} \cdot \ArmV{i}) - \AgentV{t} \cdot \ArmV{\Pull{t}}$,
and the cumulative regret up to time $T$ as
$\TotalRegret{T} = \sum_{t=1}^{T} \Regret{t}$.
Similarly, $\PayA{t} = \Pay{t}{\Pull{t}}$ is the actual incentive
payment at time $t$,
and the cumulative payment up to time $T$ is
$\TotalPay{T} = \sum_{t=1}^{T} \PayA{t}$.
More formally, the principal's goal is to find a policy
\POLICY for offering payments under which both the cumulative expected
regret
$\Expect{\TotalRegret{T}}$ the cumulative expected payment
$\Expect{\TotalPay{T}}$ are small.

To support the formulation of our results and the analysis,
we define the following additional notation.
We let
$\Best{\AgV} \in \argmax_i \AgV \cdot \ArmV{i}$
and
$ \Second{\AgV} \in \argmax_{i \neq \Best{\AgV}} \AgV \cdot \ArmV{i}$
denote the (indices of) the best and second-best arms for an agent
with attribute vector \AgV,
breaking ties arbitrarily (but consistently).

% \dkdelete{This behavior may be recovered if agents are Bayesian and
%   share a common non-informative prior distribution that is constant
%   over $\mathbb{R}^m$ and know $\sigma^2$.  In this case, the
%   posterior distribution on $u_{i}$ at time $t$ is multivariate normal
%   with mean $u_{i,t}$, and the expected value of $\theta_t \cdot u_i$
%   under this posterior conditioned on $\theta_t$ is $\theta_t \cdot
%   u_{i,t}$ (see Equation 2.13 in Section 2.5, \cite{Ge04}).
%   Alternatively, one may simply take our assumption that agents use
%   the average as their estimate of an attribute value directly without
%   such a Bayesian justification.}
% \dkcomment{I wonder what's the best way to discuss this.}

% \paragraph{Properties of \AgentDist}
Our algorithms rely on the following three assumptions on the agent distribution
\AgentDist.

\begin{assumption}[Compact Support] \label{A2}
\AgentDist has a compact support set contained in $[0,\Diam]^d$.
\end{assumption}

Let $\MinProb = \min_{i} \Prob[\AgentDist]{\Set{\AgV}{\Best{\AgV} = i}}$
denote the minimum (over all arms) fraction of users that prefer any
particular arm.

\begin{assumption}[Every arm is someone's best] \label{A3}
Each arm $i$ has a strictly positive proportion of users for whom $i$
is the best arm; that is, $\MinProb > 0$.
\end{assumption}

%Let $\FirstTwo{i}{i'} = \Set{\AgV}{\Best{\AgV} = i, \Second{\AgV} = i'}$
%be the set of agent attribute vectors whose best arm is $i$ and
%second-best arm is $i'$.
% Let $F_{i,i'}$ be the cumulative distribution function,
% $F_{i,i'}(z) = \Prob[\AgV \sim \AgentDist]{\AgV \in \FirstTwo{i}{i'}
%   \mbox{ and } (\ArmV{i}-\ArmV{i'}) \cdot \AgV \leq z}$
% of $(\ArmV{i}-\ArmV{i'}) \cdot \AgV$,
% on $\AgV \in \Omega_{i,i^{'}}$.
% In words, $F_{i,i'}$ is the distribution of the \emph{strength} of the
% preference of a random agent for arm $i$ over arm $i'$

% $\Best{\AgV} \in \argmax_i \AgV \cdot \ArmV{i}$
% and
% $ \Second{\AgV} \in \argmax_{i \neq \Best{\AgV}} \AgV \cdot \ArmV{i}$

Let \AlmostTied{z} be the cumulative distribution function (CDF)
$\AlmostTied{z} = \Prob[\AgV \sim \AgentDist]{%
  (\ArmV{\Best{\AgV}}-\ArmV{\Second{\AgV}}) \cdot \AgV \leq z}$.
In words, \AlmostTied{z} is the CDF of the \emph{strength} of the
preference of a random agent for his best arm over his second-best arm.

\begin{assumption}[Not too many near-ties] \label{A1}
Near-ties have vanishing probability; 
that is, there exists a constant \TieDensity such that for all $z \in \R^+$,
%for all pairs $i,i'$ of arms and 
$\AlmostTied{z} \leq \TieDensity \cdot z$.
\end{assumption}

\paragraph{Role of parameters:}
Our problem setting is characterized by many 
parameters.
Of these, we consider \ARMNUM, $\MinProb \leq 1/\ARMNUM$ and $T$ to be
the key parameters, while we consider $\sigma, \Diam, d, \TieDensity$ to be
constant.
We track all parameters through most of our proofs,
but report final results in terms of only the three key parameters,
except where we illustrate a specific point.
